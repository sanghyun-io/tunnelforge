"""
MySQL ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ê¸°
- ê³ ì•„ ë ˆì½”ë“œ(orphan rows) íƒì§€
- FK ê´€ê³„ ë¶„ì„ ë° ì •ë¦¬
- MySQL 8.0.x â†’ 8.4.x í˜¸í™˜ì„± ê²€ì‚¬ (Upgrade Checker í†µí•©)
- dry-run ì§€ì›
- ë¤í”„ íŒŒì¼ ë¶„ì„ (SQL/TSV)
- 2-Pass ë¶„ì„ ì•„í‚¤í…ì²˜ (FK í¬ë¡œìŠ¤ ê²€ì¦)
"""
import re
from typing import List, Dict, Set, Tuple, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from src.core.db_connector import MySQLConnector

# ============================================================
# ìƒˆ ìƒìˆ˜ ëª¨ë“ˆì—ì„œ import (migration_constants.py)
# ============================================================
from src.core.migration_constants import (
    ALL_REMOVED_FUNCTIONS,
    DEPRECATED_FUNCTIONS_84,
    OBSOLETE_SQL_MODES,
    IssueType,
    CompatibilityIssue,
    INVALID_DATE_PATTERN,
    INVALID_DATETIME_PATTERN,
    ZEROFILL_PATTERN,
    FLOAT_PRECISION_PATTERN,
    FK_NAME_LENGTH_PATTERN,
    AUTH_PLUGIN_PATTERN,
    FTS_TABLE_PREFIX_PATTERN,
    SUPER_PRIVILEGE_PATTERN,
    SYS_VAR_USAGE_PATTERN,
)

# ê·œì¹™ ëª¨ë“ˆì—ì„œ import (ì„ íƒì  - ì—ëŸ¬ ë°©ì§€)
try:
    from src.core.migration_rules import DataIntegrityRules, SchemaRules, StorageRules
    RULES_AVAILABLE = True
except ImportError:
    RULES_AVAILABLE = False

# íŒŒì„œ ëª¨ë“ˆì—ì„œ import (ì„ íƒì )
try:
    from src.core.migration_parsers import SQLParser, ParsedTable
    PARSERS_AVAILABLE = True
except ImportError:
    PARSERS_AVAILABLE = False

# Fix Query ìƒì„±ê¸° import (ì„ íƒì )
try:
    from src.core.migration_fix_generator import FixQueryGenerator
    FIX_GENERATOR_AVAILABLE = True
except ImportError:
    FIX_GENERATOR_AVAILABLE = False

# Report Exporter import (ì„ íƒì )
try:
    from src.core.migration_report import ReportExporter
    REPORT_EXPORTER_AVAILABLE = True
except ImportError:
    REPORT_EXPORTER_AVAILABLE = False


# IssueTypeì€ migration_constantsì—ì„œ importë¨


class ActionType(Enum):
    """ì¡°ì¹˜ ìœ í˜•"""
    DELETE = "delete"  # ì‚­ì œ
    UPDATE = "update"  # ì—…ë°ì´íŠ¸
    SET_NULL = "set_null"  # NULLë¡œ ì„¤ì •
    MANUAL = "manual"  # ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”


@dataclass
class OrphanRecord:
    """ê³ ì•„ ë ˆì½”ë“œ ì •ë³´"""
    child_table: str
    child_column: str
    parent_table: str
    parent_column: str
    orphan_count: int
    sample_values: List[Any] = field(default_factory=list)


@dataclass
class ForeignKeyInfo:
    """FK ê´€ê³„ ì •ë³´"""
    constraint_name: str
    child_table: str
    child_column: str
    parent_table: str
    parent_column: str
    on_delete: str
    on_update: str


# CompatibilityIssueëŠ” migration_constantsì—ì„œ import (ë‹¨ì¼ ì •ì˜)


@dataclass
class CleanupAction:
    """ì •ë¦¬ ì‘ì—…"""
    action_type: ActionType
    table: str
    description: str
    sql: str
    affected_rows: int
    dry_run: bool = True


@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼"""
    schema: str
    analyzed_at: str
    total_tables: int
    total_fk_relations: int
    orphan_records: List[OrphanRecord] = field(default_factory=list)
    compatibility_issues: List[CompatibilityIssue] = field(default_factory=list)
    cleanup_actions: List[CleanupAction] = field(default_factory=list)
    fk_tree: Dict[str, List[str]] = field(default_factory=dict)

    def to_dict(self) -> dict:
        """JSON ì§ë ¬í™”ìš© ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        import dataclasses
        return {
            'schema': self.schema,
            'analyzed_at': self.analyzed_at,
            'total_tables': self.total_tables,
            'total_fk_relations': self.total_fk_relations,
            'orphan_records': [dataclasses.asdict(o) for o in self.orphan_records],
            'compatibility_issues': [
                {**dataclasses.asdict(i), 'issue_type': i.issue_type.value}
                for i in self.compatibility_issues
            ],
            'cleanup_actions': [
                {**dataclasses.asdict(a), 'action_type': a.action_type.value}
                for a in self.cleanup_actions
            ],
            'fk_tree': self.fk_tree
        }

    @classmethod
    def from_dict(cls, data: dict) -> 'AnalysisResult':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ AnalysisResult ë³µì›"""
        orphan_records = [OrphanRecord(**o) for o in data.get('orphan_records', [])]
        compatibility_issues = [
            CompatibilityIssue(
                issue_type=IssueType(i['issue_type']),
                severity=i['severity'],
                location=i['location'],
                description=i['description'],
                suggestion=i['suggestion'],
                fix_query=i.get('fix_query'),
                doc_link=i.get('doc_link'),
                mysql_shell_check_id=i.get('mysql_shell_check_id'),
                code_snippet=i.get('code_snippet'),
                table_name=i.get('table_name'),
                column_name=i.get('column_name')
            )
            for i in data.get('compatibility_issues', [])
        ]
        cleanup_actions = [
            CleanupAction(
                action_type=ActionType(a['action_type']),
                table=a['table'],
                description=a['description'],
                sql=a['sql'],
                affected_rows=a['affected_rows'],
                dry_run=a.get('dry_run', True)
            )
            for a in data.get('cleanup_actions', [])
        ]

        return cls(
            schema=data['schema'],
            analyzed_at=data['analyzed_at'],
            total_tables=data['total_tables'],
            total_fk_relations=data['total_fk_relations'],
            orphan_records=orphan_records,
            compatibility_issues=compatibility_issues,
            cleanup_actions=cleanup_actions,
            fk_tree=data.get('fk_tree', {})
        )


class MigrationAnalyzer:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ê¸°"""

    # MySQL 8.4ì—ì„œ ì œê±°ëœ/deprecatedëœ í•¨ìˆ˜ë“¤ (ì „ì—­ ìƒìˆ˜ ì‚¬ìš©)
    DEPRECATED_FUNCTIONS = list(ALL_REMOVED_FUNCTIONS)
    # deprecatedë§Œ (ê²½ê³  ìˆ˜ì¤€ ì°¨ë“±í™”ìš©)
    _DEPRECATED_ONLY = set(DEPRECATED_FUNCTIONS_84)

    # MySQL 8.4ì—ì„œ ìƒˆë¡œìš´ ì˜ˆì•½ì–´ë“¤ (ê¸°ì¡´ 22ê°œ + 8.4 ì¶”ê°€ 4ê°œ)
    NEW_RESERVED_KEYWORDS = [
        'CUME_DIST', 'DENSE_RANK', 'EMPTY', 'EXCEPT', 'FIRST_VALUE',
        'GROUPING', 'GROUPS', 'JSON_TABLE', 'LAG', 'LAST_VALUE', 'LATERAL',
        'LEAD', 'NTH_VALUE', 'NTILE', 'OF', 'OVER', 'PERCENT_RANK',
        'RANK', 'RECURSIVE', 'ROW_NUMBER', 'SYSTEM', 'WINDOW',
        # MySQL 8.4 ì¶”ê°€ ì˜ˆì•½ì–´
        'MANUAL', 'PARALLEL', 'QUALIFY', 'TABLESAMPLE'
    ]

    def __init__(self, connector: MySQLConnector):
        self.connector = connector
        self._progress_callback: Optional[Callable[[str], None]] = None

    def set_progress_callback(self, callback: Callable[[str], None]):
        """ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •"""
        self._progress_callback = callback

    def _log(self, message: str):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        if self._progress_callback:
            self._progress_callback(message)

    def get_foreign_keys(self, schema: str) -> List[ForeignKeyInfo]:
        """ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  FK ê´€ê³„ ì¡°íšŒ"""
        query = """
        SELECT
            tc.CONSTRAINT_NAME,
            kcu.TABLE_NAME as CHILD_TABLE,
            kcu.COLUMN_NAME as CHILD_COLUMN,
            kcu.REFERENCED_TABLE_NAME as PARENT_TABLE,
            kcu.REFERENCED_COLUMN_NAME as PARENT_COLUMN,
            rc.DELETE_RULE,
            rc.UPDATE_RULE
        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc
        JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE kcu
            ON tc.CONSTRAINT_NAME = kcu.CONSTRAINT_NAME
            AND tc.TABLE_SCHEMA = kcu.TABLE_SCHEMA
        JOIN INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS rc
            ON tc.CONSTRAINT_NAME = rc.CONSTRAINT_NAME
            AND tc.TABLE_SCHEMA = rc.CONSTRAINT_SCHEMA
        WHERE tc.TABLE_SCHEMA = %s
            AND tc.CONSTRAINT_TYPE = 'FOREIGN KEY'
        ORDER BY kcu.TABLE_NAME, kcu.COLUMN_NAME
        """
        rows = self.connector.execute(query, (schema,))

        fk_list = []
        for row in rows:
            fk_list.append(ForeignKeyInfo(
                constraint_name=row['CONSTRAINT_NAME'],
                child_table=row['CHILD_TABLE'],
                child_column=row['CHILD_COLUMN'],
                parent_table=row['PARENT_TABLE'],
                parent_column=row['PARENT_COLUMN'],
                on_delete=row['DELETE_RULE'],
                on_update=row['UPDATE_RULE']
            ))

        return fk_list

    def build_fk_tree(self, schema: str) -> Dict[str, List[str]]:
        """FK ê´€ê³„ íŠ¸ë¦¬ êµ¬ì„± (ë¶€ëª¨ â†’ ìì‹ ëª©ë¡)"""
        fk_list = self.get_foreign_keys(schema)

        tree = {}
        for fk in fk_list:
            if fk.parent_table not in tree:
                tree[fk.parent_table] = []
            if fk.child_table not in tree[fk.parent_table]:
                tree[fk.parent_table].append(fk.child_table)

        return tree

    def _get_table_row_count(self, schema: str, table: str) -> int:
        """í…Œì´ë¸” ëŒ€ëµì ì¸ í–‰ ìˆ˜ ì¡°íšŒ (INFORMATION_SCHEMA ì‚¬ìš©, ë¹ ë¦„)"""
        query = f"""
        SELECT TABLE_ROWS
        FROM INFORMATION_SCHEMA.TABLES
        WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME = '{table}'
        """
        result = self.connector.execute(query)
        return result[0]['TABLE_ROWS'] if result and result[0]['TABLE_ROWS'] else 0

    def find_orphan_records(
        self,
        schema: str,
        sample_limit: int = 5,
        large_table_threshold: int = 500000  # 50ë§Œ í–‰ ì´ìƒì´ë©´ í° í…Œì´ë¸”
    ) -> List[OrphanRecord]:
        """ê³ ì•„ ë ˆì½”ë“œ íƒì§€ (ë¶€ëª¨ ì—†ëŠ” ìì‹ ë ˆì½”ë“œ)"""
        import time
        self._log("ğŸ” ê³ ì•„ ë ˆì½”ë“œ íƒì§€ ì¤‘...")

        fk_list = self.get_foreign_keys(schema)
        orphans = []

        for i, fk in enumerate(fk_list, 1):
            try:
                # í…Œì´ë¸” í¬ê¸° ì‚¬ì „ í™•ì¸
                child_rows = self._get_table_row_count(schema, fk.child_table)
                parent_rows = self._get_table_row_count(schema, fk.parent_table)
                is_large = child_rows > large_table_threshold or parent_rows > large_table_threshold

                size_info = ""
                if child_rows > 100000 or parent_rows > 100000:
                    size_info = f" [ìì‹:{child_rows:,}í–‰, ë¶€ëª¨:{parent_rows:,}í–‰]"

                self._log(f"  ê²€ì‚¬ ì¤‘: {fk.child_table}.{fk.child_column} â†’ {fk.parent_table}.{fk.parent_column} ({i}/{len(fk_list)}){size_info}")

                start_time = time.time()

                if is_large:
                    # í° í…Œì´ë¸”: NOT EXISTS ì‚¬ìš© (ë” ë¹ ë¦„)
                    self._log(f"    ğŸ“Š ëŒ€ìš©ëŸ‰ í…Œì´ë¸” - ìµœì í™” ì¿¼ë¦¬ ì‚¬ìš©")
                    count_query = f"""
                    SELECT COUNT(*) as cnt
                    FROM `{schema}`.`{fk.child_table}` c
                    WHERE c.`{fk.child_column}` IS NOT NULL
                        AND NOT EXISTS (
                            SELECT 1 FROM `{schema}`.`{fk.parent_table}` p
                            WHERE p.`{fk.parent_column}` = c.`{fk.child_column}`
                        )
                    """
                else:
                    # ì¼ë°˜ í…Œì´ë¸”: LEFT JOIN ì‚¬ìš©
                    count_query = f"""
                    SELECT COUNT(*) as cnt
                    FROM `{schema}`.`{fk.child_table}` c
                    LEFT JOIN `{schema}`.`{fk.parent_table}` p
                        ON c.`{fk.child_column}` = p.`{fk.parent_column}`
                    WHERE c.`{fk.child_column}` IS NOT NULL
                        AND p.`{fk.parent_column}` IS NULL
                    """

                result = self.connector.execute(count_query)
                orphan_count = result[0]['cnt'] if result else 0

                elapsed = time.time() - start_time
                if elapsed > 3:  # 3ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ê²½ê³ 
                    self._log(f"    â±ï¸ ì¿¼ë¦¬ ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")

                if orphan_count > 0:
                    # ìƒ˜í”Œ ê°’ ì¡°íšŒ (í•­ìƒ LIMITìœ¼ë¡œ ì œí•œ)
                    if is_large:
                        sample_query = f"""
                        SELECT DISTINCT c.`{fk.child_column}` as orphan_value
                        FROM `{schema}`.`{fk.child_table}` c
                        WHERE c.`{fk.child_column}` IS NOT NULL
                            AND NOT EXISTS (
                                SELECT 1 FROM `{schema}`.`{fk.parent_table}` p
                                WHERE p.`{fk.parent_column}` = c.`{fk.child_column}`
                            )
                        LIMIT {sample_limit}
                        """
                    else:
                        sample_query = f"""
                        SELECT DISTINCT c.`{fk.child_column}` as orphan_value
                        FROM `{schema}`.`{fk.child_table}` c
                        LEFT JOIN `{schema}`.`{fk.parent_table}` p
                            ON c.`{fk.child_column}` = p.`{fk.parent_column}`
                        WHERE c.`{fk.child_column}` IS NOT NULL
                            AND p.`{fk.parent_column}` IS NULL
                        LIMIT {sample_limit}
                        """
                    samples = self.connector.execute(sample_query)
                    sample_values = [s['orphan_value'] for s in samples]

                    orphans.append(OrphanRecord(
                        child_table=fk.child_table,
                        child_column=fk.child_column,
                        parent_table=fk.parent_table,
                        parent_column=fk.parent_column,
                        orphan_count=orphan_count,
                        sample_values=sample_values
                    ))

                    self._log(f"    âš ï¸ ê³ ì•„ ë ˆì½”ë“œ ë°œê²¬: {orphan_count}ê°œ")

            except Exception as e:
                self._log(f"    âŒ ê²€ì‚¬ ì‹¤íŒ¨: {fk.child_table}.{fk.child_column} - {str(e)}")
                continue

        return orphans

    def check_charset_issues(self, schema: str) -> List[CompatibilityIssue]:
        """utf8mb3 ì‚¬ìš© í…Œì´ë¸”/ì»¬ëŸ¼ í™•ì¸"""
        self._log("ğŸ” ë¬¸ìì…‹ ì´ìŠˆ í™•ì¸ ì¤‘...")

        issues = []

        # í…Œì´ë¸” ë ˆë²¨ charset í™•ì¸
        table_query = """
        SELECT TABLE_NAME, TABLE_COLLATION
        FROM INFORMATION_SCHEMA.TABLES
        WHERE TABLE_SCHEMA = %s
            AND TABLE_TYPE = 'BASE TABLE'
            AND (TABLE_COLLATION LIKE 'utf8\\_%%' OR TABLE_COLLATION LIKE 'utf8mb3\\_%%')
        """
        tables = self.connector.execute(table_query, (schema,))

        for t in tables:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.CHARSET_ISSUE,
                severity="warning",
                location=f"{schema}.{t['TABLE_NAME']}",
                description=f"í…Œì´ë¸”ì´ utf8mb3 collation ì‚¬ìš© ì¤‘: {t['TABLE_COLLATION']}",
                suggestion="ALTER TABLE ... CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"
            ))

        # ì»¬ëŸ¼ ë ˆë²¨ charset í™•ì¸ (VIEW ì œì™¸, BASE TABLEë§Œ)
        column_query = """
        SELECT c.TABLE_NAME, c.COLUMN_NAME, c.CHARACTER_SET_NAME, c.COLLATION_NAME
        FROM INFORMATION_SCHEMA.COLUMNS c
        JOIN INFORMATION_SCHEMA.TABLES t
            ON c.TABLE_NAME = t.TABLE_NAME
            AND c.TABLE_SCHEMA = t.TABLE_SCHEMA
        WHERE c.TABLE_SCHEMA = %s
            AND c.CHARACTER_SET_NAME IN ('utf8', 'utf8mb3')
            AND t.TABLE_TYPE = 'BASE TABLE'
        """
        columns = self.connector.execute(column_query, (schema,))

        for c in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.CHARSET_ISSUE,
                severity="warning",
                location=f"{schema}.{c['TABLE_NAME']}.{c['COLUMN_NAME']}",
                description=f"ì»¬ëŸ¼ì´ utf8mb3 ì‚¬ìš© ì¤‘: {c['CHARACTER_SET_NAME']}",
                suggestion="ALTER TABLE ... MODIFY COLUMN ... CHARACTER SET utf8mb4"
            ))

        if issues:
            self._log(f"  âš ï¸ ë¬¸ìì…‹ ì´ìŠˆ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ë¬¸ìì…‹ ì´ìŠˆ ì—†ìŒ")

        return issues

    def check_reserved_keywords(self, schema: str) -> List[CompatibilityIssue]:
        """ì˜ˆì•½ì–´ì™€ ì¶©ëŒí•˜ëŠ” ì»¬ëŸ¼/í…Œì´ë¸”ëª… í™•ì¸"""
        self._log("ğŸ” ì˜ˆì•½ì–´ ì¶©ëŒ í™•ì¸ ì¤‘...")

        issues = []
        keywords_upper = set(k.upper() for k in self.NEW_RESERVED_KEYWORDS)

        # í…Œì´ë¸”ëª… í™•ì¸
        tables = self.connector.get_tables(schema)
        for table in tables:
            if table.upper() in keywords_upper:
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.RESERVED_KEYWORD,
                    severity="error",
                    location=f"{schema}.{table}",
                    description=f"í…Œì´ë¸”ëª… '{table}'ì´ MySQL 8.4 ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                    suggestion="í…Œì´ë¸”ëª…ì„ ë°±í‹±ìœ¼ë¡œ ê°ì‹¸ê±°ë‚˜ ì´ë¦„ ë³€ê²½ í•„ìš”"
                ))

        # ì»¬ëŸ¼ëª… í™•ì¸
        column_query = """
        SELECT TABLE_NAME, COLUMN_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
        """
        columns = self.connector.execute(column_query, (schema,))

        for c in columns:
            if c['COLUMN_NAME'].upper() in keywords_upper:
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.RESERVED_KEYWORD,
                    severity="warning",
                    location=f"{schema}.{c['TABLE_NAME']}.{c['COLUMN_NAME']}",
                    description=f"ì»¬ëŸ¼ëª… '{c['COLUMN_NAME']}'ì´ MySQL 8.4 ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                    suggestion="ì»¬ëŸ¼ ì°¸ì¡° ì‹œ ë°±í‹±(`) ì‚¬ìš© í•„ìš”"
                ))

        if issues:
            self._log(f"  âš ï¸ ì˜ˆì•½ì–´ ì¶©ëŒ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ì˜ˆì•½ì–´ ì¶©ëŒ ì—†ìŒ")

        return issues

    def check_deprecated_in_routines(self, schema: str) -> List[CompatibilityIssue]:
        """ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ì—ì„œ deprecated í•¨ìˆ˜ ì‚¬ìš© í™•ì¸"""
        self._log("ğŸ” ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ ê²€ì‚¬ ì¤‘...")

        issues = []

        # ì €ì¥ í”„ë¡œì‹œì €ì™€ í•¨ìˆ˜ ì¡°íšŒ
        routine_query = """
        SELECT ROUTINE_NAME, ROUTINE_TYPE, ROUTINE_DEFINITION
        FROM INFORMATION_SCHEMA.ROUTINES
        WHERE ROUTINE_SCHEMA = %s
            AND ROUTINE_DEFINITION IS NOT NULL
        """
        routines = self.connector.execute(routine_query, (schema,))

        for routine in routines:
            definition = routine['ROUTINE_DEFINITION'].upper() if routine['ROUTINE_DEFINITION'] else ""

            for func in self.DEPRECATED_FUNCTIONS:
                if func in definition:
                    # removed vs deprecated ì°¨ë“±í™”
                    is_deprecated_only = func in self._DEPRECATED_ONLY
                    severity = "warning" if is_deprecated_only else "error"
                    label = "deprecated" if is_deprecated_only else "removed"
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.DEPRECATED_FUNCTION,
                        severity=severity,
                        location=f"{routine['ROUTINE_TYPE']} {schema}.{routine['ROUTINE_NAME']}",
                        description=f"{label} í•¨ìˆ˜ '{func}' ì‚¬ìš© ì¤‘",
                        suggestion=f"'{func}' í•¨ìˆ˜ë¥¼ ëŒ€ì²´ í•¨ìˆ˜ë¡œ ë³€ê²½ í•„ìš”"
                    ))

        if issues:
            self._log(f"  âš ï¸ deprecated í•¨ìˆ˜ ì‚¬ìš© {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… deprecated í•¨ìˆ˜ ì—†ìŒ")

        return issues

    def check_sql_modes(self) -> List[CompatibilityIssue]:
        """í˜„ì¬ SQL ëª¨ë“œ í™•ì¸"""
        self._log("ğŸ” SQL ëª¨ë“œ í™•ì¸ ì¤‘...")

        issues = []

        # deprecated SQL ëª¨ë“œë“¤ (ìƒìˆ˜ ëª¨ë“ˆì˜ OBSOLETE_SQL_MODES ì‚¬ìš©)
        deprecated_modes = OBSOLETE_SQL_MODES

        result = self.connector.execute("SELECT @@sql_mode as sql_mode")
        if result:
            sql_mode_raw = result[0].get('sql_mode') or ''
            current_modes = sql_mode_raw.split(',') if sql_mode_raw else []

            for mode in current_modes:
                mode = mode.strip()
                if mode in deprecated_modes:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.SQL_MODE_ISSUE,
                        severity="warning",
                        location="@@sql_mode",
                        description=f"deprecated SQL ëª¨ë“œ '{mode}' ì‚¬ìš© ì¤‘",
                        suggestion=f"sql_modeì—ì„œ '{mode}' ì œê±° í•„ìš”"
                    ))

        if issues:
            self._log(f"  âš ï¸ deprecated SQL ëª¨ë“œ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… SQL ëª¨ë“œ ì •ìƒ")

        return issues

    def generate_cleanup_sql(
        self,
        orphan: OrphanRecord,
        action: ActionType,
        schema: str,
        dry_run: bool = True
    ) -> CleanupAction:
        """ê³ ì•„ ë ˆì½”ë“œ ì •ë¦¬ SQL ìƒì„±"""
        if action == ActionType.DELETE:
            sql = f"""DELETE FROM `{schema}`.`{orphan.child_table}`
WHERE `{orphan.child_column}` NOT IN (
    SELECT `{orphan.parent_column}` FROM `{schema}`.`{orphan.parent_table}`
)
AND `{orphan.child_column}` IS NOT NULL"""
            description = f"{orphan.child_table}ì—ì„œ ê³ ì•„ ë ˆì½”ë“œ {orphan.orphan_count}ê°œ ì‚­ì œ"

        elif action == ActionType.SET_NULL:
            sql = f"""UPDATE `{schema}`.`{orphan.child_table}`
SET `{orphan.child_column}` = NULL
WHERE `{orphan.child_column}` NOT IN (
    SELECT `{orphan.parent_column}` FROM `{schema}`.`{orphan.parent_table}`
)
AND `{orphan.child_column}` IS NOT NULL"""
            description = f"{orphan.child_table}.{orphan.child_column}ì„ NULLë¡œ ì„¤ì • ({orphan.orphan_count}ê°œ)"

        else:
            sql = f"-- ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”: {orphan.child_table}.{orphan.child_column}"
            description = f"{orphan.child_table} ìˆ˜ë™ ê²€í†  í•„ìš”"

        return CleanupAction(
            action_type=action,
            table=orphan.child_table,
            description=description,
            sql=sql,
            affected_rows=orphan.orphan_count,
            dry_run=dry_run
        )

    def execute_cleanup(
        self,
        action: CleanupAction,
        dry_run: bool = True
    ) -> Tuple[bool, str, int]:
        """
        ì •ë¦¬ ì‘ì—… ì‹¤í–‰

        Args:
            action: ì‹¤í–‰í•  ì •ë¦¬ ì‘ì—…
            dry_run: Trueë©´ ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ì˜í–¥ë°›ëŠ” í–‰ ìˆ˜ë§Œ ë°˜í™˜

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€, ì˜í–¥ë°›ì€ í–‰ ìˆ˜)
        """
        if dry_run:
            # dry-run: ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ì˜í–¥ë°›ëŠ” í–‰ ìˆ˜ í™•ì¸
            self._log(f"ğŸ” [DRY-RUN] ì˜í–¥ ë¶„ì„: {action.table}")

            if action.action_type == ActionType.MANUAL:
                return True, "ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”", 0

            # COUNT ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì˜í–¥ë°›ëŠ” í–‰ ìˆ˜ í™•ì¸
            # DELETE/UPDATEì˜ WHERE ì ˆ ì¶”ì¶œ
            sql_upper = action.sql.upper()
            if 'WHERE' in sql_upper:
                where_idx = action.sql.upper().find('WHERE')
                where_clause = action.sql[where_idx:]

                # í…Œì´ë¸”ëª… ì¶”ì¶œ
                if action.action_type == ActionType.DELETE:
                    # DELETE FROM `schema`.`table` WHERE ...
                    count_sql = f"SELECT COUNT(*) as cnt FROM {action.sql.split('FROM')[1].split('WHERE')[0].strip()} {where_clause}"
                else:
                    # UPDATE `schema`.`table` SET ... WHERE ...
                    count_sql = f"SELECT COUNT(*) as cnt FROM {action.sql.split('UPDATE')[1].split('SET')[0].strip()} {where_clause}"

                result = self.connector.execute(count_sql)
                affected = result[0]['cnt'] if result else 0

                return True, f"[DRY-RUN] {affected}ê°œ í–‰ì´ ì˜í–¥ë°›ìŒ", affected

            return True, "[DRY-RUN] ì˜í–¥ ë¶„ì„ ì™„ë£Œ", action.affected_rows

        else:
            # ì‹¤ì œ ì‹¤í–‰
            self._log(f"ğŸ”§ ì‹¤í–‰ ì¤‘: {action.table}")

            try:
                with self.connector.connection.cursor() as cursor:
                    cursor.execute(action.sql)
                    affected = cursor.rowcount
                    self.connector.connection.commit()

                return True, f"âœ… {affected}ê°œ í–‰ ì²˜ë¦¬ë¨", affected

            except Exception as e:
                self.connector.connection.rollback()
                return False, f"âŒ ì˜¤ë¥˜: {str(e)}", 0

    def analyze_schema(
        self,
        schema: str,
        check_orphans: bool = True,
        check_charset: bool = True,
        check_keywords: bool = True,
        check_routines: bool = True,
        check_sql_mode: bool = True,
        check_auth_plugins: bool = True,
        check_zerofill: bool = True,
        check_float_precision: bool = True,
        check_fk_name_length: bool = True,
        check_invalid_dates: bool = True,
        check_year2: bool = True,
        check_deprecated_engines: bool = True,
        check_enum_empty: bool = True,
        check_timestamp_range: bool = True
    ) -> AnalysisResult:
        """
        ìŠ¤í‚¤ë§ˆ ì „ì²´ ë¶„ì„

        Args:
            schema: ë¶„ì„í•  ìŠ¤í‚¤ë§ˆëª…
            check_orphans: ê³ ì•„ ë ˆì½”ë“œ ê²€ì‚¬ ì—¬ë¶€
            check_charset: ë¬¸ìì…‹ ì´ìŠˆ ê²€ì‚¬ ì—¬ë¶€
            check_keywords: ì˜ˆì•½ì–´ ì¶©ëŒ ê²€ì‚¬ ì—¬ë¶€
            check_routines: ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ ê²€ì‚¬ ì—¬ë¶€
            check_sql_mode: SQL ëª¨ë“œ ê²€ì‚¬ ì—¬ë¶€
            check_auth_plugins: ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ê²€ì‚¬ ì—¬ë¶€
            check_zerofill: ZEROFILL ì†ì„± ê²€ì‚¬ ì—¬ë¶€
            check_float_precision: FLOAT(M,D) êµ¬ë¬¸ ê²€ì‚¬ ì—¬ë¶€
            check_fk_name_length: FK ì´ë¦„ ê¸¸ì´ ê²€ì‚¬ ì—¬ë¶€
            check_invalid_dates: 0000-00-00 ë‚ ì§œ ê²€ì‚¬ ì—¬ë¶€
            check_year2: YEAR(2) íƒ€ì… ê²€ì‚¬ ì—¬ë¶€
            check_deprecated_engines: deprecated ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ê²€ì‚¬ ì—¬ë¶€
            check_enum_empty: ENUM ë¹ˆ ë¬¸ìì—´ ê²€ì‚¬ ì—¬ë¶€
            check_timestamp_range: TIMESTAMP 2038ë…„ ë²”ìœ„ ê²€ì‚¬ ì—¬ë¶€

        Returns:
            AnalysisResult
        """
        from datetime import datetime

        self._log(f"ğŸ“Š ìŠ¤í‚¤ë§ˆ '{schema}' ë¶„ì„ ì‹œì‘...")

        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        tables = self.connector.get_tables(schema)
        fk_list = self.get_foreign_keys(schema)
        fk_tree = self.build_fk_tree(schema)

        self._log(f"  í…Œì´ë¸” ìˆ˜: {len(tables)}, FK ê´€ê³„: {len(fk_list)}")

        result = AnalysisResult(
            schema=schema,
            analyzed_at=datetime.now().isoformat(),
            total_tables=len(tables),
            total_fk_relations=len(fk_list),
            fk_tree=fk_tree
        )

        # ê³ ì•„ ë ˆì½”ë“œ ê²€ì‚¬
        if check_orphans and fk_list:
            self._log("ğŸ“Œ [1/14] ê³ ì•„ ë ˆì½”ë“œ ê²€ì‚¬ ì‹œì‘...")
            result.orphan_records = self.find_orphan_records(schema)
            self._log(f"âœ… [1/14] ê³ ì•„ ë ˆì½”ë“œ ê²€ì‚¬ ì™„ë£Œ (ë°œê²¬: {len(result.orphan_records)}ê±´)")

        # í˜¸í™˜ì„± ê²€ì‚¬ë“¤ (ê¸°ì¡´)
        if check_charset:
            self._log("ğŸ“Œ [2/14] ë¬¸ìì…‹ ì´ìŠˆ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_charset_issues(schema))

        if check_keywords:
            self._log("ğŸ“Œ [3/14] ì˜ˆì•½ì–´ ì¶©ëŒ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_reserved_keywords(schema))

        if check_routines:
            self._log("ğŸ“Œ [4/14] ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_deprecated_in_routines(schema))

        if check_sql_mode:
            self._log("ğŸ“Œ [5/14] SQL ëª¨ë“œ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_sql_modes())

        # MySQL 8.4 Upgrade Checker ê²€ì‚¬ë“¤
        if check_auth_plugins:
            self._log("ğŸ“Œ [6/14] ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_auth_plugins())

        if check_zerofill:
            self._log("ğŸ“Œ [7/14] ZEROFILL ì†ì„± ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_zerofill_columns(schema))

        if check_float_precision:
            self._log("ğŸ“Œ [8/14] FLOAT(M,D) êµ¬ë¬¸ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_float_precision(schema))

        if check_fk_name_length:
            self._log("ğŸ“Œ [9/14] FK ì´ë¦„ ê¸¸ì´ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_fk_name_length(schema))

        if check_invalid_dates:
            self._log("ğŸ“Œ [10/14] 0000-00-00 ë‚ ì§œê°’ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_invalid_date_values(schema))

        # ì¶”ê°€ í˜¸í™˜ì„± ê²€ì‚¬ë“¤
        if check_year2:
            self._log("ğŸ“Œ [11/14] YEAR(2) íƒ€ì… ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_year2_type(schema))

        if check_deprecated_engines:
            self._log("ğŸ“Œ [12/14] deprecated ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_deprecated_engines(schema))

        if check_enum_empty:
            self._log("ğŸ“Œ [13/14] ENUM ë¹ˆ ë¬¸ìì—´ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_enum_empty_value(schema))

        if check_timestamp_range:
            self._log("ğŸ“Œ [14/14] TIMESTAMP ë²”ìœ„ ê²€ì‚¬...")
            result.compatibility_issues.extend(self.check_timestamp_range(schema))

        # ì •ë¦¬ ì‘ì—… ìƒì„± (ê³ ì•„ ë ˆì½”ë“œì— ëŒ€í•´)
        for orphan in result.orphan_records:
            # ê¸°ë³¸ì ìœ¼ë¡œ DELETE ì‘ì—… ìƒì„± (dry-run)
            cleanup = self.generate_cleanup_sql(orphan, ActionType.DELETE, schema, dry_run=True)
            result.cleanup_actions.append(cleanup)

        self._log("âœ… ë¶„ì„ ì™„ë£Œ")
        self._log(f"  - ê³ ì•„ ë ˆì½”ë“œ: {len(result.orphan_records)}ê°œ FK ê´€ê³„ì—ì„œ ë°œê²¬")
        self._log(f"  - í˜¸í™˜ì„± ì´ìŠˆ: {len(result.compatibility_issues)}ê°œ")

        return result

    # ============================================================
    # MySQL 8.4 Upgrade Checker ê²€ì‚¬ ë©”ì„œë“œë“¤ (ì‹ ê·œ)
    # ============================================================

    def check_auth_plugins(self) -> List[CompatibilityIssue]:
        """mysql_native_password, sha256_password ì‚¬ìš©ì í™•ì¸"""
        self._log("ğŸ” ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘...")

        issues = []

        # ì‚¬ìš©ìë³„ ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì¡°íšŒ
        query = """
        SELECT User, Host, plugin
        FROM mysql.user
        WHERE plugin IN ('mysql_native_password', 'sha256_password', 'authentication_fido', 'authentication_fido_client')
        """
        try:
            users = self.connector.execute(query)

            for user in users:
                plugin = user['plugin']

                if plugin == 'mysql_native_password':
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                        severity="error",
                        location=f"'{user['User']}'@'{user['Host']}'",
                        description="mysql_native_password ì¸ì¦ ì‚¬ìš© (8.4ì—ì„œ ê¸°ë³¸ ë¹„í™œì„±í™”)",
                        suggestion="ALTER USER ... IDENTIFIED WITH caching_sha2_password"
                    ))
                elif plugin == 'sha256_password':
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                        severity="warning",
                        location=f"'{user['User']}'@'{user['Host']}'",
                        description="sha256_password ì¸ì¦ ì‚¬ìš© (deprecated)",
                        suggestion="ALTER USER ... IDENTIFIED WITH caching_sha2_password ê¶Œì¥"
                    ))
                elif plugin in ('authentication_fido', 'authentication_fido_client'):
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                        severity="error",
                        location=f"'{user['User']}'@'{user['Host']}'",
                        description=f"{plugin} í”ŒëŸ¬ê·¸ì¸ ì‚¬ìš© (8.4ì—ì„œ ì œê±°ë¨)",
                        suggestion="authentication_webauthn ë˜ëŠ” ë‹¤ë¥¸ ì¸ì¦ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ í•„ìš”"
                    ))

            if issues:
                self._log(f"  âš ï¸ ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì´ìŠˆ {len(issues)}ê°œ ë°œê²¬")
            else:
                self._log("  âœ… ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì •ìƒ")

        except Exception as e:
            self._log(f"  âš ï¸ ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

        return issues

    def check_zerofill_columns(self, schema: str) -> List[CompatibilityIssue]:
        """ZEROFILL ì†ì„± ì‚¬ìš© ì»¬ëŸ¼ í™•ì¸"""
        self._log("ğŸ” ZEROFILL ì†ì„± í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND COLUMN_TYPE LIKE '%%ZEROFILL%%'
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.ZEROFILL_USAGE,
                severity="warning",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description=f"ZEROFILL ì†ì„± ì‚¬ìš©: {col['COLUMN_TYPE']}",
                suggestion="ZEROFILLì€ deprecatedë¨, ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ LPAD() ë“±ìœ¼ë¡œ ì²˜ë¦¬ ê¶Œì¥"
            ))

        if issues:
            self._log(f"  âš ï¸ ZEROFILL ì‚¬ìš© {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ZEROFILL ì‚¬ìš© ì—†ìŒ")

        return issues

    def check_float_precision(self, schema: str) -> List[CompatibilityIssue]:
        """FLOAT(M,D), DOUBLE(M,D) êµ¬ë¬¸ í™•ì¸"""
        self._log("ğŸ” FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸ í™•ì¸ ì¤‘...")

        issues = []

        # FLOAT(M,D), DOUBLE(M,D) í˜•íƒœ í™•ì¸
        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE, DATA_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE IN ('float', 'double')
            AND COLUMN_TYPE REGEXP '^(float|double)\\\\([0-9]+,[0-9]+\\\\)'
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.FLOAT_PRECISION,
                severity="warning",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description=f"FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸ ì‚¬ìš©: {col['COLUMN_TYPE']}",
                suggestion="FLOAT(M,D) êµ¬ë¬¸ì€ deprecatedë¨, FLOAT ë˜ëŠ” DECIMAL(M,D) ì‚¬ìš© ê¶Œì¥"
            ))

        if issues:
            self._log(f"  âš ï¸ FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… FLOAT/DOUBLE êµ¬ë¬¸ ì •ìƒ")

        return issues

    def check_fk_name_length(self, schema: str) -> List[CompatibilityIssue]:
        """FK ì´ë¦„ 64ì ì´ˆê³¼ í™•ì¸"""
        self._log("ğŸ” FK ì´ë¦„ ê¸¸ì´ í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT CONSTRAINT_NAME, TABLE_NAME
        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS
        WHERE TABLE_SCHEMA = %s
            AND CONSTRAINT_TYPE = 'FOREIGN KEY'
            AND LENGTH(CONSTRAINT_NAME) > 64
        """
        fks = self.connector.execute(query, (schema,))

        for fk in fks:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.FK_NAME_LENGTH,
                severity="error",
                location=f"{schema}.{fk['TABLE_NAME']}.{fk['CONSTRAINT_NAME']}",
                description=f"FK ì´ë¦„ì´ 64ì ì´ˆê³¼: {len(fk['CONSTRAINT_NAME'])}ì",
                suggestion="FK ì´ë¦„ì„ 64ì ì´í•˜ë¡œ ë³€ê²½ í•„ìš” (8.4 ì œí•œ)"
            ))

        if issues:
            self._log(f"  âš ï¸ FK ì´ë¦„ ê¸¸ì´ ì´ˆê³¼ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… FK ì´ë¦„ ê¸¸ì´ ì •ìƒ")

        return issues

    def check_invalid_date_values(self, schema: str) -> List[CompatibilityIssue]:
        """0000-00-00 ë° ì˜ëª»ëœ ë‚ ì§œê°’ ê²€ì‚¬ (MySQL 8.4 í˜¸í™˜ì„±)

        MySQL 8.4ì—ì„œëŠ” NO_ZERO_DATE, NO_ZERO_IN_DATEê°€ ê¸°ë³¸ sql_modeì— í¬í•¨ë¨.
        0000-00-00 ë˜ëŠ” 2024-00-15 ê°™ì€ ë‚ ì§œëŠ” ë” ì´ìƒ í—ˆìš©ë˜ì§€ ì•ŠìŒ.
        """
        self._log("ğŸ” 0000-00-00 ë‚ ì§œê°’ í™•ì¸ ì¤‘...")

        issues = []

        # DATE, DATETIME, TIMESTAMP ì»¬ëŸ¼ ì¡°íšŒ
        col_query = """
        SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE, COLUMN_DEFAULT
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE IN ('date', 'datetime', 'timestamp')
        ORDER BY TABLE_NAME, COLUMN_NAME
        """
        columns = self.connector.execute(col_query, (schema,))

        if not columns:
            self._log("  âœ… DATE/DATETIME ì»¬ëŸ¼ ì—†ìŒ")
            return issues

        self._log(f"  DATE/DATETIME ì»¬ëŸ¼ {len(columns)}ê°œ ê²€ì‚¬ ì¤‘...")

        checked_count = 0
        for col in columns:
            table = col['TABLE_NAME']
            column = col['COLUMN_NAME']
            data_type = col['DATA_TYPE']

            try:
                # 0000-00-00 ê°’ ì¡´ì¬ í™•ì¸ (COUNTë¡œ ë¹ ë¥´ê²Œ)
                if data_type == 'date':
                    check_query = f"""
                    SELECT COUNT(*) as cnt
                    FROM `{schema}`.`{table}`
                    WHERE `{column}` = '0000-00-00'
                        OR (`{column}` IS NOT NULL
                            AND (MONTH(`{column}`) = 0 OR DAY(`{column}`) = 0))
                    """
                else:  # datetime, timestamp
                    check_query = f"""
                    SELECT COUNT(*) as cnt
                    FROM `{schema}`.`{table}`
                    WHERE `{column}` = '0000-00-00 00:00:00'
                        OR (`{column}` IS NOT NULL
                            AND (MONTH(`{column}`) = 0 OR DAY(`{column}`) = 0))
                    """

                result = self.connector.execute(check_query)
                invalid_count = result[0]['cnt'] if result else 0

                if invalid_count > 0:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.INVALID_DATE,
                        severity="error",
                        location=f"{schema}.{table}.{column}",
                        description=f"ì˜ëª»ëœ ë‚ ì§œê°’ {invalid_count:,}ê°œ ë°œê²¬ (0000-00-00 ë“±)",
                        suggestion="NULLë¡œ ë³€ê²½í•˜ê±°ë‚˜ ìœ íš¨í•œ ë‚ ì§œë¡œ ìˆ˜ì • í•„ìš” (8.4 NO_ZERO_DATE)",
                        table_name=table,
                        column_name=column,
                        fix_query=f"UPDATE `{schema}`.`{table}` SET `{column}` = NULL WHERE `{column}` = '0000-00-00' OR MONTH(`{column}`) = 0 OR DAY(`{column}`) = 0;"
                    ))
                    self._log(f"    âš ï¸ {table}.{column}: ì˜ëª»ëœ ë‚ ì§œ {invalid_count:,}ê°œ")

                checked_count += 1

            except Exception as e:
                # íŠ¹ì • í…Œì´ë¸” ê²€ì‚¬ ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ (ê¶Œí•œ ë“±)
                self._log(f"    â­ï¸ {table}.{column} ê²€ì‚¬ ìŠ¤í‚µ: {str(e)[:50]}")
                continue

        if issues:
            self._log(f"  âš ï¸ ì˜ëª»ëœ ë‚ ì§œê°’ {len(issues)}ê°œ ì»¬ëŸ¼ì—ì„œ ë°œê²¬")
        else:
            self._log(f"  âœ… ì˜ëª»ëœ ë‚ ì§œê°’ ì—†ìŒ ({checked_count}ê°œ ì»¬ëŸ¼ ê²€ì‚¬)")

        return issues

    def check_int_display_width(self, schema: str) -> List[CompatibilityIssue]:
        """INT(11) ë“± í‘œì‹œ ë„ˆë¹„ ì‚¬ìš© í™•ì¸ (TINYINT(1) ì œì™¸)"""
        self._log("ğŸ” INT í‘œì‹œ ë„ˆë¹„ í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE IN ('tinyint', 'smallint', 'mediumint', 'int', 'bigint')
            AND COLUMN_TYPE REGEXP '^(tinyint|smallint|mediumint|int|bigint)\\\\([0-9]+\\\\)'
            AND NOT (DATA_TYPE = 'tinyint' AND COLUMN_TYPE LIKE 'tinyint(1)%%')
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.INT_DISPLAY_WIDTH,
                severity="info",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description=f"INT í‘œì‹œ ë„ˆë¹„ ì‚¬ìš©: {col['COLUMN_TYPE']}",
                suggestion="í‘œì‹œ ë„ˆë¹„ëŠ” deprecatedë¨, 8.4ì—ì„œ ìë™ ë¬´ì‹œë¨ (ì˜í–¥ ìµœì†Œ)"
            ))

        if issues:
            self._log(f"  â„¹ï¸ INT í‘œì‹œ ë„ˆë¹„ {len(issues)}ê°œ ë°œê²¬ (ê²½ë¯¸)")
        else:
            self._log("  âœ… INT í‘œì‹œ ë„ˆë¹„ ì—†ìŒ")

        return issues

    def check_year2_type(self, schema: str) -> List[CompatibilityIssue]:
        """YEAR(2) íƒ€ì… ê²€ì‚¬ - MySQL 8.0ì—ì„œ ì œê±°ë¨"""
        self._log("ğŸ” YEAR(2) íƒ€ì… í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND COLUMN_TYPE = 'year(2)'
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.YEAR2_TYPE,
                severity="error",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description="YEAR(2) íƒ€ì… ì‚¬ìš© - MySQL 8.0ì—ì„œ ì œê±°ë¨",
                suggestion="YEAR(4) ë˜ëŠ” YEARë¡œ ë³€ê²½ í•„ìš”",
                table_name=col['TABLE_NAME'],
                column_name=col['COLUMN_NAME'],
                fix_query=f"ALTER TABLE `{schema}`.`{col['TABLE_NAME']}` MODIFY `{col['COLUMN_NAME']}` YEAR;"
            ))

        if issues:
            self._log(f"  âš ï¸ YEAR(2) íƒ€ì… {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… YEAR(2) íƒ€ì… ì—†ìŒ")

        return issues

    def check_deprecated_engines(self, schema: str) -> List[CompatibilityIssue]:
        """deprecated ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ê²€ì‚¬"""
        self._log("ğŸ” deprecated ìŠ¤í† ë¦¬ì§€ ì—”ì§„ í™•ì¸ ì¤‘...")

        issues = []

        # deprecated ì—”ì§„ ëª©ë¡
        deprecated_engines = {
            'MyISAM': ('warning', 'InnoDBë¡œ ë³€í™˜ ê¶Œì¥ (íŠ¸ëœì­ì…˜/FK ì§€ì›)'),
            'ARCHIVE': ('warning', 'InnoDBë¡œ ë³€í™˜ ê¶Œì¥'),
            'BLACKHOLE': ('info', 'í…ŒìŠ¤íŠ¸/ë³µì œìš© ì—”ì§„ - í•„ìš”ì‹œ ìœ ì§€'),
            'FEDERATED': ('warning', 'MySQL 8.4ì—ì„œ ì œê±° ì˜ˆì •'),
            'MERGE': ('error', 'MySQL 8.4ì—ì„œ ì œê±°ë¨ - InnoDB íŒŒí‹°ì…”ë‹ìœ¼ë¡œ ëŒ€ì²´'),
            'MEMORY': ('info', 'ì„ì‹œ í…Œì´ë¸”ìš©ìœ¼ë¡œëŠ” ìœ ì§€ ê°€ëŠ¥'),
        }

        query = """
        SELECT TABLE_NAME, ENGINE
        FROM INFORMATION_SCHEMA.TABLES
        WHERE TABLE_SCHEMA = %s
            AND TABLE_TYPE = 'BASE TABLE'
            AND ENGINE IS NOT NULL
        """
        tables = self.connector.execute(query, (schema,))

        for table in tables:
            engine = table['ENGINE']
            if engine in deprecated_engines:
                severity, suggestion = deprecated_engines[engine]
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.DEPRECATED_ENGINE,
                    severity=severity,
                    location=f"{schema}.{table['TABLE_NAME']}",
                    description=f"deprecated ìŠ¤í† ë¦¬ì§€ ì—”ì§„: {engine}",
                    suggestion=suggestion,
                    table_name=table['TABLE_NAME'],
                    fix_query=f"ALTER TABLE `{schema}`.`{table['TABLE_NAME']}` ENGINE=InnoDB;" if engine != 'MEMORY' else None
                ))

        if issues:
            self._log(f"  âš ï¸ deprecated ì—”ì§„ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… deprecated ì—”ì§„ ì—†ìŒ")

        return issues

    def check_enum_empty_value(self, schema: str) -> List[CompatibilityIssue]:
        """ENUM ë¹ˆ ë¬¸ìì—´('') ì •ì˜ ê²€ì‚¬ - 8.4ì—ì„œ ì—„ê²©í•´ì§"""
        self._log("ğŸ” ENUM ë¹ˆ ë¬¸ìì—´ í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE = 'enum'
            AND COLUMN_TYPE LIKE "%%''%%"
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.ENUM_EMPTY_VALUE,
                severity="warning",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description="ENUMì— ë¹ˆ ë¬¸ìì—´('') ì •ì˜ë¨",
                suggestion="ë¹ˆ ë¬¸ìì—´ ëŒ€ì‹  NULL í—ˆìš© ë˜ëŠ” ëª…ì‹œì  ê°’ ì‚¬ìš© ê¶Œì¥",
                table_name=col['TABLE_NAME'],
                column_name=col['COLUMN_NAME']
            ))

        if issues:
            self._log(f"  âš ï¸ ENUM ë¹ˆ ë¬¸ìì—´ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ENUM ë¹ˆ ë¬¸ìì—´ ì—†ìŒ")

        return issues

    def check_timestamp_range(self, schema: str) -> List[CompatibilityIssue]:
        """TIMESTAMP ë²”ìœ„ ì´ˆê³¼ ë°ì´í„° ê²€ì‚¬ (2038ë…„ ë¬¸ì œ)"""
        self._log("ğŸ” TIMESTAMP ë²”ìœ„ í™•ì¸ ì¤‘...")

        issues = []

        # TIMESTAMP ì»¬ëŸ¼ ì¡°íšŒ
        col_query = """
        SELECT TABLE_NAME, COLUMN_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE = 'timestamp'
        """
        columns = self.connector.execute(col_query, (schema,))

        for col in columns:
            table = col['TABLE_NAME']
            column = col['COLUMN_NAME']

            try:
                # 2038-01-19 ì´í›„ ë°ì´í„° í™•ì¸
                check_query = f"""
                SELECT COUNT(*) as cnt
                FROM `{schema}`.`{table}`
                WHERE `{column}` > '2038-01-19 03:14:07'
                """
                result = self.connector.execute(check_query)
                count = result[0]['cnt'] if result else 0

                if count > 0:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.TIMESTAMP_RANGE,
                        severity="error",
                        location=f"{schema}.{table}.{column}",
                        description=f"TIMESTAMP ë²”ìœ„ ì´ˆê³¼ ë°ì´í„° {count:,}ê°œ (2038ë…„ ë¬¸ì œ)",
                        suggestion="DATETIMEìœ¼ë¡œ íƒ€ì… ë³€ê²½ ê¶Œì¥",
                        table_name=table,
                        column_name=column,
                        fix_query=f"ALTER TABLE `{schema}`.`{table}` MODIFY `{column}` DATETIME;"
                    ))

            except Exception as e:
                self._log(f"    â­ï¸ {table}.{column} TIMESTAMP ê²€ì‚¬ ìŠ¤í‚µ: {str(e)[:80]}")
                continue

        if issues:
            self._log(f"  âš ï¸ TIMESTAMP ë²”ìœ„ ì´ˆê³¼ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log(f"  âœ… TIMESTAMP ë²”ìœ„ ì •ìƒ")

        return issues

    def get_fk_visualization(self, schema: str) -> str:
        """FK ê´€ê³„ë¥¼ íŠ¸ë¦¬ í˜•íƒœë¡œ ì‹œê°í™”"""
        fk_tree = self.build_fk_tree(schema)

        if not fk_tree:
            return "FK ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines = ["FK ê´€ê³„ íŠ¸ë¦¬:", ""]

        # ë£¨íŠ¸ í…Œì´ë¸” ì°¾ê¸° (ë‹¤ë¥¸ í…Œì´ë¸”ì˜ ìì‹ì´ ì•„ë‹Œ í…Œì´ë¸”)
        all_children = set()
        for children in fk_tree.values():
            all_children.update(children)

        root_tables = set(fk_tree.keys()) - all_children

        def print_tree(table: str, prefix: str = "", is_last: bool = True, visited: set = None):
            if visited is None:
                visited = set()

            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "

            # ìˆœí™˜ ì°¸ì¡° ê°ì§€
            if table in visited:
                lines.append(f"{prefix}{connector}ğŸ”„ {table} (ìˆœí™˜ ì°¸ì¡°)")
                return

            lines.append(f"{prefix}{connector}{table}")

            if table in fk_tree:
                children = fk_tree[table]
                child_prefix = prefix + ("    " if is_last else "â”‚   ")
                for i, child in enumerate(children):
                    print_tree(child, child_prefix, i == len(children) - 1, visited | {table})

        for i, root in enumerate(sorted(root_tables)):
            print_tree(root, "", i == len(root_tables) - 1, set())

        return "\n".join(lines)


# ============================================================
# ë¤í”„ íŒŒì¼ ë¶„ì„ê¸° (Task 3)
# ============================================================

@dataclass
class DumpAnalysisResult:
    """ë¤í”„ íŒŒì¼ ë¶„ì„ ê²°ê³¼"""
    dump_path: str
    analyzed_at: str
    total_sql_files: int
    total_tsv_files: int
    compatibility_issues: List[CompatibilityIssue] = field(default_factory=list)


class DumpFileAnalyzer:
    """
    mysqlsh ë¤í”„ íŒŒì¼ ë¶„ì„ê¸°

    ë¤í”„ í´ë”ì˜ SQL/TSV íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ MySQL 8.4 í˜¸í™˜ì„± ì´ìŠˆë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self._progress_callback: Optional[Callable[[str], None]] = None
        self._issue_callback: Optional[Callable[[CompatibilityIssue], None]] = None

    def set_progress_callback(self, callback: Callable[[str], None]):
        """ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •"""
        self._progress_callback = callback

    def set_issue_callback(self, callback: Callable[[CompatibilityIssue], None]):
        """ì´ìŠˆ ë°œê²¬ ì‹œ ì½œë°± ì„¤ì •"""
        self._issue_callback = callback

    def _log(self, message: str):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        if self._progress_callback:
            self._progress_callback(message)

    def _report_issue(self, issue: CompatibilityIssue):
        """ì´ìŠˆ ë°œê²¬ ì‹œ ì½œë°± í˜¸ì¶œ"""
        if self._issue_callback:
            self._issue_callback(issue)

    def analyze_dump_folder(self, dump_path: str) -> DumpAnalysisResult:
        """
        ë¤í”„ í´ë” ì „ì²´ ë¶„ì„

        Args:
            dump_path: mysqlsh ë¤í”„ í´ë” ê²½ë¡œ

        Returns:
            DumpAnalysisResult
        """
        from datetime import datetime

        path = Path(dump_path)
        if not path.exists():
            raise FileNotFoundError(f"ë¤í”„ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dump_path}")

        self._log(f"ğŸ” ë¤í”„ í´ë” ë¶„ì„ ì‹œì‘: {dump_path}")

        issues: List[CompatibilityIssue] = []

        # SQL íŒŒì¼ ëª©ë¡
        sql_files = list(path.glob("*.sql"))
        tsv_files = list(path.glob("*.tsv")) + list(path.glob("*.tsv.zst"))

        self._log(f"  SQL íŒŒì¼: {len(sql_files)}ê°œ, ë°ì´í„° íŒŒì¼: {len(tsv_files)}ê°œ")

        # SQL íŒŒì¼ ë¶„ì„
        for i, sql_file in enumerate(sql_files, 1):
            self._log(f"  [{i}/{len(sql_files)}] {sql_file.name} ë¶„ì„ ì¤‘...")
            file_issues = self._analyze_sql_file(sql_file)
            issues.extend(file_issues)

            # ì‹¤ì‹œê°„ ì´ìŠˆ ì½œë°±
            for issue in file_issues:
                self._report_issue(issue)

        # TSV ë°ì´í„° íŒŒì¼ ë¶„ì„ (0000-00-00 ë‚ ì§œ ë“±)
        # ì••ì¶•ë˜ì§€ ì•Šì€ TSV íŒŒì¼ë§Œ ë¶„ì„ (ì••ì¶• íŒŒì¼ì€ ë„ˆë¬´ ëŠë¦¼)
        uncompressed_tsv = [f for f in tsv_files if not str(f).endswith('.zst')]
        if uncompressed_tsv:
            for i, tsv_file in enumerate(uncompressed_tsv, 1):
                self._log(f"  [{i}/{len(uncompressed_tsv)}] {tsv_file.name} ë¶„ì„ ì¤‘...")
                file_issues = self._analyze_tsv_file(tsv_file)
                issues.extend(file_issues)

                for issue in file_issues:
                    self._report_issue(issue)

        # ê²°ê³¼ ìƒì„±
        result = DumpAnalysisResult(
            dump_path=str(dump_path),
            analyzed_at=datetime.now().isoformat(),
            total_sql_files=len(sql_files),
            total_tsv_files=len(tsv_files),
            compatibility_issues=issues
        )

        # ìš”ì•½
        error_count = sum(1 for i in issues if i.severity == "error")
        warning_count = sum(1 for i in issues if i.severity == "warning")

        self._log("âœ… ë¤í”„ ë¶„ì„ ì™„ë£Œ")
        self._log(f"  - ì˜¤ë¥˜: {error_count}ê°œ")
        self._log(f"  - ê²½ê³ : {warning_count}ê°œ")

        return result

    def _analyze_sql_file(self, file_path: Path) -> List[CompatibilityIssue]:
        """
        SQL íŒŒì¼ ë¶„ì„ - ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì‚¬

        Args:
            file_path: SQL íŒŒì¼ ê²½ë¡œ

        Returns:
            ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
        """
        issues = []

        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')

            # 1. ZEROFILL ì†ì„± ê²€ì‚¬
            for match in ZEROFILL_PATTERN.finditer(content):
                # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í…Œì´ë¸”/ì»¬ëŸ¼ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                line_start = content.rfind('\n', 0, match.start()) + 1
                line_end = content.find('\n', match.end())
                line = content[line_start:line_end]

                issues.append(CompatibilityIssue(
                    issue_type=IssueType.ZEROFILL_USAGE,
                    severity="warning",
                    location=f"{file_path.name}",
                    description=f"ZEROFILL ì†ì„± ì‚¬ìš©: {line.strip()[:80]}...",
                    suggestion="ZEROFILLì€ deprecatedë¨"
                ))

            # 2. FLOAT(M,D), DOUBLE(M,D) êµ¬ë¬¸ ê²€ì‚¬
            for match in FLOAT_PRECISION_PATTERN.finditer(content):
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.FLOAT_PRECISION,
                    severity="warning",
                    location=f"{file_path.name}",
                    description=f"FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸: {match.group(0)}",
                    suggestion="FLOAT(M,D) êµ¬ë¬¸ì€ deprecatedë¨"
                ))

            # 3. FK ì´ë¦„ 64ì ì´ˆê³¼ ê²€ì‚¬
            for match in FK_NAME_LENGTH_PATTERN.finditer(content):
                fk_name = match.group(1)
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.FK_NAME_LENGTH,
                    severity="error",
                    location=f"{file_path.name}",
                    description=f"FK ì´ë¦„ 64ì ì´ˆê³¼: {fk_name[:30]}... ({len(fk_name)}ì)",
                    suggestion="FK ì´ë¦„ì„ 64ì ì´í•˜ë¡œ ë³€ê²½ í•„ìš”"
                ))

            # 4. ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ê²€ì‚¬
            for match in AUTH_PLUGIN_PATTERN.finditer(content):
                plugin = match.group(1).lower()
                # removed(fido ê³„ì—´)=error, disabled(native)=error, deprecated(sha256)=warning
                if plugin in ('authentication_fido', 'authentication_fido_client'):
                    severity = "error"
                    desc = f"{plugin} í”ŒëŸ¬ê·¸ì¸ ì‚¬ìš© (8.4ì—ì„œ ì œê±°ë¨)"
                elif plugin == 'mysql_native_password':
                    severity = "error"
                    desc = f"{plugin} ì¸ì¦ ì‚¬ìš© (8.4ì—ì„œ ê¸°ë³¸ ë¹„í™œì„±í™”)"
                else:
                    severity = "warning"
                    desc = f"{plugin} ì¸ì¦ ì‚¬ìš© (deprecated)"
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                    severity=severity,
                    location=f"{file_path.name}",
                    description=desc,
                    suggestion="caching_sha2_password ì‚¬ìš© ê¶Œì¥"
                ))

            # 5. FTS_ í…Œì´ë¸”ëª… ê²€ì‚¬
            for match in FTS_TABLE_PREFIX_PATTERN.finditer(content):
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.FTS_TABLE_PREFIX,
                    severity="error",
                    location=f"{file_path.name}",
                    description="FTS_ ì ‘ë‘ì‚¬ í…Œì´ë¸”ëª… (ë‚´ë¶€ ì˜ˆì•½ì–´)",
                    suggestion="FTS_ ì ‘ë‘ì‚¬ëŠ” ë‚´ë¶€ ì „ë¬¸ ê²€ìƒ‰ìš©ìœ¼ë¡œ ì˜ˆì•½ë¨, í…Œì´ë¸”ëª… ë³€ê²½ í•„ìš”"
                ))

            # 6. SUPER ê¶Œí•œ ê²€ì‚¬
            for match in SUPER_PRIVILEGE_PATTERN.finditer(content):
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.SUPER_PRIVILEGE,
                    severity="warning",
                    location=f"{file_path.name}",
                    description="SUPER ê¶Œí•œ ì‚¬ìš© (deprecated)",
                    suggestion="ë™ì  ê¶Œí•œ (BINLOG_ADMIN, CONNECTION_ADMIN ë“±)ìœ¼ë¡œ ì„¸ë¶„í™” ê¶Œì¥"
                ))

            # 7. ì œê±°ëœ ì‹œìŠ¤í…œ ë³€ìˆ˜ ì‚¬ìš© ê²€ì‚¬
            for match in SYS_VAR_USAGE_PATTERN.finditer(content):
                var_name = match.group(1)
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.REMOVED_SYS_VAR,
                    severity="error",
                    location=f"{file_path.name}",
                    description=f"ì œê±°ëœ ì‹œìŠ¤í…œ ë³€ìˆ˜ ì‚¬ìš©: {var_name}",
                    suggestion=f"'{var_name}'ì€ 8.4ì—ì„œ ì œê±°ë¨, ëŒ€ì²´ ë°©ë²• í™•ì¸ í•„ìš”"
                ))

            # 8. ì˜ˆì•½ì–´ ì¶©ëŒ (í…Œì´ë¸”/ì»¬ëŸ¼ ì´ë¦„) - CREATE TABLE ë¬¸ì—ì„œ
            table_pattern = re.compile(
                r'CREATE\s+TABLE\s+`?(\w+)`?\s*\(',
                re.IGNORECASE
            )
            column_pattern = re.compile(
                r'`(\w+)`\s+(?:INT|VARCHAR|TEXT|DATE|DECIMAL|FLOAT|DOUBLE|CHAR|BLOB|ENUM|SET)',
                re.IGNORECASE
            )

            keywords_upper = set(k.upper() for k in MigrationAnalyzer.NEW_RESERVED_KEYWORDS)

            for match in table_pattern.finditer(content):
                table_name = match.group(1)
                if table_name.upper() in keywords_upper:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.RESERVED_KEYWORD,
                        severity="error",
                        location=f"{file_path.name}",
                        description=f"í…Œì´ë¸”ëª… '{table_name}'ì´ ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                        suggestion="í…Œì´ë¸”ëª… ë³€ê²½ ë˜ëŠ” ë°±í‹±(`) ì‚¬ìš© í•„ìš”"
                    ))

            for match in column_pattern.finditer(content):
                column_name = match.group(1)
                if column_name.upper() in keywords_upper:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.RESERVED_KEYWORD,
                        severity="warning",
                        location=f"{file_path.name}",
                        description=f"ì»¬ëŸ¼ëª… '{column_name}'ì´ ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                        suggestion="ì»¬ëŸ¼ ì°¸ì¡° ì‹œ ë°±í‹±(`) ì‚¬ìš© í•„ìš”"
                    ))

        except Exception as e:
            self._log(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        return issues

    def _analyze_tsv_file(self, file_path: Path) -> List[CompatibilityIssue]:
        """
        TSV ë°ì´í„° íŒŒì¼ ë¶„ì„ - ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬

        Args:
            file_path: TSV íŒŒì¼ ê²½ë¡œ

        Returns:
            ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
        """
        issues = []
        invalid_date_count = 0

        try:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìƒ˜í”Œë§
            max_lines = 10000
            line_count = 0

            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                for line in f:
                    line_count += 1
                    if line_count > max_lines:
                        break

                    # 0000-00-00 ë‚ ì§œ ê²€ì‚¬
                    if INVALID_DATE_PATTERN.search(line) or INVALID_DATETIME_PATTERN.search(line):
                        invalid_date_count += 1

            if invalid_date_count > 0:
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.INVALID_DATE,
                    severity="error",
                    location=f"{file_path.name}",
                    description=f"ì˜ëª»ëœ ë‚ ì§œ ê°’ ë°œê²¬: {invalid_date_count}ê°œ í–‰ (0000-00-00)",
                    suggestion="NO_ZERO_DATE SQL ëª¨ë“œ í™œì„±í™” ì‹œ ì˜¤ë¥˜ ë°œìƒ, ìœ íš¨í•œ ë‚ ì§œë¡œ ë³€í™˜ í•„ìš”"
                ))

        except Exception as e:
            self._log(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        return issues

    def quick_scan(self, dump_path: str) -> Tuple[int, int, int]:
        """
        ë¹ ë¥¸ ìŠ¤ìº” - ì´ìŠˆ ê°œìˆ˜ë§Œ ë°˜í™˜

        Args:
            dump_path: ë¤í”„ í´ë” ê²½ë¡œ

        Returns:
            (ì˜¤ë¥˜ ìˆ˜, ê²½ê³  ìˆ˜, ì •ë³´ ìˆ˜)
        """
        try:
            result = self.analyze_dump_folder(dump_path)
            error_count = sum(1 for i in result.compatibility_issues if i.severity == "error")
            warning_count = sum(1 for i in result.compatibility_issues if i.severity == "warning")
            info_count = sum(1 for i in result.compatibility_issues if i.severity == "info")
            return error_count, warning_count, info_count
        except Exception as e:
            self._log(f"  âš ï¸ ìš”ì•½ ì¹´ìš´íŠ¸ ì˜¤ë¥˜: {str(e)[:80]}")
            return 0, 0, 0


# ============================================================
# 2-Pass ë¶„ì„ê¸° (Task 5)
# ============================================================

@dataclass
class TableIndexInfo:
    """í…Œì´ë¸” ì¸ë±ìŠ¤ ì •ë³´"""
    schema: Optional[str]
    table_name: str
    index_name: str
    columns: List[str]
    is_unique: bool
    is_primary: bool

    def covers_columns(self, cols: List[str]) -> bool:
        """ì£¼ì–´ì§„ ì»¬ëŸ¼ë“¤ì´ ì´ ì¸ë±ìŠ¤ë¡œ ì»¤ë²„ë˜ëŠ”ì§€ í™•ì¸"""
        cols_lower = [c.lower() for c in cols]
        idx_cols_lower = [c.lower() for c in self.columns[:len(cols)]]
        return cols_lower == idx_cols_lower


@dataclass
class TableCharsetInfo:
    """í…Œì´ë¸” charset ì •ë³´"""
    schema: Optional[str]
    table_name: str
    charset: str
    collation: Optional[str] = None
    column_charsets: Dict[str, str] = field(default_factory=dict)


@dataclass
class PendingFKCheck:
    """ì§€ì—°ëœ FK ê²€ì¦ ì •ë³´"""
    fk_name: str
    source_schema: Optional[str]
    source_table: str
    source_columns: List[str]
    ref_table: str
    ref_columns: List[str]
    location: str
    line_number: Optional[int] = None


class TwoPassAnalyzer:
    """2-Pass ë¤í”„ íŒŒì¼ ë¶„ì„ê¸°

    connectorê°€ ì—†ìœ¼ë©´ content-based ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ê³ ,
    connectorê°€ ì£¼ì…ë˜ë©´ live DB ê¸°ë°˜ ê·œì¹™ ê²€ì‚¬ë„ í™œì„±í™”ë©ë‹ˆë‹¤.
    """

    def __init__(self, connector: Optional[MySQLConnector] = None):
        # Pass 1 ìˆ˜ì§‘ ë°ì´í„°
        self.table_indexes: Dict[str, List[TableIndexInfo]] = {}
        self.table_charsets: Dict[str, TableCharsetInfo] = {}
        self.known_tables: Set[str] = set()

        # Pass 2 ìˆ˜ì§‘ ë°ì´í„°
        self.pending_fk_checks: List[PendingFKCheck] = []

        # DB ì»¤ë„¥í„° (ì˜µì…˜ - live DB ê·œì¹™ ê²€ì‚¬ìš©)
        self.connector = connector

        # íŒŒì„œ (ì˜µì…˜)
        self.sql_parser = None
        if PARSERS_AVAILABLE:
            self.sql_parser = SQLParser()

        # ê·œì¹™ ëª¨ë“ˆ (ì˜µì…˜) - connectorê°€ ìˆìœ¼ë©´ live DB ê²€ì‚¬ë„ í™œì„±í™”
        self.data_rules = None
        self.schema_rules = None
        self.storage_rules = None
        if RULES_AVAILABLE:
            self.data_rules = DataIntegrityRules(connector=connector)
            self.schema_rules = SchemaRules(connector=connector)
            self.storage_rules = StorageRules(connector=connector)

        # Fix Query ìƒì„±ê¸° (ì˜µì…˜)
        self.fix_generator = None
        if FIX_GENERATOR_AVAILABLE:
            self.fix_generator = FixQueryGenerator()

        # ì½œë°±
        self._progress_callback: Optional[Callable[[str], None]] = None
        self._issue_callback: Optional[Callable[[CompatibilityIssue], None]] = None

    def set_callbacks(
        self,
        progress_callback: Optional[Callable[[str], None]] = None,
        issue_callback: Optional[Callable[[CompatibilityIssue], None]] = None
    ):
        """ì½œë°± ì„¤ì •"""
        self._progress_callback = progress_callback
        self._issue_callback = issue_callback

        # ê·œì¹™ ëª¨ë“ˆì—ë„ ì½œë°± ì „íŒŒ
        if self.data_rules and progress_callback:
            self.data_rules.set_progress_callback(progress_callback)
        if self.schema_rules and progress_callback:
            self.schema_rules.set_progress_callback(progress_callback)
        if self.storage_rules and progress_callback:
            self.storage_rules.set_progress_callback(progress_callback)

    def _log(self, message: str):
        if self._progress_callback:
            self._progress_callback(message)

    def _report_issue(self, issue: CompatibilityIssue):
        # Fix Query ìƒì„±
        if self.fix_generator:
            issue = self.fix_generator.generate(issue)

        if self._issue_callback:
            self._issue_callback(issue)

    def clear_state(self):
        """ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”"""
        self.table_indexes.clear()
        self.table_charsets.clear()
        self.known_tables.clear()
        self.pending_fk_checks.clear()

    def _make_table_key(self, schema: Optional[str], table: str) -> str:
        """í…Œì´ë¸” ì¡°íšŒ í‚¤ ìƒì„±"""
        if schema:
            return f"{schema.lower()}.{table.lower()}"
        return table.lower()

    def _register_known_table(self, schema: Optional[str], table_name: str):
        """ì•Œë ¤ì§„ í…Œì´ë¸” ë“±ë¡"""
        key = self._make_table_key(schema, table_name)
        self.known_tables.add(key)

    # ================================================================
    # Pass 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    # ================================================================
    def pass1_collect_metadata(self, files: List[Path]):
        """Pass 1: í…Œì´ë¸” ì¸ë±ìŠ¤ ë° charset ì •ë³´ ìˆ˜ì§‘"""
        self._log("ğŸ“Š Pass 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        for file_path in files:
            if not file_path.suffix.lower() == '.sql':
                continue

            self._log(f"  ìˆ˜ì§‘ ì¤‘: {file_path.name}")

            try:
                content = file_path.read_text(encoding='utf-8', errors='replace')

                # CREATE TABLE ë¬¸ ì¶”ì¶œ ë° íŒŒì‹±
                if self.sql_parser:
                    for sql in self.sql_parser.extract_create_table_statements(content):
                        parsed = self.sql_parser.parse_table(sql)
                        if parsed:
                            self._collect_table_indexes(parsed)
                            self._collect_table_charset(parsed)
                            self._register_known_table(parsed.schema, parsed.name)
                else:
                    # íŒŒì„œ ì—†ì´ ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œ í…Œì´ë¸”ëª…ë§Œ ìˆ˜ì§‘
                    table_pattern = re.compile(
                        r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?'
                        r'(?:`?(\w+)`?\.)?`?(\w+)`?',
                        re.IGNORECASE
                    )
                    for match in table_pattern.finditer(content):
                        schema = match.group(1)
                        table_name = match.group(2)
                        self._register_known_table(schema, table_name)

            except Exception as e:
                self._log(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        self._log(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: í…Œì´ë¸” {len(self.known_tables)}ê°œ")

    def _collect_table_indexes(self, table: 'ParsedTable'):
        """í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        key = self._make_table_key(table.schema, table.name)

        if key not in self.table_indexes:
            self.table_indexes[key] = []

        for idx in table.indexes:
            self.table_indexes[key].append(TableIndexInfo(
                schema=table.schema,
                table_name=table.name,
                index_name=idx.name,
                columns=idx.columns,
                is_unique=idx.is_unique,
                is_primary=idx.is_primary
            ))

    def _collect_table_charset(self, table: 'ParsedTable'):
        """í…Œì´ë¸”ì˜ charset ì •ë³´ ìˆ˜ì§‘"""
        key = self._make_table_key(table.schema, table.name)

        column_charsets = {}
        for col in table.columns:
            if col.charset:
                column_charsets[col.name] = col.charset

        self.table_charsets[key] = TableCharsetInfo(
            schema=table.schema,
            table_name=table.name,
            charset=table.charset or 'utf8mb4',
            collation=table.collation,
            column_charsets=column_charsets
        )

    # ================================================================
    # Pass 2: ì „ì²´ ë¶„ì„ + FK ìˆ˜ì§‘
    # ================================================================
    def pass2_full_analysis(self, files: List[Path]) -> List[CompatibilityIssue]:
        """Pass 2: ì „ì²´ ë¶„ì„ ë° FK ì°¸ì¡° ìˆ˜ì§‘"""
        self._log("ğŸ” Pass 2: ì „ì²´ ë¶„ì„ ì¤‘...")

        all_issues = []

        for file_path in files:
            self._log(f"  ë¶„ì„ ì¤‘: {file_path.name}")

            try:
                if file_path.suffix.lower() == '.sql':
                    issues = self._analyze_sql_file_pass2(file_path)
                elif file_path.suffix.lower() in ('.tsv', '.txt'):
                    issues = self._analyze_data_file_pass2(file_path)
                else:
                    continue

                all_issues.extend(issues)

                # ì‹¤ì‹œê°„ ì´ìŠˆ ë¦¬í¬íŠ¸
                for issue in issues:
                    self._report_issue(issue)

            except Exception as e:
                self._log(f"  âš ï¸ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        return all_issues

    def _analyze_sql_file_pass2(self, file_path: Path) -> List[CompatibilityIssue]:
        """SQL íŒŒì¼ ë¶„ì„ (Pass 2)"""
        issues = []
        content = file_path.read_text(encoding='utf-8', errors='replace')
        location = file_path.name

        # ê·œì¹™ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ ì‹œ í™•ì¥ ê²€ì‚¬
        if self.schema_rules:
            issues.extend(self.schema_rules.check_all_sql_content(content, location))

        if self.storage_rules:
            issues.extend(self.storage_rules.check_all_sql_content(content, location))

        if self.data_rules:
            issues.extend(self.data_rules.check_all_sql_content(content, location))

        # FK ì°¸ì¡° ìˆ˜ì§‘ (í¬ë¡œìŠ¤ ê²€ì¦ìš©)
        if self.sql_parser:
            for sql in self.sql_parser.extract_create_table_statements(content):
                parsed = self.sql_parser.parse_table(sql)
                if parsed:
                    self._collect_fk_references(parsed, location)

        return issues

    def _analyze_data_file_pass2(self, file_path: Path) -> List[CompatibilityIssue]:
        """ë°ì´í„° íŒŒì¼ ë¶„ì„ (Pass 2)"""
        issues = []

        if self.data_rules:
            issues.extend(self.data_rules.check_all_data_file(file_path))

        return issues

    def _collect_fk_references(self, table: 'ParsedTable', location: str):
        """í…Œì´ë¸”ì˜ FK ì°¸ì¡° ì •ë³´ ìˆ˜ì§‘"""
        for fk in table.foreign_keys:
            self.pending_fk_checks.append(PendingFKCheck(
                fk_name=fk.name,
                source_schema=table.schema,
                source_table=table.name,
                source_columns=fk.columns,
                ref_table=fk.ref_table,
                ref_columns=fk.ref_columns,
                location=location
            ))

    # ================================================================
    # Pass 2.5: í¬ë¡œìŠ¤ ê²€ì¦
    # ================================================================
    def pass2_5_cross_validate(self) -> List[CompatibilityIssue]:
        """Pass 2.5: FK í¬ë¡œìŠ¤ ê²€ì¦"""
        self._log("âœ… Pass 2.5: FK í¬ë¡œìŠ¤ ê²€ì¦ ì¤‘...")

        issues = []

        for fk in self.pending_fk_checks:
            # FK ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            ref_key = self._make_table_key(fk.source_schema, fk.ref_table)

            if ref_key not in self.known_tables:
                issue = CompatibilityIssue(
                    issue_type=IssueType.FK_REF_NOT_FOUND,
                    severity="error",
                    location=fk.location,
                    description=f"FK '{fk.fk_name}': ì°¸ì¡° í…Œì´ë¸” '{fk.ref_table}' ë¯¸ì¡´ì¬",
                    suggestion="ì°¸ì¡° í…Œì´ë¸”ì´ ë¤í”„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”",
                    table_name=fk.source_table
                )
                issues.append(issue)
                self._report_issue(issue)
                continue

            # FK ì°¸ì¡° ì»¬ëŸ¼ì´ PK/UNIQUE ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
            if not self._is_valid_fk_reference(fk):
                issue = CompatibilityIssue(
                    issue_type=IssueType.FK_NON_UNIQUE_REF,
                    severity="error",
                    location=fk.location,
                    description=f"FK '{fk.fk_name}': ì°¸ì¡° ì»¬ëŸ¼ì´ PK/UNIQUE ì•„ë‹˜",
                    suggestion=f"'{fk.ref_table}.{', '.join(fk.ref_columns)}'ì— UNIQUE ì¸ë±ìŠ¤ ì¶”ê°€ í•„ìš”",
                    table_name=fk.source_table
                )
                issues.append(issue)
                self._report_issue(issue)

        self._log(f"  âœ… í¬ë¡œìŠ¤ ê²€ì¦ ì™„ë£Œ: ì´ìŠˆ {len(issues)}ê°œ")
        return issues

    def _is_valid_fk_reference(self, fk: PendingFKCheck) -> bool:
        """FK ì°¸ì¡°ê°€ ìœ íš¨í•œì§€ í™•ì¸ (PK ë˜ëŠ” UNIQUE)"""
        ref_key = self._make_table_key(fk.source_schema, fk.ref_table)
        indexes = self.table_indexes.get(ref_key, [])

        for idx in indexes:
            if idx.is_primary or idx.is_unique:
                if idx.covers_columns(fk.ref_columns):
                    return True

        return False

    # ================================================================
    # í†µí•© ë¶„ì„ ë©”ì„œë“œ
    # ================================================================
    def analyze_dump_folder(self, dump_path: str) -> DumpAnalysisResult:
        """ë¤í”„ í´ë” 2-Pass ë¶„ì„"""
        from datetime import datetime

        self.clear_state()

        path = Path(dump_path)
        if not path.exists():
            raise FileNotFoundError(f"ë¤í”„ í´ë” ì—†ìŒ: {dump_path}")

        self._log(f"ğŸ” 2-Pass ë¶„ì„ ì‹œì‘: {dump_path}")

        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        sql_files = list(path.glob("*.sql"))
        data_files = [f for f in path.glob("*.tsv") if not str(f).endswith('.zst')]

        self._log(f"  SQL: {len(sql_files)}ê°œ, ë°ì´í„°: {len(data_files)}ê°œ")

        # Pass 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        self.pass1_collect_metadata(sql_files)

        # Pass 2: ì „ì²´ ë¶„ì„
        all_issues = self.pass2_full_analysis(sql_files + data_files)

        # Pass 2.5: í¬ë¡œìŠ¤ ê²€ì¦
        cross_issues = self.pass2_5_cross_validate()
        all_issues.extend(cross_issues)

        # ìš”ì•½
        error_count = sum(1 for i in all_issues if i.severity == "error")
        warning_count = sum(1 for i in all_issues if i.severity == "warning")

        self._log("âœ… 2-Pass ë¶„ì„ ì™„ë£Œ")
        self._log(f"  - ì˜¤ë¥˜: {error_count}ê°œ")
        self._log(f"  - ê²½ê³ : {warning_count}ê°œ")

        # ê²°ê³¼ ìƒì„±
        return DumpAnalysisResult(
            dump_path=str(dump_path),
            analyzed_at=datetime.now().isoformat(),
            total_sql_files=len(sql_files),
            total_tsv_files=len(data_files),
            compatibility_issues=all_issues
        )


# ============================================================
# í™•ì¥ DumpFileAnalyzer (2-Pass ì§€ì›)
# ============================================================

class EnhancedDumpFileAnalyzer(DumpFileAnalyzer):
    """í™•ì¥ ë¤í”„ íŒŒì¼ ë¶„ì„ê¸° (2-Pass ì§€ì›)"""

    def __init__(self, use_two_pass: bool = True):
        super().__init__()
        self.use_two_pass = use_two_pass

        if use_two_pass:
            self._two_pass_analyzer = TwoPassAnalyzer()
        else:
            self._two_pass_analyzer = None

    def analyze_dump_folder(self, dump_path: str) -> DumpAnalysisResult:
        """ë¤í”„ í´ë” ë¶„ì„ (2-Pass ë˜ëŠ” ê¸°ì¡´ ë°©ì‹)"""
        if self.use_two_pass and self._two_pass_analyzer:
            self._two_pass_analyzer.set_callbacks(
                self._progress_callback,
                self._issue_callback
            )
            return self._two_pass_analyzer.analyze_dump_folder(dump_path)
        else:
            # ê¸°ì¡´ ë‹¨ì¼ íŒ¨ìŠ¤ ë¶„ì„
            return super().analyze_dump_folder(dump_path)

    def export_report(self, result: DumpAnalysisResult, filepath: str, format: str = 'json'):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë¦¬í¬íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if REPORT_EXPORTER_AVAILABLE:
            exporter = ReportExporter(result.compatibility_issues)
            exporter.save_to_file(filepath, format)
            return filepath
        else:
            raise ImportError("ReportExporter ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
