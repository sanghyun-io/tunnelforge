"""
MySQL ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ê¸°
- ê³ ì•„ ë ˆì½”ë“œ(orphan rows) íƒì§€
- FK ê´€ê³„ ë¶„ì„ ë° ì •ë¦¬
- MySQL 8.0.x â†’ 8.4.x í˜¸í™˜ì„± ê²€ì‚¬ (Upgrade Checker í†µí•©)
- dry-run ì§€ì›
- ë¤í”„ íŒŒì¼ ë¶„ì„ (SQL/TSV)
- 2-Pass ë¶„ì„ ì•„í‚¤í…ì²˜ (FK í¬ë¡œìŠ¤ ê²€ì¦)
"""
import re
from typing import List, Dict, Set, Tuple, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from src.core.db_connector import MySQLConnector

# ============================================================
# ìƒˆ ìƒìˆ˜ ëª¨ë“ˆì—ì„œ import (migration_constants.py)
# ============================================================
from src.core.migration_constants import (
    REMOVED_SYS_VARS_84,
    NEW_RESERVED_KEYWORDS_84,
    REMOVED_FUNCTIONS_84,
    AUTH_PLUGINS,
    OBSOLETE_SQL_MODES,
    SYS_VARS_NEW_DEFAULTS_84,
    IssueType,
    INVALID_DATE_PATTERN,
    INVALID_DATETIME_PATTERN,
    ZEROFILL_PATTERN,
    FLOAT_PRECISION_PATTERN,
    INT_DISPLAY_WIDTH_PATTERN,
    FK_NAME_LENGTH_PATTERN,
    AUTH_PLUGIN_PATTERN,
    FTS_TABLE_PREFIX_PATTERN,
    SUPER_PRIVILEGE_PATTERN,
    SYS_VAR_USAGE_PATTERN,
    ALL_RESERVED_KEYWORDS,
)

# ê·œì¹™ ëª¨ë“ˆì—ì„œ import (ì„ íƒì  - ì—ëŸ¬ ë°©ì§€)
try:
    from src.core.migration_rules import DataIntegrityRules, SchemaRules, StorageRules
    RULES_AVAILABLE = True
except ImportError:
    RULES_AVAILABLE = False

# íŒŒì„œ ëª¨ë“ˆì—ì„œ import (ì„ íƒì )
try:
    from src.core.migration_parsers import SQLParser, ParsedTable, ParsedIndex, ParsedForeignKey
    PARSERS_AVAILABLE = True
except ImportError:
    PARSERS_AVAILABLE = False

# Fix Query ìƒì„±ê¸° import (ì„ íƒì )
try:
    from src.core.migration_fix_generator import FixQueryGenerator
    FIX_GENERATOR_AVAILABLE = True
except ImportError:
    FIX_GENERATOR_AVAILABLE = False

# Report Exporter import (ì„ íƒì )
try:
    from src.core.migration_report import ReportExporter
    REPORT_EXPORTER_AVAILABLE = True
except ImportError:
    REPORT_EXPORTER_AVAILABLE = False


# IssueTypeì€ migration_constantsì—ì„œ importë¨


class ActionType(Enum):
    """ì¡°ì¹˜ ìœ í˜•"""
    DELETE = "delete"  # ì‚­ì œ
    UPDATE = "update"  # ì—…ë°ì´íŠ¸
    SET_NULL = "set_null"  # NULLë¡œ ì„¤ì •
    MANUAL = "manual"  # ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”


@dataclass
class OrphanRecord:
    """ê³ ì•„ ë ˆì½”ë“œ ì •ë³´"""
    child_table: str
    child_column: str
    parent_table: str
    parent_column: str
    orphan_count: int
    sample_values: List[Any] = field(default_factory=list)


@dataclass
class ForeignKeyInfo:
    """FK ê´€ê³„ ì •ë³´"""
    constraint_name: str
    child_table: str
    child_column: str
    parent_table: str
    parent_column: str
    on_delete: str
    on_update: str


@dataclass
class CompatibilityIssue:
    """í˜¸í™˜ì„± ë¬¸ì œ"""
    issue_type: IssueType
    severity: str  # "error", "warning", "info"
    location: str  # í…Œì´ë¸”ëª… ë˜ëŠ” ìœ„ì¹˜
    description: str
    suggestion: str
    fix_query: Optional[str] = None      # ìˆ˜ì • SQL
    doc_link: Optional[str] = None       # ë¬¸ì„œ ë§í¬
    mysql_shell_check_id: Optional[str] = None  # MySQL Shell ì²´í¬ ID
    code_snippet: Optional[str] = None   # ê´€ë ¨ ì½”ë“œ
    table_name: Optional[str] = None     # í…Œì´ë¸”ëª…
    column_name: Optional[str] = None    # ì»¬ëŸ¼ëª…


@dataclass
class CleanupAction:
    """ì •ë¦¬ ì‘ì—…"""
    action_type: ActionType
    table: str
    description: str
    sql: str
    affected_rows: int
    dry_run: bool = True


@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼"""
    schema: str
    analyzed_at: str
    total_tables: int
    total_fk_relations: int
    orphan_records: List[OrphanRecord] = field(default_factory=list)
    compatibility_issues: List[CompatibilityIssue] = field(default_factory=list)
    cleanup_actions: List[CleanupAction] = field(default_factory=list)
    fk_tree: Dict[str, List[str]] = field(default_factory=dict)


class MigrationAnalyzer:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¶„ì„ê¸°"""

    # MySQL 8.4ì—ì„œ ì œê±°ëœ/deprecatedëœ í•¨ìˆ˜ë“¤ (ì „ì—­ ìƒìˆ˜ ì‚¬ìš©)
    DEPRECATED_FUNCTIONS = list(REMOVED_FUNCTIONS_84)

    # MySQL 8.4ì—ì„œ ìƒˆë¡œìš´ ì˜ˆì•½ì–´ë“¤ (ê¸°ì¡´ 22ê°œ + 8.4 ì¶”ê°€ 4ê°œ)
    NEW_RESERVED_KEYWORDS = [
        'CUME_DIST', 'DENSE_RANK', 'EMPTY', 'EXCEPT', 'FIRST_VALUE',
        'GROUPING', 'GROUPS', 'JSON_TABLE', 'LAG', 'LAST_VALUE', 'LATERAL',
        'LEAD', 'NTH_VALUE', 'NTILE', 'OF', 'OVER', 'PERCENT_RANK',
        'RANK', 'RECURSIVE', 'ROW_NUMBER', 'SYSTEM', 'WINDOW',
        # MySQL 8.4 ì¶”ê°€ ì˜ˆì•½ì–´
        'MANUAL', 'PARALLEL', 'QUALIFY', 'TABLESAMPLE'
    ]

    def __init__(self, connector: MySQLConnector):
        self.connector = connector
        self._progress_callback: Optional[Callable[[str], None]] = None

    def set_progress_callback(self, callback: Callable[[str], None]):
        """ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •"""
        self._progress_callback = callback

    def _log(self, message: str):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        if self._progress_callback:
            self._progress_callback(message)

    def get_foreign_keys(self, schema: str) -> List[ForeignKeyInfo]:
        """ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  FK ê´€ê³„ ì¡°íšŒ"""
        query = """
        SELECT
            tc.CONSTRAINT_NAME,
            kcu.TABLE_NAME as CHILD_TABLE,
            kcu.COLUMN_NAME as CHILD_COLUMN,
            kcu.REFERENCED_TABLE_NAME as PARENT_TABLE,
            kcu.REFERENCED_COLUMN_NAME as PARENT_COLUMN,
            rc.DELETE_RULE,
            rc.UPDATE_RULE
        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc
        JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE kcu
            ON tc.CONSTRAINT_NAME = kcu.CONSTRAINT_NAME
            AND tc.TABLE_SCHEMA = kcu.TABLE_SCHEMA
        JOIN INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS rc
            ON tc.CONSTRAINT_NAME = rc.CONSTRAINT_NAME
            AND tc.TABLE_SCHEMA = rc.CONSTRAINT_SCHEMA
        WHERE tc.TABLE_SCHEMA = %s
            AND tc.CONSTRAINT_TYPE = 'FOREIGN KEY'
        ORDER BY kcu.TABLE_NAME, kcu.COLUMN_NAME
        """
        rows = self.connector.execute(query, (schema,))

        fk_list = []
        for row in rows:
            fk_list.append(ForeignKeyInfo(
                constraint_name=row['CONSTRAINT_NAME'],
                child_table=row['CHILD_TABLE'],
                child_column=row['CHILD_COLUMN'],
                parent_table=row['PARENT_TABLE'],
                parent_column=row['PARENT_COLUMN'],
                on_delete=row['DELETE_RULE'],
                on_update=row['UPDATE_RULE']
            ))

        return fk_list

    def build_fk_tree(self, schema: str) -> Dict[str, List[str]]:
        """FK ê´€ê³„ íŠ¸ë¦¬ êµ¬ì„± (ë¶€ëª¨ â†’ ìì‹ ëª©ë¡)"""
        fk_list = self.get_foreign_keys(schema)

        tree = {}
        for fk in fk_list:
            if fk.parent_table not in tree:
                tree[fk.parent_table] = []
            if fk.child_table not in tree[fk.parent_table]:
                tree[fk.parent_table].append(fk.child_table)

        return tree

    def find_orphan_records(
        self,
        schema: str,
        sample_limit: int = 5
    ) -> List[OrphanRecord]:
        """ê³ ì•„ ë ˆì½”ë“œ íƒì§€ (ë¶€ëª¨ ì—†ëŠ” ìì‹ ë ˆì½”ë“œ)"""
        self._log("ğŸ” ê³ ì•„ ë ˆì½”ë“œ íƒì§€ ì¤‘...")

        fk_list = self.get_foreign_keys(schema)
        orphans = []

        for i, fk in enumerate(fk_list, 1):
            self._log(f"  ê²€ì‚¬ ì¤‘: {fk.child_table}.{fk.child_column} â†’ {fk.parent_table}.{fk.parent_column} ({i}/{len(fk_list)})")

            # ê³ ì•„ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ
            count_query = f"""
            SELECT COUNT(*) as cnt
            FROM `{schema}`.`{fk.child_table}` c
            LEFT JOIN `{schema}`.`{fk.parent_table}` p
                ON c.`{fk.child_column}` = p.`{fk.parent_column}`
            WHERE c.`{fk.child_column}` IS NOT NULL
                AND p.`{fk.parent_column}` IS NULL
            """
            result = self.connector.execute(count_query)
            orphan_count = result[0]['cnt'] if result else 0

            if orphan_count > 0:
                # ìƒ˜í”Œ ê°’ ì¡°íšŒ
                sample_query = f"""
                SELECT DISTINCT c.`{fk.child_column}` as orphan_value
                FROM `{schema}`.`{fk.child_table}` c
                LEFT JOIN `{schema}`.`{fk.parent_table}` p
                    ON c.`{fk.child_column}` = p.`{fk.parent_column}`
                WHERE c.`{fk.child_column}` IS NOT NULL
                    AND p.`{fk.parent_column}` IS NULL
                LIMIT {sample_limit}
                """
                samples = self.connector.execute(sample_query)
                sample_values = [s['orphan_value'] for s in samples]

                orphans.append(OrphanRecord(
                    child_table=fk.child_table,
                    child_column=fk.child_column,
                    parent_table=fk.parent_table,
                    parent_column=fk.parent_column,
                    orphan_count=orphan_count,
                    sample_values=sample_values
                ))

                self._log(f"    âš ï¸ ê³ ì•„ ë ˆì½”ë“œ ë°œê²¬: {orphan_count}ê°œ")

        return orphans

    def check_charset_issues(self, schema: str) -> List[CompatibilityIssue]:
        """utf8mb3 ì‚¬ìš© í…Œì´ë¸”/ì»¬ëŸ¼ í™•ì¸"""
        self._log("ğŸ” ë¬¸ìì…‹ ì´ìŠˆ í™•ì¸ ì¤‘...")

        issues = []

        # í…Œì´ë¸” ë ˆë²¨ charset í™•ì¸
        table_query = """
        SELECT TABLE_NAME, TABLE_COLLATION
        FROM INFORMATION_SCHEMA.TABLES
        WHERE TABLE_SCHEMA = %s
            AND TABLE_TYPE = 'BASE TABLE'
            AND (TABLE_COLLATION LIKE 'utf8_%%' OR TABLE_COLLATION LIKE 'utf8mb3_%%')
        """
        tables = self.connector.execute(table_query, (schema,))

        for t in tables:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.CHARSET_ISSUE,
                severity="warning",
                location=f"{schema}.{t['TABLE_NAME']}",
                description=f"í…Œì´ë¸”ì´ utf8mb3 collation ì‚¬ìš© ì¤‘: {t['TABLE_COLLATION']}",
                suggestion="ALTER TABLE ... CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"
            ))

        # ì»¬ëŸ¼ ë ˆë²¨ charset í™•ì¸
        column_query = """
        SELECT TABLE_NAME, COLUMN_NAME, CHARACTER_SET_NAME, COLLATION_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND CHARACTER_SET_NAME IN ('utf8', 'utf8mb3')
        """
        columns = self.connector.execute(column_query, (schema,))

        for c in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.CHARSET_ISSUE,
                severity="warning",
                location=f"{schema}.{c['TABLE_NAME']}.{c['COLUMN_NAME']}",
                description=f"ì»¬ëŸ¼ì´ utf8mb3 ì‚¬ìš© ì¤‘: {c['CHARACTER_SET_NAME']}",
                suggestion="ALTER TABLE ... MODIFY COLUMN ... CHARACTER SET utf8mb4"
            ))

        if issues:
            self._log(f"  âš ï¸ ë¬¸ìì…‹ ì´ìŠˆ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ë¬¸ìì…‹ ì´ìŠˆ ì—†ìŒ")

        return issues

    def check_reserved_keywords(self, schema: str) -> List[CompatibilityIssue]:
        """ì˜ˆì•½ì–´ì™€ ì¶©ëŒí•˜ëŠ” ì»¬ëŸ¼/í…Œì´ë¸”ëª… í™•ì¸"""
        self._log("ğŸ” ì˜ˆì•½ì–´ ì¶©ëŒ í™•ì¸ ì¤‘...")

        issues = []
        keywords_upper = set(k.upper() for k in self.NEW_RESERVED_KEYWORDS)

        # í…Œì´ë¸”ëª… í™•ì¸
        tables = self.connector.get_tables(schema)
        for table in tables:
            if table.upper() in keywords_upper:
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.RESERVED_KEYWORD,
                    severity="error",
                    location=f"{schema}.{table}",
                    description=f"í…Œì´ë¸”ëª… '{table}'ì´ MySQL 8.4 ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                    suggestion=f"í…Œì´ë¸”ëª…ì„ ë°±í‹±ìœ¼ë¡œ ê°ì‹¸ê±°ë‚˜ ì´ë¦„ ë³€ê²½ í•„ìš”"
                ))

        # ì»¬ëŸ¼ëª… í™•ì¸
        column_query = """
        SELECT TABLE_NAME, COLUMN_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
        """
        columns = self.connector.execute(column_query, (schema,))

        for c in columns:
            if c['COLUMN_NAME'].upper() in keywords_upper:
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.RESERVED_KEYWORD,
                    severity="warning",
                    location=f"{schema}.{c['TABLE_NAME']}.{c['COLUMN_NAME']}",
                    description=f"ì»¬ëŸ¼ëª… '{c['COLUMN_NAME']}'ì´ MySQL 8.4 ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                    suggestion="ì»¬ëŸ¼ ì°¸ì¡° ì‹œ ë°±í‹±(`) ì‚¬ìš© í•„ìš”"
                ))

        if issues:
            self._log(f"  âš ï¸ ì˜ˆì•½ì–´ ì¶©ëŒ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ì˜ˆì•½ì–´ ì¶©ëŒ ì—†ìŒ")

        return issues

    def check_deprecated_in_routines(self, schema: str) -> List[CompatibilityIssue]:
        """ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ì—ì„œ deprecated í•¨ìˆ˜ ì‚¬ìš© í™•ì¸"""
        self._log("ğŸ” ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ ê²€ì‚¬ ì¤‘...")

        issues = []

        # ì €ì¥ í”„ë¡œì‹œì €ì™€ í•¨ìˆ˜ ì¡°íšŒ
        routine_query = """
        SELECT ROUTINE_NAME, ROUTINE_TYPE, ROUTINE_DEFINITION
        FROM INFORMATION_SCHEMA.ROUTINES
        WHERE ROUTINE_SCHEMA = %s
            AND ROUTINE_DEFINITION IS NOT NULL
        """
        routines = self.connector.execute(routine_query, (schema,))

        for routine in routines:
            definition = routine['ROUTINE_DEFINITION'].upper() if routine['ROUTINE_DEFINITION'] else ""

            for func in self.DEPRECATED_FUNCTIONS:
                if func in definition:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.DEPRECATED_FUNCTION,
                        severity="error",
                        location=f"{routine['ROUTINE_TYPE']} {schema}.{routine['ROUTINE_NAME']}",
                        description=f"deprecated í•¨ìˆ˜ '{func}' ì‚¬ìš© ì¤‘",
                        suggestion=f"'{func}' í•¨ìˆ˜ë¥¼ ëŒ€ì²´ í•¨ìˆ˜ë¡œ ë³€ê²½ í•„ìš”"
                    ))

        if issues:
            self._log(f"  âš ï¸ deprecated í•¨ìˆ˜ ì‚¬ìš© {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… deprecated í•¨ìˆ˜ ì—†ìŒ")

        return issues

    def check_sql_modes(self) -> List[CompatibilityIssue]:
        """í˜„ì¬ SQL ëª¨ë“œ í™•ì¸"""
        self._log("ğŸ” SQL ëª¨ë“œ í™•ì¸ ì¤‘...")

        issues = []

        # deprecated SQL ëª¨ë“œë“¤
        deprecated_modes = [
            'NO_AUTO_CREATE_USER',  # 8.0ì—ì„œ ì œê±°ë¨
            'NO_FIELD_OPTIONS',
            'NO_KEY_OPTIONS',
            'NO_TABLE_OPTIONS',
        ]

        result = self.connector.execute("SELECT @@sql_mode as sql_mode")
        if result:
            current_modes = result[0]['sql_mode'].split(',')

            for mode in current_modes:
                mode = mode.strip()
                if mode in deprecated_modes:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.SQL_MODE_ISSUE,
                        severity="warning",
                        location="@@sql_mode",
                        description=f"deprecated SQL ëª¨ë“œ '{mode}' ì‚¬ìš© ì¤‘",
                        suggestion=f"sql_modeì—ì„œ '{mode}' ì œê±° í•„ìš”"
                    ))

        if issues:
            self._log(f"  âš ï¸ deprecated SQL ëª¨ë“œ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… SQL ëª¨ë“œ ì •ìƒ")

        return issues

    def generate_cleanup_sql(
        self,
        orphan: OrphanRecord,
        action: ActionType,
        schema: str,
        dry_run: bool = True
    ) -> CleanupAction:
        """ê³ ì•„ ë ˆì½”ë“œ ì •ë¦¬ SQL ìƒì„±"""
        if action == ActionType.DELETE:
            sql = f"""DELETE FROM `{schema}`.`{orphan.child_table}`
WHERE `{orphan.child_column}` NOT IN (
    SELECT `{orphan.parent_column}` FROM `{schema}`.`{orphan.parent_table}`
)
AND `{orphan.child_column}` IS NOT NULL"""
            description = f"{orphan.child_table}ì—ì„œ ê³ ì•„ ë ˆì½”ë“œ {orphan.orphan_count}ê°œ ì‚­ì œ"

        elif action == ActionType.SET_NULL:
            sql = f"""UPDATE `{schema}`.`{orphan.child_table}`
SET `{orphan.child_column}` = NULL
WHERE `{orphan.child_column}` NOT IN (
    SELECT `{orphan.parent_column}` FROM `{schema}`.`{orphan.parent_table}`
)
AND `{orphan.child_column}` IS NOT NULL"""
            description = f"{orphan.child_table}.{orphan.child_column}ì„ NULLë¡œ ì„¤ì • ({orphan.orphan_count}ê°œ)"

        else:
            sql = f"-- ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”: {orphan.child_table}.{orphan.child_column}"
            description = f"{orphan.child_table} ìˆ˜ë™ ê²€í†  í•„ìš”"

        return CleanupAction(
            action_type=action,
            table=orphan.child_table,
            description=description,
            sql=sql,
            affected_rows=orphan.orphan_count,
            dry_run=dry_run
        )

    def execute_cleanup(
        self,
        action: CleanupAction,
        dry_run: bool = True
    ) -> Tuple[bool, str, int]:
        """
        ì •ë¦¬ ì‘ì—… ì‹¤í–‰

        Args:
            action: ì‹¤í–‰í•  ì •ë¦¬ ì‘ì—…
            dry_run: Trueë©´ ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ì˜í–¥ë°›ëŠ” í–‰ ìˆ˜ë§Œ ë°˜í™˜

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€, ì˜í–¥ë°›ì€ í–‰ ìˆ˜)
        """
        if dry_run:
            # dry-run: ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ì˜í–¥ë°›ëŠ” í–‰ ìˆ˜ í™•ì¸
            self._log(f"ğŸ” [DRY-RUN] ì˜í–¥ ë¶„ì„: {action.table}")

            if action.action_type == ActionType.MANUAL:
                return True, "ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”", 0

            # COUNT ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì—¬ ì˜í–¥ë°›ëŠ” í–‰ ìˆ˜ í™•ì¸
            # DELETE/UPDATEì˜ WHERE ì ˆ ì¶”ì¶œ
            sql_upper = action.sql.upper()
            if 'WHERE' in sql_upper:
                where_idx = action.sql.upper().find('WHERE')
                where_clause = action.sql[where_idx:]

                # í…Œì´ë¸”ëª… ì¶”ì¶œ
                if action.action_type == ActionType.DELETE:
                    # DELETE FROM `schema`.`table` WHERE ...
                    count_sql = f"SELECT COUNT(*) as cnt FROM {action.sql.split('FROM')[1].split('WHERE')[0].strip()} {where_clause}"
                else:
                    # UPDATE `schema`.`table` SET ... WHERE ...
                    count_sql = f"SELECT COUNT(*) as cnt FROM {action.sql.split('UPDATE')[1].split('SET')[0].strip()} {where_clause}"

                result = self.connector.execute(count_sql)
                affected = result[0]['cnt'] if result else 0

                return True, f"[DRY-RUN] {affected}ê°œ í–‰ì´ ì˜í–¥ë°›ìŒ", affected

            return True, "[DRY-RUN] ì˜í–¥ ë¶„ì„ ì™„ë£Œ", action.affected_rows

        else:
            # ì‹¤ì œ ì‹¤í–‰
            self._log(f"ğŸ”§ ì‹¤í–‰ ì¤‘: {action.table}")

            try:
                with self.connector.connection.cursor() as cursor:
                    cursor.execute(action.sql)
                    affected = cursor.rowcount
                    self.connector.connection.commit()

                return True, f"âœ… {affected}ê°œ í–‰ ì²˜ë¦¬ë¨", affected

            except Exception as e:
                self.connector.connection.rollback()
                return False, f"âŒ ì˜¤ë¥˜: {str(e)}", 0

    def analyze_schema(
        self,
        schema: str,
        check_orphans: bool = True,
        check_charset: bool = True,
        check_keywords: bool = True,
        check_routines: bool = True,
        check_sql_mode: bool = True,
        check_auth_plugins: bool = True,
        check_zerofill: bool = True,
        check_float_precision: bool = True,
        check_fk_name_length: bool = True
    ) -> AnalysisResult:
        """
        ìŠ¤í‚¤ë§ˆ ì „ì²´ ë¶„ì„

        Args:
            schema: ë¶„ì„í•  ìŠ¤í‚¤ë§ˆëª…
            check_orphans: ê³ ì•„ ë ˆì½”ë“œ ê²€ì‚¬ ì—¬ë¶€
            check_charset: ë¬¸ìì…‹ ì´ìŠˆ ê²€ì‚¬ ì—¬ë¶€
            check_keywords: ì˜ˆì•½ì–´ ì¶©ëŒ ê²€ì‚¬ ì—¬ë¶€
            check_routines: ì €ì¥ í”„ë¡œì‹œì €/í•¨ìˆ˜ ê²€ì‚¬ ì—¬ë¶€
            check_sql_mode: SQL ëª¨ë“œ ê²€ì‚¬ ì—¬ë¶€
            check_auth_plugins: ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ê²€ì‚¬ ì—¬ë¶€
            check_zerofill: ZEROFILL ì†ì„± ê²€ì‚¬ ì—¬ë¶€
            check_float_precision: FLOAT(M,D) êµ¬ë¬¸ ê²€ì‚¬ ì—¬ë¶€
            check_fk_name_length: FK ì´ë¦„ ê¸¸ì´ ê²€ì‚¬ ì—¬ë¶€

        Returns:
            AnalysisResult
        """
        from datetime import datetime

        self._log(f"ğŸ“Š ìŠ¤í‚¤ë§ˆ '{schema}' ë¶„ì„ ì‹œì‘...")

        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        tables = self.connector.get_tables(schema)
        fk_list = self.get_foreign_keys(schema)
        fk_tree = self.build_fk_tree(schema)

        self._log(f"  í…Œì´ë¸” ìˆ˜: {len(tables)}, FK ê´€ê³„: {len(fk_list)}")

        result = AnalysisResult(
            schema=schema,
            analyzed_at=datetime.now().isoformat(),
            total_tables=len(tables),
            total_fk_relations=len(fk_list),
            fk_tree=fk_tree
        )

        # ê³ ì•„ ë ˆì½”ë“œ ê²€ì‚¬
        if check_orphans and fk_list:
            result.orphan_records = self.find_orphan_records(schema)

        # í˜¸í™˜ì„± ê²€ì‚¬ë“¤ (ê¸°ì¡´)
        if check_charset:
            result.compatibility_issues.extend(self.check_charset_issues(schema))

        if check_keywords:
            result.compatibility_issues.extend(self.check_reserved_keywords(schema))

        if check_routines:
            result.compatibility_issues.extend(self.check_deprecated_in_routines(schema))

        if check_sql_mode:
            result.compatibility_issues.extend(self.check_sql_modes())

        # MySQL 8.4 Upgrade Checker ê²€ì‚¬ë“¤ (ì‹ ê·œ)
        if check_auth_plugins:
            result.compatibility_issues.extend(self.check_auth_plugins())

        if check_zerofill:
            result.compatibility_issues.extend(self.check_zerofill_columns(schema))

        if check_float_precision:
            result.compatibility_issues.extend(self.check_float_precision(schema))

        if check_fk_name_length:
            result.compatibility_issues.extend(self.check_fk_name_length(schema))

        # ì •ë¦¬ ì‘ì—… ìƒì„± (ê³ ì•„ ë ˆì½”ë“œì— ëŒ€í•´)
        for orphan in result.orphan_records:
            # ê¸°ë³¸ì ìœ¼ë¡œ DELETE ì‘ì—… ìƒì„± (dry-run)
            cleanup = self.generate_cleanup_sql(orphan, ActionType.DELETE, schema, dry_run=True)
            result.cleanup_actions.append(cleanup)

        self._log(f"âœ… ë¶„ì„ ì™„ë£Œ")
        self._log(f"  - ê³ ì•„ ë ˆì½”ë“œ: {len(result.orphan_records)}ê°œ FK ê´€ê³„ì—ì„œ ë°œê²¬")
        self._log(f"  - í˜¸í™˜ì„± ì´ìŠˆ: {len(result.compatibility_issues)}ê°œ")

        return result

    # ============================================================
    # MySQL 8.4 Upgrade Checker ê²€ì‚¬ ë©”ì„œë“œë“¤ (ì‹ ê·œ)
    # ============================================================

    def check_auth_plugins(self) -> List[CompatibilityIssue]:
        """mysql_native_password, sha256_password ì‚¬ìš©ì í™•ì¸"""
        self._log("ğŸ” ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘...")

        issues = []

        # ì‚¬ìš©ìë³„ ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì¡°íšŒ
        query = """
        SELECT User, Host, plugin
        FROM mysql.user
        WHERE plugin IN ('mysql_native_password', 'sha256_password', 'authentication_fido')
        """
        try:
            users = self.connector.execute(query)

            for user in users:
                plugin = user['plugin']

                if plugin == 'mysql_native_password':
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                        severity="error",
                        location=f"'{user['User']}'@'{user['Host']}'",
                        description=f"mysql_native_password ì¸ì¦ ì‚¬ìš© (8.4ì—ì„œ ê¸°ë³¸ ë¹„í™œì„±í™”)",
                        suggestion="ALTER USER ... IDENTIFIED WITH caching_sha2_password"
                    ))
                elif plugin == 'sha256_password':
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                        severity="warning",
                        location=f"'{user['User']}'@'{user['Host']}'",
                        description=f"sha256_password ì¸ì¦ ì‚¬ìš© (deprecated)",
                        suggestion="ALTER USER ... IDENTIFIED WITH caching_sha2_password ê¶Œì¥"
                    ))
                elif plugin == 'authentication_fido':
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                        severity="error",
                        location=f"'{user['User']}'@'{user['Host']}'",
                        description=f"authentication_fido í”ŒëŸ¬ê·¸ì¸ ì‚¬ìš© (8.4ì—ì„œ ì œê±°ë¨)",
                        suggestion="authentication_webauthn ë˜ëŠ” ë‹¤ë¥¸ ì¸ì¦ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ í•„ìš”"
                    ))

            if issues:
                self._log(f"  âš ï¸ ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì´ìŠˆ {len(issues)}ê°œ ë°œê²¬")
            else:
                self._log("  âœ… ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ì •ìƒ")

        except Exception as e:
            self._log(f"  âš ï¸ ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

        return issues

    def check_zerofill_columns(self, schema: str) -> List[CompatibilityIssue]:
        """ZEROFILL ì†ì„± ì‚¬ìš© ì»¬ëŸ¼ í™•ì¸"""
        self._log("ğŸ” ZEROFILL ì†ì„± í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND COLUMN_TYPE LIKE '%%ZEROFILL%%'
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.ZEROFILL_USAGE,
                severity="warning",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description=f"ZEROFILL ì†ì„± ì‚¬ìš©: {col['COLUMN_TYPE']}",
                suggestion="ZEROFILLì€ deprecatedë¨, ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ LPAD() ë“±ìœ¼ë¡œ ì²˜ë¦¬ ê¶Œì¥"
            ))

        if issues:
            self._log(f"  âš ï¸ ZEROFILL ì‚¬ìš© {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… ZEROFILL ì‚¬ìš© ì—†ìŒ")

        return issues

    def check_float_precision(self, schema: str) -> List[CompatibilityIssue]:
        """FLOAT(M,D), DOUBLE(M,D) êµ¬ë¬¸ í™•ì¸"""
        self._log("ğŸ” FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸ í™•ì¸ ì¤‘...")

        issues = []

        # FLOAT(M,D), DOUBLE(M,D) í˜•íƒœ í™•ì¸
        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE, DATA_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE IN ('float', 'double')
            AND COLUMN_TYPE REGEXP '^(float|double)\\\\([0-9]+,[0-9]+\\\\)'
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.FLOAT_PRECISION,
                severity="warning",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description=f"FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸ ì‚¬ìš©: {col['COLUMN_TYPE']}",
                suggestion="FLOAT(M,D) êµ¬ë¬¸ì€ deprecatedë¨, FLOAT ë˜ëŠ” DECIMAL(M,D) ì‚¬ìš© ê¶Œì¥"
            ))

        if issues:
            self._log(f"  âš ï¸ FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… FLOAT/DOUBLE êµ¬ë¬¸ ì •ìƒ")

        return issues

    def check_fk_name_length(self, schema: str) -> List[CompatibilityIssue]:
        """FK ì´ë¦„ 64ì ì´ˆê³¼ í™•ì¸"""
        self._log("ğŸ” FK ì´ë¦„ ê¸¸ì´ í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT CONSTRAINT_NAME, TABLE_NAME
        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS
        WHERE TABLE_SCHEMA = %s
            AND CONSTRAINT_TYPE = 'FOREIGN KEY'
            AND LENGTH(CONSTRAINT_NAME) > 64
        """
        fks = self.connector.execute(query, (schema,))

        for fk in fks:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.FK_NAME_LENGTH,
                severity="error",
                location=f"{schema}.{fk['TABLE_NAME']}.{fk['CONSTRAINT_NAME']}",
                description=f"FK ì´ë¦„ì´ 64ì ì´ˆê³¼: {len(fk['CONSTRAINT_NAME'])}ì",
                suggestion="FK ì´ë¦„ì„ 64ì ì´í•˜ë¡œ ë³€ê²½ í•„ìš” (8.4 ì œí•œ)"
            ))

        if issues:
            self._log(f"  âš ï¸ FK ì´ë¦„ ê¸¸ì´ ì´ˆê³¼ {len(issues)}ê°œ ë°œê²¬")
        else:
            self._log("  âœ… FK ì´ë¦„ ê¸¸ì´ ì •ìƒ")

        return issues

    def check_int_display_width(self, schema: str) -> List[CompatibilityIssue]:
        """INT(11) ë“± í‘œì‹œ ë„ˆë¹„ ì‚¬ìš© í™•ì¸ (TINYINT(1) ì œì™¸)"""
        self._log("ğŸ” INT í‘œì‹œ ë„ˆë¹„ í™•ì¸ ì¤‘...")

        issues = []

        query = """
        SELECT TABLE_NAME, COLUMN_NAME, COLUMN_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = %s
            AND DATA_TYPE IN ('tinyint', 'smallint', 'mediumint', 'int', 'bigint')
            AND COLUMN_TYPE REGEXP '^(tinyint|smallint|mediumint|int|bigint)\\\\([0-9]+\\\\)'
            AND NOT (DATA_TYPE = 'tinyint' AND COLUMN_TYPE LIKE 'tinyint(1)%%')
        """
        columns = self.connector.execute(query, (schema,))

        for col in columns:
            issues.append(CompatibilityIssue(
                issue_type=IssueType.INT_DISPLAY_WIDTH,
                severity="info",
                location=f"{schema}.{col['TABLE_NAME']}.{col['COLUMN_NAME']}",
                description=f"INT í‘œì‹œ ë„ˆë¹„ ì‚¬ìš©: {col['COLUMN_TYPE']}",
                suggestion="í‘œì‹œ ë„ˆë¹„ëŠ” deprecatedë¨, 8.4ì—ì„œ ìë™ ë¬´ì‹œë¨ (ì˜í–¥ ìµœì†Œ)"
            ))

        if issues:
            self._log(f"  â„¹ï¸ INT í‘œì‹œ ë„ˆë¹„ {len(issues)}ê°œ ë°œê²¬ (ê²½ë¯¸)")
        else:
            self._log("  âœ… INT í‘œì‹œ ë„ˆë¹„ ì—†ìŒ")

        return issues

    def get_fk_visualization(self, schema: str) -> str:
        """FK ê´€ê³„ë¥¼ íŠ¸ë¦¬ í˜•íƒœë¡œ ì‹œê°í™”"""
        fk_tree = self.build_fk_tree(schema)

        if not fk_tree:
            return "FK ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines = ["FK ê´€ê³„ íŠ¸ë¦¬:", ""]

        # ë£¨íŠ¸ í…Œì´ë¸” ì°¾ê¸° (ë‹¤ë¥¸ í…Œì´ë¸”ì˜ ìì‹ì´ ì•„ë‹Œ í…Œì´ë¸”)
        all_children = set()
        for children in fk_tree.values():
            all_children.update(children)

        root_tables = set(fk_tree.keys()) - all_children

        def print_tree(table: str, prefix: str = "", is_last: bool = True):
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            lines.append(f"{prefix}{connector}{table}")

            if table in fk_tree:
                children = fk_tree[table]
                child_prefix = prefix + ("    " if is_last else "â”‚   ")
                for i, child in enumerate(children):
                    print_tree(child, child_prefix, i == len(children) - 1)

        for i, root in enumerate(sorted(root_tables)):
            print_tree(root, "", i == len(root_tables) - 1)

        return "\n".join(lines)


# ============================================================
# ë¤í”„ íŒŒì¼ ë¶„ì„ê¸° (Task 3)
# ============================================================

@dataclass
class DumpAnalysisResult:
    """ë¤í”„ íŒŒì¼ ë¶„ì„ ê²°ê³¼"""
    dump_path: str
    analyzed_at: str
    total_sql_files: int
    total_tsv_files: int
    compatibility_issues: List[CompatibilityIssue] = field(default_factory=list)


class DumpFileAnalyzer:
    """
    mysqlsh ë¤í”„ íŒŒì¼ ë¶„ì„ê¸°

    ë¤í”„ í´ë”ì˜ SQL/TSV íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ MySQL 8.4 í˜¸í™˜ì„± ì´ìŠˆë¥¼ íƒì§€í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self._progress_callback: Optional[Callable[[str], None]] = None
        self._issue_callback: Optional[Callable[[CompatibilityIssue], None]] = None

    def set_progress_callback(self, callback: Callable[[str], None]):
        """ì§„í–‰ ìƒí™© ì½œë°± ì„¤ì •"""
        self._progress_callback = callback

    def set_issue_callback(self, callback: Callable[[CompatibilityIssue], None]):
        """ì´ìŠˆ ë°œê²¬ ì‹œ ì½œë°± ì„¤ì •"""
        self._issue_callback = callback

    def _log(self, message: str):
        """ì§„í–‰ ìƒí™© ë¡œê¹…"""
        if self._progress_callback:
            self._progress_callback(message)

    def _report_issue(self, issue: CompatibilityIssue):
        """ì´ìŠˆ ë°œê²¬ ì‹œ ì½œë°± í˜¸ì¶œ"""
        if self._issue_callback:
            self._issue_callback(issue)

    def analyze_dump_folder(self, dump_path: str) -> DumpAnalysisResult:
        """
        ë¤í”„ í´ë” ì „ì²´ ë¶„ì„

        Args:
            dump_path: mysqlsh ë¤í”„ í´ë” ê²½ë¡œ

        Returns:
            DumpAnalysisResult
        """
        from datetime import datetime

        path = Path(dump_path)
        if not path.exists():
            raise FileNotFoundError(f"ë¤í”„ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dump_path}")

        self._log(f"ğŸ” ë¤í”„ í´ë” ë¶„ì„ ì‹œì‘: {dump_path}")

        issues: List[CompatibilityIssue] = []

        # SQL íŒŒì¼ ëª©ë¡
        sql_files = list(path.glob("*.sql"))
        tsv_files = list(path.glob("*.tsv")) + list(path.glob("*.tsv.zst"))

        self._log(f"  SQL íŒŒì¼: {len(sql_files)}ê°œ, ë°ì´í„° íŒŒì¼: {len(tsv_files)}ê°œ")

        # SQL íŒŒì¼ ë¶„ì„
        for i, sql_file in enumerate(sql_files, 1):
            self._log(f"  [{i}/{len(sql_files)}] {sql_file.name} ë¶„ì„ ì¤‘...")
            file_issues = self._analyze_sql_file(sql_file)
            issues.extend(file_issues)

            # ì‹¤ì‹œê°„ ì´ìŠˆ ì½œë°±
            for issue in file_issues:
                self._report_issue(issue)

        # TSV ë°ì´í„° íŒŒì¼ ë¶„ì„ (0000-00-00 ë‚ ì§œ ë“±)
        # ì••ì¶•ë˜ì§€ ì•Šì€ TSV íŒŒì¼ë§Œ ë¶„ì„ (ì••ì¶• íŒŒì¼ì€ ë„ˆë¬´ ëŠë¦¼)
        uncompressed_tsv = [f for f in tsv_files if not str(f).endswith('.zst')]
        if uncompressed_tsv:
            for i, tsv_file in enumerate(uncompressed_tsv, 1):
                self._log(f"  [{i}/{len(uncompressed_tsv)}] {tsv_file.name} ë¶„ì„ ì¤‘...")
                file_issues = self._analyze_tsv_file(tsv_file)
                issues.extend(file_issues)

                for issue in file_issues:
                    self._report_issue(issue)

        # ê²°ê³¼ ìƒì„±
        result = DumpAnalysisResult(
            dump_path=str(dump_path),
            analyzed_at=datetime.now().isoformat(),
            total_sql_files=len(sql_files),
            total_tsv_files=len(tsv_files),
            compatibility_issues=issues
        )

        # ìš”ì•½
        error_count = sum(1 for i in issues if i.severity == "error")
        warning_count = sum(1 for i in issues if i.severity == "warning")

        self._log(f"âœ… ë¤í”„ ë¶„ì„ ì™„ë£Œ")
        self._log(f"  - ì˜¤ë¥˜: {error_count}ê°œ")
        self._log(f"  - ê²½ê³ : {warning_count}ê°œ")

        return result

    def _analyze_sql_file(self, file_path: Path) -> List[CompatibilityIssue]:
        """
        SQL íŒŒì¼ ë¶„ì„ - ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì‚¬

        Args:
            file_path: SQL íŒŒì¼ ê²½ë¡œ

        Returns:
            ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
        """
        issues = []

        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')

            # 1. ZEROFILL ì†ì„± ê²€ì‚¬
            for match in ZEROFILL_PATTERN.finditer(content):
                # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í…Œì´ë¸”/ì»¬ëŸ¼ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                line_start = content.rfind('\n', 0, match.start()) + 1
                line_end = content.find('\n', match.end())
                line = content[line_start:line_end]

                issues.append(CompatibilityIssue(
                    issue_type=IssueType.ZEROFILL_USAGE,
                    severity="warning",
                    location=f"{file_path.name}",
                    description=f"ZEROFILL ì†ì„± ì‚¬ìš©: {line.strip()[:80]}...",
                    suggestion="ZEROFILLì€ deprecatedë¨"
                ))

            # 2. FLOAT(M,D), DOUBLE(M,D) êµ¬ë¬¸ ê²€ì‚¬
            for match in FLOAT_PRECISION_PATTERN.finditer(content):
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.FLOAT_PRECISION,
                    severity="warning",
                    location=f"{file_path.name}",
                    description=f"FLOAT/DOUBLE ì •ë°€ë„ êµ¬ë¬¸: {match.group(0)}",
                    suggestion="FLOAT(M,D) êµ¬ë¬¸ì€ deprecatedë¨"
                ))

            # 3. FK ì´ë¦„ 64ì ì´ˆê³¼ ê²€ì‚¬
            for match in FK_NAME_LENGTH_PATTERN.finditer(content):
                fk_name = match.group(1)
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.FK_NAME_LENGTH,
                    severity="error",
                    location=f"{file_path.name}",
                    description=f"FK ì´ë¦„ 64ì ì´ˆê³¼: {fk_name[:30]}... ({len(fk_name)}ì)",
                    suggestion="FK ì´ë¦„ì„ 64ì ì´í•˜ë¡œ ë³€ê²½ í•„ìš”"
                ))

            # 4. ì¸ì¦ í”ŒëŸ¬ê·¸ì¸ ê²€ì‚¬
            for match in AUTH_PLUGIN_PATTERN.finditer(content):
                plugin = match.group(1).lower()
                severity = "error" if plugin == "mysql_native_password" else "warning"
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.AUTH_PLUGIN_ISSUE,
                    severity=severity,
                    location=f"{file_path.name}",
                    description=f"ì¸ì¦ í”ŒëŸ¬ê·¸ì¸: {plugin}",
                    suggestion="caching_sha2_password ì‚¬ìš© ê¶Œì¥"
                ))

            # 5. FTS_ í…Œì´ë¸”ëª… ê²€ì‚¬
            for match in FTS_TABLE_PREFIX_PATTERN.finditer(content):
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.FTS_TABLE_PREFIX,
                    severity="error",
                    location=f"{file_path.name}",
                    description="FTS_ ì ‘ë‘ì‚¬ í…Œì´ë¸”ëª… (ë‚´ë¶€ ì˜ˆì•½ì–´)",
                    suggestion="FTS_ ì ‘ë‘ì‚¬ëŠ” ë‚´ë¶€ ì „ë¬¸ ê²€ìƒ‰ìš©ìœ¼ë¡œ ì˜ˆì•½ë¨, í…Œì´ë¸”ëª… ë³€ê²½ í•„ìš”"
                ))

            # 6. SUPER ê¶Œí•œ ê²€ì‚¬
            for match in SUPER_PRIVILEGE_PATTERN.finditer(content):
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.SUPER_PRIVILEGE,
                    severity="warning",
                    location=f"{file_path.name}",
                    description="SUPER ê¶Œí•œ ì‚¬ìš© (deprecated)",
                    suggestion="ë™ì  ê¶Œí•œ (BINLOG_ADMIN, CONNECTION_ADMIN ë“±)ìœ¼ë¡œ ì„¸ë¶„í™” ê¶Œì¥"
                ))

            # 7. ì œê±°ëœ ì‹œìŠ¤í…œ ë³€ìˆ˜ ì‚¬ìš© ê²€ì‚¬
            for match in SYS_VAR_USAGE_PATTERN.finditer(content):
                var_name = match.group(1)
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.REMOVED_SYS_VAR,
                    severity="error",
                    location=f"{file_path.name}",
                    description=f"ì œê±°ëœ ì‹œìŠ¤í…œ ë³€ìˆ˜ ì‚¬ìš©: {var_name}",
                    suggestion=f"'{var_name}'ì€ 8.4ì—ì„œ ì œê±°ë¨, ëŒ€ì²´ ë°©ë²• í™•ì¸ í•„ìš”"
                ))

            # 8. ì˜ˆì•½ì–´ ì¶©ëŒ (í…Œì´ë¸”/ì»¬ëŸ¼ ì´ë¦„) - CREATE TABLE ë¬¸ì—ì„œ
            table_pattern = re.compile(
                r'CREATE\s+TABLE\s+`?(\w+)`?\s*\(',
                re.IGNORECASE
            )
            column_pattern = re.compile(
                r'`(\w+)`\s+(?:INT|VARCHAR|TEXT|DATE|DECIMAL|FLOAT|DOUBLE|CHAR|BLOB|ENUM|SET)',
                re.IGNORECASE
            )

            keywords_upper = set(k.upper() for k in MigrationAnalyzer.NEW_RESERVED_KEYWORDS)

            for match in table_pattern.finditer(content):
                table_name = match.group(1)
                if table_name.upper() in keywords_upper:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.RESERVED_KEYWORD,
                        severity="error",
                        location=f"{file_path.name}",
                        description=f"í…Œì´ë¸”ëª… '{table_name}'ì´ ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                        suggestion="í…Œì´ë¸”ëª… ë³€ê²½ ë˜ëŠ” ë°±í‹±(`) ì‚¬ìš© í•„ìš”"
                    ))

            for match in column_pattern.finditer(content):
                column_name = match.group(1)
                if column_name.upper() in keywords_upper:
                    issues.append(CompatibilityIssue(
                        issue_type=IssueType.RESERVED_KEYWORD,
                        severity="warning",
                        location=f"{file_path.name}",
                        description=f"ì»¬ëŸ¼ëª… '{column_name}'ì´ ì˜ˆì•½ì–´ì™€ ì¶©ëŒ",
                        suggestion="ì»¬ëŸ¼ ì°¸ì¡° ì‹œ ë°±í‹±(`) ì‚¬ìš© í•„ìš”"
                    ))

        except Exception as e:
            self._log(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        return issues

    def _analyze_tsv_file(self, file_path: Path) -> List[CompatibilityIssue]:
        """
        TSV ë°ì´í„° íŒŒì¼ ë¶„ì„ - ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬

        Args:
            file_path: TSV íŒŒì¼ ê²½ë¡œ

        Returns:
            ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
        """
        issues = []
        invalid_date_count = 0

        try:
            # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ìƒ˜í”Œë§
            max_lines = 10000
            line_count = 0

            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                for line in f:
                    line_count += 1
                    if line_count > max_lines:
                        break

                    # 0000-00-00 ë‚ ì§œ ê²€ì‚¬
                    if INVALID_DATE_PATTERN.search(line) or INVALID_DATETIME_PATTERN.search(line):
                        invalid_date_count += 1

            if invalid_date_count > 0:
                issues.append(CompatibilityIssue(
                    issue_type=IssueType.INVALID_DATE,
                    severity="error",
                    location=f"{file_path.name}",
                    description=f"ì˜ëª»ëœ ë‚ ì§œ ê°’ ë°œê²¬: {invalid_date_count}ê°œ í–‰ (0000-00-00)",
                    suggestion="NO_ZERO_DATE SQL ëª¨ë“œ í™œì„±í™” ì‹œ ì˜¤ë¥˜ ë°œìƒ, ìœ íš¨í•œ ë‚ ì§œë¡œ ë³€í™˜ í•„ìš”"
                ))

        except Exception as e:
            self._log(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        return issues

    def quick_scan(self, dump_path: str) -> Tuple[int, int, int]:
        """
        ë¹ ë¥¸ ìŠ¤ìº” - ì´ìŠˆ ê°œìˆ˜ë§Œ ë°˜í™˜

        Args:
            dump_path: ë¤í”„ í´ë” ê²½ë¡œ

        Returns:
            (ì˜¤ë¥˜ ìˆ˜, ê²½ê³  ìˆ˜, ì •ë³´ ìˆ˜)
        """
        try:
            result = self.analyze_dump_folder(dump_path)
            error_count = sum(1 for i in result.compatibility_issues if i.severity == "error")
            warning_count = sum(1 for i in result.compatibility_issues if i.severity == "warning")
            info_count = sum(1 for i in result.compatibility_issues if i.severity == "info")
            return error_count, warning_count, info_count
        except Exception:
            return 0, 0, 0


# ============================================================
# 2-Pass ë¶„ì„ê¸° (Task 5)
# ============================================================

@dataclass
class TableIndexInfo:
    """í…Œì´ë¸” ì¸ë±ìŠ¤ ì •ë³´"""
    schema: Optional[str]
    table_name: str
    index_name: str
    columns: List[str]
    is_unique: bool
    is_primary: bool

    def covers_columns(self, cols: List[str]) -> bool:
        """ì£¼ì–´ì§„ ì»¬ëŸ¼ë“¤ì´ ì´ ì¸ë±ìŠ¤ë¡œ ì»¤ë²„ë˜ëŠ”ì§€ í™•ì¸"""
        cols_lower = [c.lower() for c in cols]
        idx_cols_lower = [c.lower() for c in self.columns[:len(cols)]]
        return cols_lower == idx_cols_lower


@dataclass
class TableCharsetInfo:
    """í…Œì´ë¸” charset ì •ë³´"""
    schema: Optional[str]
    table_name: str
    charset: str
    collation: Optional[str] = None
    column_charsets: Dict[str, str] = field(default_factory=dict)


@dataclass
class PendingFKCheck:
    """ì§€ì—°ëœ FK ê²€ì¦ ì •ë³´"""
    fk_name: str
    source_schema: Optional[str]
    source_table: str
    source_columns: List[str]
    ref_table: str
    ref_columns: List[str]
    location: str
    line_number: Optional[int] = None


class TwoPassAnalyzer:
    """2-Pass ë¤í”„ íŒŒì¼ ë¶„ì„ê¸°"""

    def __init__(self):
        # Pass 1 ìˆ˜ì§‘ ë°ì´í„°
        self.table_indexes: Dict[str, List[TableIndexInfo]] = {}
        self.table_charsets: Dict[str, TableCharsetInfo] = {}
        self.known_tables: Set[str] = set()

        # Pass 2 ìˆ˜ì§‘ ë°ì´í„°
        self.pending_fk_checks: List[PendingFKCheck] = []

        # íŒŒì„œ (ì˜µì…˜)
        self.sql_parser = None
        if PARSERS_AVAILABLE:
            self.sql_parser = SQLParser()

        # ê·œì¹™ ëª¨ë“ˆ (ì˜µì…˜)
        self.data_rules = None
        self.schema_rules = None
        self.storage_rules = None
        if RULES_AVAILABLE:
            self.data_rules = DataIntegrityRules()
            self.schema_rules = SchemaRules()
            self.storage_rules = StorageRules()

        # Fix Query ìƒì„±ê¸° (ì˜µì…˜)
        self.fix_generator = None
        if FIX_GENERATOR_AVAILABLE:
            self.fix_generator = FixQueryGenerator()

        # ì½œë°±
        self._progress_callback: Optional[Callable[[str], None]] = None
        self._issue_callback: Optional[Callable[[CompatibilityIssue], None]] = None

    def set_callbacks(
        self,
        progress_callback: Optional[Callable[[str], None]] = None,
        issue_callback: Optional[Callable[[CompatibilityIssue], None]] = None
    ):
        """ì½œë°± ì„¤ì •"""
        self._progress_callback = progress_callback
        self._issue_callback = issue_callback

        # ê·œì¹™ ëª¨ë“ˆì—ë„ ì½œë°± ì „íŒŒ
        if self.data_rules and progress_callback:
            self.data_rules.set_progress_callback(progress_callback)
        if self.schema_rules and progress_callback:
            self.schema_rules.set_progress_callback(progress_callback)
        if self.storage_rules and progress_callback:
            self.storage_rules.set_progress_callback(progress_callback)

    def _log(self, message: str):
        if self._progress_callback:
            self._progress_callback(message)

    def _report_issue(self, issue: CompatibilityIssue):
        # Fix Query ìƒì„±
        if self.fix_generator:
            issue = self.fix_generator.generate(issue)

        if self._issue_callback:
            self._issue_callback(issue)

    def clear_state(self):
        """ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”"""
        self.table_indexes.clear()
        self.table_charsets.clear()
        self.known_tables.clear()
        self.pending_fk_checks.clear()

    def _make_table_key(self, schema: Optional[str], table: str) -> str:
        """í…Œì´ë¸” ì¡°íšŒ í‚¤ ìƒì„±"""
        if schema:
            return f"{schema.lower()}.{table.lower()}"
        return table.lower()

    def _register_known_table(self, schema: Optional[str], table_name: str):
        """ì•Œë ¤ì§„ í…Œì´ë¸” ë“±ë¡"""
        key = self._make_table_key(schema, table_name)
        self.known_tables.add(key)

    # ================================================================
    # Pass 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    # ================================================================
    def pass1_collect_metadata(self, files: List[Path]):
        """Pass 1: í…Œì´ë¸” ì¸ë±ìŠ¤ ë° charset ì •ë³´ ìˆ˜ì§‘"""
        self._log("ğŸ“Š Pass 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        for file_path in files:
            if not file_path.suffix.lower() == '.sql':
                continue

            self._log(f"  ìˆ˜ì§‘ ì¤‘: {file_path.name}")

            try:
                content = file_path.read_text(encoding='utf-8', errors='replace')

                # CREATE TABLE ë¬¸ ì¶”ì¶œ ë° íŒŒì‹±
                if self.sql_parser:
                    for sql in self.sql_parser.extract_create_table_statements(content):
                        parsed = self.sql_parser.parse_table(sql)
                        if parsed:
                            self._collect_table_indexes(parsed)
                            self._collect_table_charset(parsed)
                            self._register_known_table(parsed.schema, parsed.name)
                else:
                    # íŒŒì„œ ì—†ì´ ê°„ë‹¨í•œ ì •ê·œì‹ìœ¼ë¡œ í…Œì´ë¸”ëª…ë§Œ ìˆ˜ì§‘
                    table_pattern = re.compile(
                        r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?'
                        r'(?:`?(\w+)`?\.)?`?(\w+)`?',
                        re.IGNORECASE
                    )
                    for match in table_pattern.finditer(content):
                        schema = match.group(1)
                        table_name = match.group(2)
                        self._register_known_table(schema, table_name)

            except Exception as e:
                self._log(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        self._log(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: í…Œì´ë¸” {len(self.known_tables)}ê°œ")

    def _collect_table_indexes(self, table: 'ParsedTable'):
        """í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘"""
        key = self._make_table_key(table.schema, table.name)

        if key not in self.table_indexes:
            self.table_indexes[key] = []

        for idx in table.indexes:
            self.table_indexes[key].append(TableIndexInfo(
                schema=table.schema,
                table_name=table.name,
                index_name=idx.name,
                columns=idx.columns,
                is_unique=idx.is_unique,
                is_primary=idx.is_primary
            ))

    def _collect_table_charset(self, table: 'ParsedTable'):
        """í…Œì´ë¸”ì˜ charset ì •ë³´ ìˆ˜ì§‘"""
        key = self._make_table_key(table.schema, table.name)

        column_charsets = {}
        for col in table.columns:
            if col.charset:
                column_charsets[col.name] = col.charset

        self.table_charsets[key] = TableCharsetInfo(
            schema=table.schema,
            table_name=table.name,
            charset=table.charset or 'utf8mb4',
            collation=table.collation,
            column_charsets=column_charsets
        )

    # ================================================================
    # Pass 2: ì „ì²´ ë¶„ì„ + FK ìˆ˜ì§‘
    # ================================================================
    def pass2_full_analysis(self, files: List[Path]) -> List[CompatibilityIssue]:
        """Pass 2: ì „ì²´ ë¶„ì„ ë° FK ì°¸ì¡° ìˆ˜ì§‘"""
        self._log("ğŸ” Pass 2: ì „ì²´ ë¶„ì„ ì¤‘...")

        all_issues = []

        for file_path in files:
            self._log(f"  ë¶„ì„ ì¤‘: {file_path.name}")

            try:
                if file_path.suffix.lower() == '.sql':
                    issues = self._analyze_sql_file_pass2(file_path)
                elif file_path.suffix.lower() in ('.tsv', '.txt'):
                    issues = self._analyze_data_file_pass2(file_path)
                else:
                    continue

                all_issues.extend(issues)

                # ì‹¤ì‹œê°„ ì´ìŠˆ ë¦¬í¬íŠ¸
                for issue in issues:
                    self._report_issue(issue)

            except Exception as e:
                self._log(f"  âš ï¸ íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {file_path.name} - {str(e)}")

        return all_issues

    def _analyze_sql_file_pass2(self, file_path: Path) -> List[CompatibilityIssue]:
        """SQL íŒŒì¼ ë¶„ì„ (Pass 2)"""
        issues = []
        content = file_path.read_text(encoding='utf-8', errors='replace')
        location = file_path.name

        # ê·œì¹™ ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ ì‹œ í™•ì¥ ê²€ì‚¬
        if self.schema_rules:
            issues.extend(self.schema_rules.check_all_sql_content(content, location))

        if self.storage_rules:
            issues.extend(self.storage_rules.check_all_sql_content(content, location))

        if self.data_rules:
            issues.extend(self.data_rules.check_all_sql_content(content, location))

        # FK ì°¸ì¡° ìˆ˜ì§‘ (í¬ë¡œìŠ¤ ê²€ì¦ìš©)
        if self.sql_parser:
            for sql in self.sql_parser.extract_create_table_statements(content):
                parsed = self.sql_parser.parse_table(sql)
                if parsed:
                    self._collect_fk_references(parsed, location)

        return issues

    def _analyze_data_file_pass2(self, file_path: Path) -> List[CompatibilityIssue]:
        """ë°ì´í„° íŒŒì¼ ë¶„ì„ (Pass 2)"""
        issues = []

        if self.data_rules:
            issues.extend(self.data_rules.check_all_data_file(file_path))

        return issues

    def _collect_fk_references(self, table: 'ParsedTable', location: str):
        """í…Œì´ë¸”ì˜ FK ì°¸ì¡° ì •ë³´ ìˆ˜ì§‘"""
        for fk in table.foreign_keys:
            self.pending_fk_checks.append(PendingFKCheck(
                fk_name=fk.name,
                source_schema=table.schema,
                source_table=table.name,
                source_columns=fk.columns,
                ref_table=fk.ref_table,
                ref_columns=fk.ref_columns,
                location=location
            ))

    # ================================================================
    # Pass 2.5: í¬ë¡œìŠ¤ ê²€ì¦
    # ================================================================
    def pass2_5_cross_validate(self) -> List[CompatibilityIssue]:
        """Pass 2.5: FK í¬ë¡œìŠ¤ ê²€ì¦"""
        self._log("âœ… Pass 2.5: FK í¬ë¡œìŠ¤ ê²€ì¦ ì¤‘...")

        issues = []

        for fk in self.pending_fk_checks:
            # FK ì°¸ì¡° í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            ref_key = self._make_table_key(fk.source_schema, fk.ref_table)

            if ref_key not in self.known_tables:
                issue = CompatibilityIssue(
                    issue_type=IssueType.FK_REF_NOT_FOUND,
                    severity="error",
                    location=fk.location,
                    description=f"FK '{fk.fk_name}': ì°¸ì¡° í…Œì´ë¸” '{fk.ref_table}' ë¯¸ì¡´ì¬",
                    suggestion="ì°¸ì¡° í…Œì´ë¸”ì´ ë¤í”„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”",
                    table_name=fk.source_table
                )
                issues.append(issue)
                self._report_issue(issue)
                continue

            # FK ì°¸ì¡° ì»¬ëŸ¼ì´ PK/UNIQUE ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
            if not self._is_valid_fk_reference(fk):
                issue = CompatibilityIssue(
                    issue_type=IssueType.FK_NON_UNIQUE_REF,
                    severity="error",
                    location=fk.location,
                    description=f"FK '{fk.fk_name}': ì°¸ì¡° ì»¬ëŸ¼ì´ PK/UNIQUE ì•„ë‹˜",
                    suggestion=f"'{fk.ref_table}.{', '.join(fk.ref_columns)}'ì— UNIQUE ì¸ë±ìŠ¤ ì¶”ê°€ í•„ìš”",
                    table_name=fk.source_table
                )
                issues.append(issue)
                self._report_issue(issue)

        self._log(f"  âœ… í¬ë¡œìŠ¤ ê²€ì¦ ì™„ë£Œ: ì´ìŠˆ {len(issues)}ê°œ")
        return issues

    def _is_valid_fk_reference(self, fk: PendingFKCheck) -> bool:
        """FK ì°¸ì¡°ê°€ ìœ íš¨í•œì§€ í™•ì¸ (PK ë˜ëŠ” UNIQUE)"""
        ref_key = self._make_table_key(fk.source_schema, fk.ref_table)
        indexes = self.table_indexes.get(ref_key, [])

        for idx in indexes:
            if idx.is_primary or idx.is_unique:
                if idx.covers_columns(fk.ref_columns):
                    return True

        return False

    # ================================================================
    # í†µí•© ë¶„ì„ ë©”ì„œë“œ
    # ================================================================
    def analyze_dump_folder(self, dump_path: str) -> DumpAnalysisResult:
        """ë¤í”„ í´ë” 2-Pass ë¶„ì„"""
        from datetime import datetime

        self.clear_state()

        path = Path(dump_path)
        if not path.exists():
            raise FileNotFoundError(f"ë¤í”„ í´ë” ì—†ìŒ: {dump_path}")

        self._log(f"ğŸ” 2-Pass ë¶„ì„ ì‹œì‘: {dump_path}")

        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        sql_files = list(path.glob("*.sql"))
        data_files = [f for f in path.glob("*.tsv") if not str(f).endswith('.zst')]

        self._log(f"  SQL: {len(sql_files)}ê°œ, ë°ì´í„°: {len(data_files)}ê°œ")

        # Pass 1: ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        self.pass1_collect_metadata(sql_files)

        # Pass 2: ì „ì²´ ë¶„ì„
        all_issues = self.pass2_full_analysis(sql_files + data_files)

        # Pass 2.5: í¬ë¡œìŠ¤ ê²€ì¦
        cross_issues = self.pass2_5_cross_validate()
        all_issues.extend(cross_issues)

        # ìš”ì•½
        error_count = sum(1 for i in all_issues if i.severity == "error")
        warning_count = sum(1 for i in all_issues if i.severity == "warning")

        self._log(f"âœ… 2-Pass ë¶„ì„ ì™„ë£Œ")
        self._log(f"  - ì˜¤ë¥˜: {error_count}ê°œ")
        self._log(f"  - ê²½ê³ : {warning_count}ê°œ")

        # ê²°ê³¼ ìƒì„±
        return DumpAnalysisResult(
            dump_path=str(dump_path),
            analyzed_at=datetime.now().isoformat(),
            total_sql_files=len(sql_files),
            total_tsv_files=len(data_files),
            compatibility_issues=all_issues
        )


# ============================================================
# í™•ì¥ DumpFileAnalyzer (2-Pass ì§€ì›)
# ============================================================

class EnhancedDumpFileAnalyzer(DumpFileAnalyzer):
    """í™•ì¥ ë¤í”„ íŒŒì¼ ë¶„ì„ê¸° (2-Pass ì§€ì›)"""

    def __init__(self, use_two_pass: bool = True):
        super().__init__()
        self.use_two_pass = use_two_pass

        if use_two_pass:
            self._two_pass_analyzer = TwoPassAnalyzer()
        else:
            self._two_pass_analyzer = None

    def analyze_dump_folder(self, dump_path: str) -> DumpAnalysisResult:
        """ë¤í”„ í´ë” ë¶„ì„ (2-Pass ë˜ëŠ” ê¸°ì¡´ ë°©ì‹)"""
        if self.use_two_pass and self._two_pass_analyzer:
            self._two_pass_analyzer.set_callbacks(
                self._progress_callback,
                self._issue_callback
            )
            return self._two_pass_analyzer.analyze_dump_folder(dump_path)
        else:
            # ê¸°ì¡´ ë‹¨ì¼ íŒ¨ìŠ¤ ë¶„ì„
            return super().analyze_dump_folder(dump_path)

    def export_report(self, result: DumpAnalysisResult, filepath: str, format: str = 'json'):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë¦¬í¬íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if REPORT_EXPORTER_AVAILABLE:
            exporter = ReportExporter(result.compatibility_issues)
            exporter.save_to_file(filepath, format)
            return filepath
        else:
            raise ImportError("ReportExporter ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
