"""
MySQL Shell ê¸°ë°˜ ë³‘ë ¬ Export/Import
- ë©€í‹°ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬
- FK ì˜ì¡´ì„± ìë™ ë¶„ì„ ë° ì²˜ë¦¬
- ì „ì²´ ìŠ¤í‚¤ë§ˆ / ì¼ë¶€ í…Œì´ë¸” ì§€ì›
"""
import os
import re
import subprocess
import shutil
import json
import threading
import time
import glob as glob_module
from datetime import datetime
from typing import List, Dict, Set, Tuple, Callable, Optional
from dataclasses import dataclass

from src.core.db_connector import MySQLConnector


@dataclass
class MySQLShellConfig:
    """MySQL Shell ì—°ê²° ì„¤ì •"""
    host: str
    port: int
    user: str
    password: str

    def get_uri(self) -> str:
        """mysqlsh URI í˜•ì‹ ë°˜í™˜"""
        return f"{self.user}:{self.password}@{self.host}:{self.port}"

    def get_masked_uri(self) -> str:
        """ë¹„ë°€ë²ˆí˜¸ ë§ˆìŠ¤í‚¹ëœ URI"""
        return f"{self.user}:****@{self.host}:{self.port}"


class MySQLShellChecker:
    """MySQL Shell ì„¤ì¹˜ í™•ì¸"""

    @staticmethod
    def check_installation() -> Tuple[bool, str, Optional[str]]:
        """
        mysqlsh ì„¤ì¹˜ í™•ì¸

        Returns:
            (ì„¤ì¹˜ì—¬ë¶€, ë©”ì‹œì§€, ë²„ì „)
        """
        try:
            result = subprocess.run(
                ["mysqlsh", "--version"],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                version = result.stdout.strip()
                return True, version, version
            else:
                return False, "mysqlsh ì‹¤í–‰ ì‹¤íŒ¨", None

        except FileNotFoundError:
            return False, "mysqlshê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.", None
        except subprocess.TimeoutExpired:
            return False, "mysqlsh ë²„ì „ í™•ì¸ ì‹œê°„ ì´ˆê³¼", None
        except Exception as e:
            return False, f"ì˜¤ë¥˜: {str(e)}", None

    @staticmethod
    def get_install_guide() -> str:
        """ì„¤ì¹˜ ê°€ì´ë“œ ë°˜í™˜"""
        return """
MySQL Shell ì„¤ì¹˜ ë°©ë²•:

[Windows]
1. https://dev.mysql.com/downloads/shell/ ì—ì„œ ë‹¤ìš´ë¡œë“œ
2. MySQL Shell 8.x Windows (x86, 64-bit) MSI Installer ì„ íƒ
3. ì„¤ì¹˜ í›„ PATHì— ìë™ ì¶”ê°€ë¨

[macOS]
brew install mysql-shell

[Linux (Ubuntu/Debian)]
sudo apt-get install mysql-shell

[Linux (RHEL/CentOS)]
sudo yum install mysql-shell
"""


class ForeignKeyResolver:
    """FK ì˜ì¡´ì„± ë¶„ì„ ë° í•´ê²°"""

    def __init__(self, connector: MySQLConnector):
        self.connector = connector

    def get_all_dependencies(self, schema: str) -> Dict[str, Set[str]]:
        """
        ìŠ¤í‚¤ë§ˆ ë‚´ ëª¨ë“  FK ì˜ì¡´ì„± ì¡°íšŒ

        Returns:
            { table: set(ì°¸ì¡°í•˜ëŠ” ë¶€ëª¨ í…Œì´ë¸”ë“¤) }
        """
        query = """
        SELECT TABLE_NAME, REFERENCED_TABLE_NAME
        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
        WHERE TABLE_SCHEMA = %s
          AND REFERENCED_TABLE_NAME IS NOT NULL
        """
        rows = self.connector.execute(query, (schema,))

        deps = {}
        for row in rows:
            table = row['TABLE_NAME']
            ref_table = row['REFERENCED_TABLE_NAME']
            if table != ref_table:  # ìê¸° ì°¸ì¡° ì œì™¸
                if table not in deps:
                    deps[table] = set()
                deps[table].add(ref_table)

        return deps

    def resolve_required_tables(
        self,
        selected_tables: List[str],
        schema: str
    ) -> Tuple[List[str], List[str]]:
        """
        ì„ íƒëœ í…Œì´ë¸”ì— í•„ìš”í•œ FK ë¶€ëª¨ í…Œì´ë¸” ìë™ ì¶”ê°€

        Args:
            selected_tables: ì‚¬ìš©ìê°€ ì„ íƒí•œ í…Œì´ë¸” ëª©ë¡
            schema: ìŠ¤í‚¤ë§ˆëª…

        Returns:
            (ì „ì²´ í•„ìš” í…Œì´ë¸” ëª©ë¡, ìë™ ì¶”ê°€ëœ í…Œì´ë¸” ëª©ë¡)
        """
        all_deps = self.get_all_dependencies(schema)

        required = set(selected_tables)
        added = []

        # ì¬ê·€ì ìœ¼ë¡œ ë¶€ëª¨ í…Œì´ë¸” ì¶”ê°€
        changed = True
        while changed:
            changed = False
            for table in list(required):
                if table in all_deps:
                    for parent in all_deps[table]:
                        if parent not in required:
                            required.add(parent)
                            added.append(parent)
                            changed = True

        # ì •ë ¬ëœ ëª©ë¡ ë°˜í™˜
        return sorted(list(required)), sorted(added)


class MySQLShellExporter:
    """MySQL Shell ê¸°ë°˜ Export"""

    def __init__(self, config: MySQLShellConfig):
        self.config = config
        self._connector: Optional[MySQLConnector] = None

    def _get_connector(self) -> MySQLConnector:
        """ë‚´ë¶€ ì—°ê²° ê´€ë¦¬"""
        if self._connector is None:
            self._connector = MySQLConnector(
                self.config.host,
                self.config.port,
                self.config.user,
                self.config.password
            )
            self._connector.connect()
        return self._connector

    def _cleanup(self):
        """ì—°ê²° ì •ë¦¬"""
        if self._connector:
            self._connector.disconnect()
            self._connector = None

    def export_full_schema(
        self,
        schema: str,
        output_dir: str,
        threads: int = 4,
        compression: str = "zstd",
        progress_callback: Optional[Callable[[str], None]] = None,
        table_progress_callback: Optional[Callable[[int, int, str], None]] = None,
        detail_callback: Optional[Callable[[dict], None]] = None,
        table_status_callback: Optional[Callable[[str, str, str], None]] = None,
        raw_output_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """
        ì „ì²´ ìŠ¤í‚¤ë§ˆ Export (ë³‘ë ¬ ì²˜ë¦¬)

        Args:
            schema: ìŠ¤í‚¤ë§ˆëª…
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            threads: ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
            compression: ì••ì¶• ë°©ì‹ (zstd, gzip, none)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± (msg)
            table_progress_callback: í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ì½œë°± (current, total, table_name)
            detail_callback: ìƒì„¸ ì§„í–‰ ì •ë³´ ì½œë°± (percent, mb_done, mb_total, speed)
            table_status_callback: í…Œì´ë¸”ë³„ ìƒíƒœ ì½œë°± (table_name, status, message)
            raw_output_callback: mysqlsh ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°±

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            # í…Œì´ë¸” ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ìš©)
            tables = []
            if table_progress_callback:
                if progress_callback:
                    progress_callback("í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì¤‘...")
                connector = self._get_connector()
                tables = connector.get_tables(schema)

            # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
            if os.path.exists(output_dir):
                if progress_callback:
                    progress_callback(f"ê¸°ì¡´ í´ë” ì‚­ì œ ì¤‘: {output_dir}")
                shutil.rmtree(output_dir)
                # Windowsì—ì„œ ì‚­ì œê°€ ì™„ì „íˆ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                wait_count = 0
                while os.path.exists(output_dir) and wait_count < 20:
                    time.sleep(0.1)
                    wait_count += 1

            # mysqlshê°€ ì§ì ‘ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ë„ë¡ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë§Œ í™•ì¸
            parent_dir = os.path.dirname(output_dir)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)

            if progress_callback:
                if tables:
                    progress_callback(f"ìŠ¤í‚¤ë§ˆ '{schema}' Export ì‹œì‘ ({len(tables)}ê°œ í…Œì´ë¸”, ìŠ¤ë ˆë“œ: {threads})")
                else:
                    progress_callback(f"ìŠ¤í‚¤ë§ˆ '{schema}' Export ì‹œì‘ (ìŠ¤ë ˆë“œ: {threads})")

            # mysqlsh ëª…ë ¹ êµ¬ì„±
            output_dir_escaped = output_dir.replace('\\', '/')
            js_code = f"""
util.dumpSchemas(["{schema}"], "{output_dir_escaped}", {{
    threads: {threads},
    compression: "{compression}",
    chunking: true,
    bytesPerChunk: "64M",
    showProgress: true
}});
"""

            success, msg = self._run_mysqlsh(
                js_code,
                progress_callback,
                output_dir=output_dir,
                schema=schema,
                tables=tables if tables else None,
                table_progress_callback=table_progress_callback,
                detail_callback=detail_callback,
                table_status_callback=table_status_callback,
                raw_output_callback=raw_output_callback
            )

            if success:
                # Export ì„±ê³µ í›„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±
                self._write_metadata(output_dir, schema, "full", tables)
                if progress_callback:
                    progress_callback(f"âœ… Export ì™„ë£Œ: {output_dir}")

            return success, msg

        except Exception as e:
            return False, f"Export ì˜¤ë¥˜: {str(e)}"

    def export_tables(
        self,
        schema: str,
        tables: List[str],
        output_dir: str,
        threads: int = 4,
        compression: str = "zstd",
        include_fk_parents: bool = True,
        progress_callback: Optional[Callable[[str], None]] = None,
        table_progress_callback: Optional[Callable[[int, int, str], None]] = None,
        detail_callback: Optional[Callable[[dict], None]] = None,
        table_status_callback: Optional[Callable[[str, str, str], None]] = None,
        raw_output_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str, List[str]]:
        """
        ì„ íƒëœ í…Œì´ë¸”ë§Œ Export (FK ì˜ì¡´ì„± ìë™ ì²˜ë¦¬)

        Args:
            schema: ìŠ¤í‚¤ë§ˆëª…
            tables: ë‚´ë³´ë‚¼ í…Œì´ë¸” ëª©ë¡
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            threads: ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
            compression: ì••ì¶• ë°©ì‹
            include_fk_parents: FK ë¶€ëª¨ í…Œì´ë¸” ìë™ í¬í•¨ ì—¬ë¶€
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± (msg)
            table_progress_callback: í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ì½œë°± (current, total, table_name)
            detail_callback: ìƒì„¸ ì§„í–‰ ì •ë³´ ì½œë°± (percent, mb_done, mb_total, speed)
            table_status_callback: í…Œì´ë¸”ë³„ ìƒíƒœ ì½œë°± (table_name, status, message)
            raw_output_callback: mysqlsh ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°±

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€, ì‹¤ì œ Exportëœ í…Œì´ë¸” ëª©ë¡)
        """
        try:
            added_tables = []
            final_tables = tables.copy()

            # FK ë¶€ëª¨ í…Œì´ë¸” ìë™ ì¶”ê°€
            if include_fk_parents:
                if progress_callback:
                    progress_callback("FK ì˜ì¡´ì„± ë¶„ì„ ì¤‘...")

                connector = self._get_connector()
                resolver = ForeignKeyResolver(connector)
                final_tables, added_tables = resolver.resolve_required_tables(tables, schema)

                if added_tables and progress_callback:
                    progress_callback(f"FK ì˜ì¡´ì„±ìœ¼ë¡œ {len(added_tables)}ê°œ í…Œì´ë¸” ì¶”ê°€: {', '.join(added_tables)}")

            # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
            if os.path.exists(output_dir):
                if progress_callback:
                    progress_callback(f"ê¸°ì¡´ í´ë” ì‚­ì œ ì¤‘: {output_dir}")
                shutil.rmtree(output_dir)
                # Windowsì—ì„œ ì‚­ì œê°€ ì™„ì „íˆ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                wait_count = 0
                while os.path.exists(output_dir) and wait_count < 20:
                    time.sleep(0.1)
                    wait_count += 1

            # mysqlshê°€ ì§ì ‘ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ë„ë¡ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë§Œ í™•ì¸
            parent_dir = os.path.dirname(output_dir)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)

            if progress_callback:
                progress_callback(f"{len(final_tables)}ê°œ í…Œì´ë¸” Export ì‹œì‘ (ìŠ¤ë ˆë“œ: {threads})")

            # í…Œì´ë¸” ëª©ë¡ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            tables_json = json.dumps(final_tables)
            output_dir_escaped = output_dir.replace('\\', '/')

            # mysqlsh ëª…ë ¹ êµ¬ì„±
            js_code = f"""
util.dumpTables("{schema}", {tables_json}, "{output_dir_escaped}", {{
    threads: {threads},
    compression: "{compression}",
    chunking: true,
    bytesPerChunk: "64M",
    showProgress: true
}});
"""

            success, msg = self._run_mysqlsh(
                js_code,
                progress_callback,
                output_dir=output_dir,
                schema=schema,
                tables=final_tables,
                table_progress_callback=table_progress_callback,
                detail_callback=detail_callback,
                table_status_callback=table_status_callback,
                raw_output_callback=raw_output_callback
            )

            if success:
                # Export ì„±ê³µ í›„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±
                self._write_metadata(output_dir, schema, "partial", final_tables, added_tables)
                if progress_callback:
                    progress_callback(f"âœ… {len(final_tables)}ê°œ í…Œì´ë¸” Export ì™„ë£Œ")
                return True, f"{len(final_tables)}ê°œ í…Œì´ë¸” Export ì™„ë£Œ", final_tables
            else:
                return False, msg, []

        except Exception as e:
            return False, f"Export ì˜¤ë¥˜: {str(e)}", []
        finally:
            self._cleanup()

    def _run_mysqlsh(
        self,
        js_code: str,
        progress_callback: Optional[Callable[[str], None]] = None,
        output_dir: Optional[str] = None,
        schema: Optional[str] = None,
        tables: Optional[List[str]] = None,
        table_progress_callback: Optional[Callable[[int, int, str], None]] = None,
        detail_callback: Optional[Callable[[dict], None]] = None,
        table_status_callback: Optional[Callable[[str, str, str], None]] = None,
        raw_output_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """
        mysqlsh ëª…ë ¹ ì‹¤í–‰ (í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì§€ì› + ì‹¤ì‹œê°„ stdout íŒŒì‹±)

        Args:
            js_code: ì‹¤í–‰í•  JavaScript ì½”ë“œ
            progress_callback: ì¼ë°˜ ë©”ì‹œì§€ ì½œë°±
            output_dir: ì¶œë ¥ í´ë” (ëª¨ë‹ˆí„°ë§ìš©)
            schema: ìŠ¤í‚¤ë§ˆëª… (ëª¨ë‹ˆí„°ë§ìš©)
            tables: í…Œì´ë¸” ëª©ë¡ (ëª¨ë‹ˆí„°ë§ìš©)
            table_progress_callback: í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ì½œë°± (current, total, table_name)
            detail_callback: ìƒì„¸ ì§„í–‰ ì •ë³´ ì½œë°± (percent, mb_done, mb_total, speed)
            table_status_callback: í…Œì´ë¸”ë³„ ìƒíƒœ ì½œë°± (table_name, status, message)
            raw_output_callback: mysqlsh ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°±
        """
        try:
            # mysqlsh ëª…ë ¹ êµ¬ì„±
            cmd = [
                "mysqlsh",
                "--uri", self.config.get_uri(),
                "--js",
                "-e", js_code
            ]

            if progress_callback:
                progress_callback(f"mysqlsh ì‹¤í–‰: {self.config.get_masked_uri()}")

            # í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì„¤ì •
            stop_monitor = threading.Event()
            monitor_thread = None
            process = None

            if output_dir and schema and tables and table_progress_callback:
                monitor_thread = threading.Thread(
                    target=self._monitor_export_progress,
                    args=(output_dir, schema, tables, table_progress_callback, table_status_callback, stop_monitor),
                    daemon=True
                )
                monitor_thread.start()

            # Popenìœ¼ë¡œ ì‹¤í–‰ (ì‹¤ì‹œê°„ ì¶œë ¥ ì½ê¸° + ëª¨ë‹ˆí„°ë§ ë³‘í–‰)
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                encoding='utf-8',
                errors='replace',
                bufsize=1,
                universal_newlines=True
            )

            # ì‹¤ì‹œê°„ stdout íŒŒì‹± (Export ì§„í–‰ë¥ )
            completed_tables_set = set()
            last_percent = 0

            while True:
                line = process.stdout.readline()

                if not line and process.poll() is not None:
                    break

                if line:
                    stripped_line = line.strip()
                    timestamp = datetime.now().strftime("%H:%M:%S")

                    # ì½˜ì†” ë””ë²„ê¹… ì¶œë ¥
                    print(f"[mysqlsh export] {stripped_line}")

                    # ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°±
                    if raw_output_callback:
                        raw_output_callback(f"[{timestamp}] {stripped_line}")

                    # --- íŒ¨í„´ 1: ìƒì„¸ ì§„í–‰ ì •ë³´ íŒŒì‹± ---
                    # Export ì˜ˆ: "4 thds dumping - 27% (2.24M rows / ~8.23M rows), 25.39K rows/s, 6.60 MB/s uncompressed"
                    # í¼ì„¼íŠ¸ë§Œ íŒŒì‹± (rows ì •ë³´ëŠ” ë°ì´í„° í¬ê¸°ë¡œ ë³€í™˜ ë¶ˆê°€)
                    percent_match = re.search(r'dumping.*?(\d+)%', stripped_line)
                    if percent_match and detail_callback:
                        percent = int(percent_match.group(1))

                        # ì†ë„ íŒŒì‹± (MB/s, KB/s ë“± - "uncompressed" ì•ì˜ ì†ë„)
                        speed_match = re.search(r'([0-9.]+)\s*([KMGT]?B)/s\s+uncompressed', stripped_line)
                        speed_str = "0 B/s"
                        if speed_match:
                            speed_str = f"{speed_match.group(1)} {speed_match.group(2)}/s"

                        # ì§„í–‰ë¥ ì´ ì¦ê°€í•œ ê²½ìš°ì—ë§Œ ì½œë°± í˜¸ì¶œ (ì¤‘ë³µ ë°©ì§€)
                        if percent > last_percent:
                            detail_callback({
                                'percent': percent,
                                'mb_done': 0,  # ExportëŠ” rowsë§Œ í‘œì‹œí•˜ë¯€ë¡œ 0ìœ¼ë¡œ
                                'mb_total': 0,
                                'speed': speed_str
                            })
                            last_percent = percent

                    # --- íŒ¨í„´ 2: í…Œì´ë¸” ì™„ë£Œ ê°ì§€ ---
                    # ì˜ˆ: "Writing DDL for table `schema`.`table_name`"
                    # ì˜ˆ: "Writing data for table `schema`.`table_name`"
                    table_match = re.search(r"`([^`]+)`\.`([^`]+)`", stripped_line)
                    if table_match and tables and table_status_callback:
                        table_name = table_match.group(2)

                        if table_name in tables:
                            # "Writing" íŒ¨í„´ì¸ ê²½ìš° loading ìƒíƒœë¡œ
                            if "Writing" in stripped_line or "dumping" in stripped_line.lower():
                                if table_name not in completed_tables_set:
                                    table_status_callback(table_name, 'loading', '')
                            # "done" íŒ¨í„´ì¸ ê²½ìš° ì™„ë£Œ ìƒíƒœë¡œ
                            elif "done" in stripped_line.lower():
                                if table_name not in completed_tables_set:
                                    completed_tables_set.add(table_name)
                                    table_status_callback(table_name, 'done', '')

            # ì™„ë£Œ ëŒ€ê¸°
            rc = process.poll()
            if rc is None:
                process.wait(timeout=3600)
                rc = process.returncode

            stdout = ""
            stderr = ""

            # ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            stop_monitor.set()
            if monitor_thread:
                monitor_thread.join(timeout=2)

            if rc == 0:
                # ìµœì¢… ì§„í–‰ë¥  100% í‘œì‹œ
                if detail_callback:
                    detail_callback({
                        'percent': 100,
                        'mb_done': 0,
                        'mb_total': 0,
                        'speed': '0 B/s'
                    })

                # ëª¨ë“  í…Œì´ë¸” ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                if tables and table_status_callback:
                    for table in tables:
                        if table not in completed_tables_set:
                            table_status_callback(table, 'done', '')

                return True, "ì„±ê³µ"
            else:
                error_msg = stderr or stdout or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                return False, error_msg

        except subprocess.TimeoutExpired:
            stop_monitor.set()
            if process:
                process.kill()
            return False, "ì‘ì—… ì‹œê°„ ì´ˆê³¼ (1ì‹œê°„)"
        except Exception as e:
            stop_monitor.set()
            return False, str(e)

    def _monitor_export_progress(
        self,
        output_dir: str,
        schema: str,
        tables: List[str],
        callback: Callable[[int, int, str], None],
        table_status_callback: Optional[Callable[[str, str, str], None]],
        stop_event: threading.Event
    ):
        """
        ì¶œë ¥ í´ë”ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì—¬ í…Œì´ë¸”ë³„ Export ì§„í–‰ë¥  ì¶”ì 

        mysqlshëŠ” ë°ì´í„° export ì‹œ {schema}@{table}@@{chunk}.zst íŒŒì¼ ìƒì„±
        (.json/.sqlì€ ì´ˆë°˜ì— ì¼ê´„ ìƒì„±ë˜ë¯€ë¡œ ì™„ë£Œ íŒì •ì— ë¶€ì í•©)

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            schema: ìŠ¤í‚¤ë§ˆëª…
            tables: í…Œì´ë¸” ëª©ë¡
            callback: í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ì½œë°± (current, total, table_name)
            table_status_callback: í…Œì´ë¸”ë³„ ìƒíƒœ ì½œë°± (table_name, status, message)
            stop_event: ì¤‘ì§€ ì´ë²¤íŠ¸
        """
        total = len(tables)
        completed_tables = set()
        tables_set = set(tables)  # ë¹ ë¥¸ ì¡°íšŒìš©

        # í´ë” ì‚­ì œ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
        wait_count = 0
        while wait_count < 50:
            if not os.path.exists(output_dir):
                break
            time.sleep(0.1)
            wait_count += 1

        # ëª¨ë‹ˆí„°ë§ ì‹œì‘ ì‹œ ê¸°ì¡´ ë°ì´í„° íŒŒì¼ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ (baseline)
        baseline_tables = set()
        if os.path.exists(output_dir):
            existing = glob_module.glob(os.path.join(output_dir, f"{schema}@*@@*.zst"))
            for f in existing:
                filename = os.path.basename(f)
                if "@@" in filename:
                    table_part = filename.split("@@")[0]
                    table_name = table_part[len(f"{schema}@"):]
                    baseline_tables.add(table_name)

        # mysqlshê°€ í´ë”ë¥¼ ìƒì„±í•  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
        folder_wait = 0
        while not stop_event.is_set() and folder_wait < 100:
            if os.path.exists(output_dir):
                break
            time.sleep(0.1)
            folder_wait += 1

        # ëª¨ë“  í…Œì´ë¸”ì„ pending ìƒíƒœë¡œ ì´ˆê¸°í™”
        if table_status_callback:
            for table in tables:
                table_status_callback(table, 'pending', '')

        while not stop_event.is_set():
            try:
                # ë°ì´í„° íŒŒì¼ (.zst) í™•ì¸
                pattern = os.path.join(output_dir, f"{schema}@*@@*.zst")
                data_files = glob_module.glob(pattern)

                for data_file in data_files:
                    filename = os.path.basename(data_file)

                    # {schema}@{table}@@{chunk}.zst í˜•ì‹ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
                    if "@@" in filename:
                        table_part = filename.split("@@")[0]
                        table_name = table_part[len(f"{schema}@"):]

                        # baseline í…Œì´ë¸”ì€ ë¬´ì‹œ
                        if table_name in baseline_tables:
                            continue

                        if table_name in tables_set and table_name not in completed_tables:
                            completed_tables.add(table_name)
                            callback(len(completed_tables), total, table_name)
                            # í…Œì´ë¸” ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
                            if table_status_callback:
                                table_status_callback(table_name, 'done', '')

                # ëª¨ë“  í…Œì´ë¸” ì™„ë£Œ í™•ì¸
                if len(completed_tables) >= total:
                    break

                time.sleep(0.15)

            except Exception:
                pass

        # ìµœì¢… ì •ë¦¬: ë¹ˆ í…Œì´ë¸” ì²˜ë¦¬ (ë°ì´í„° íŒŒì¼ ì—†ì´ .jsonë§Œ ìˆëŠ” ê²½ìš°)
        if len(completed_tables) < total:
            time.sleep(0.3)
            try:
                # .json íŒŒì¼ë¡œ ëª¨ë“  í…Œì´ë¸” í™•ì¸
                json_pattern = os.path.join(output_dir, f"{schema}@*.json")
                json_files = glob_module.glob(json_pattern)

                for json_file in json_files:
                    filename = os.path.basename(json_file)
                    if filename.startswith(f"{schema}@") and filename.endswith(".json"):
                        table_name = filename[len(f"{schema}@"):-5]

                        if table_name in baseline_tables:
                            continue

                        if table_name in tables_set and table_name not in completed_tables:
                            # ë¹ˆ í…Œì´ë¸”ë¡œ ê°„ì£¼í•˜ì—¬ ì™„ë£Œ ì²˜ë¦¬
                            completed_tables.add(table_name)
                            callback(len(completed_tables), total, table_name)
                            # í…Œì´ë¸” ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
                            if table_status_callback:
                                table_status_callback(table_name, 'done', '')
            except Exception:
                pass

    def _write_metadata(
        self,
        output_dir: str,
        schema: str,
        export_type: str,
        tables: List[str],
        added_tables: List[str] = None
    ):
        """Export ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„±"""
        metadata = {
            "export_time": datetime.now().isoformat(),
            "schema": schema,
            "type": export_type,
            "tables": tables,
            "added_fk_tables": added_tables or [],
            "source": f"{self.config.host}:{self.config.port}"
        }

        filepath = os.path.join(output_dir, "_export_metadata.json")
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)


class MySQLShellImporter:
    """MySQL Shell ê¸°ë°˜ Import"""

    def __init__(self, config: MySQLShellConfig):
        self.config = config

    def _analyze_dump_metadata(self, dump_dir: str) -> Optional[Dict]:
        """
        Dump ë©”íƒ€ë°ì´í„° ë¶„ì„ - í…Œì´ë¸”ë³„ Chunk ì •ë³´ ì¶”ì¶œ

        Args:
            dump_dir: Dump ë””ë ‰í† ë¦¬ ê²½ë¡œ

        Returns:
            {
                'chunk_counts': {'table_name': chunk_count, ...},
                'table_sizes': {'table_name': bytes, ...},
                'total_bytes': int,
                'schema': str
            }
            ë˜ëŠ” None (ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
        """
        try:
            done_json_path = os.path.join(dump_dir, '@.done.json')

            if not os.path.exists(done_json_path):
                return None

            with open(done_json_path, 'r', encoding='utf-8') as f:
                done_data = json.load(f)

            # chunkFileBytesì—ì„œ í…Œì´ë¸”ë³„ chunk ìˆ˜ ê³„ì‚°
            chunk_counts = {}  # {'df_subs': 81, 'df_call_logs': 8, ...}
            chunk_file_bytes = done_data.get('chunkFileBytes', {})

            for chunk_file in chunk_file_bytes.keys():
                # "dataflare@df_subs@15.tsv.zst" ë˜ëŠ” "dataflare@df_subs@@0.tsv.zst" í˜•ì‹
                if '@' in chunk_file:
                    parts = chunk_file.split('@')
                    if len(parts) >= 3:
                        # schema@table@chunk ë˜ëŠ” schema@table@@chunk í˜•ì‹
                        table_name = parts[1]
                        chunk_counts[table_name] = chunk_counts.get(table_name, 0) + 1

            # tableDataBytesì—ì„œ í…Œì´ë¸”ë³„ í¬ê¸° ì¶”ì¶œ
            table_data_bytes = done_data.get('tableDataBytes', {})
            table_sizes = {}
            schema = None

            # tableDataBytes êµ¬ì¡°: {'schema_name': {'table_name': bytes, ...}}
            for schema_name, tables in table_data_bytes.items():
                schema = schema_name  # ìŠ¤í‚¤ë§ˆëª… ì €ì¥
                for table_name, size_bytes in tables.items():
                    table_sizes[table_name] = size_bytes

            total_bytes = done_data.get('dataBytes', 0)

            return {
                'chunk_counts': chunk_counts,
                'table_sizes': table_sizes,
                'total_bytes': total_bytes,
                'schema': schema or ''
            }

        except Exception as e:
            # ë©”íƒ€ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
            return None

    def import_dump(
        self,
        input_dir: str,
        target_schema: Optional[str] = None,
        threads: int = 4,
        import_mode: str = "replace",
        timezone_sql: Optional[str] = None,
        progress_callback: Optional[Callable[[str], None]] = None,
        table_progress_callback: Optional[Callable[[int, int, str], None]] = None,
        detail_callback: Optional[Callable[[dict], None]] = None,
        table_status_callback: Optional[Callable[[str, str, str], None]] = None,
        raw_output_callback: Optional[Callable[[str], None]] = None,
        retry_tables: Optional[List[str]] = None,
        metadata_callback: Optional[Callable[[dict], None]] = None
    ) -> Tuple[bool, str, dict]:
        """
        Dump íŒŒì¼ Import (3ê°€ì§€ ëª¨ë“œ ì§€ì›)

        Args:
            input_dir: Dump ë””ë ‰í† ë¦¬ ê²½ë¡œ
            target_schema: ëŒ€ìƒ ìŠ¤í‚¤ë§ˆ (Noneì´ë©´ ì›ë³¸ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            threads: ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
            import_mode: Import ëª¨ë“œ
                - "merge": ë³‘í•© (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
                - "replace": ì „ì²´ êµì²´ (ëª¨ë“  ê°ì²´ ì¬ìƒì„±, resetProgress=true)
                - "recreate": ì™„ì „ ì¬ìƒì„± (ìŠ¤í‚¤ë§ˆ DROP í›„ ì¬ìƒì„±)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±
            detail_callback: ìƒì„¸ ì§„í–‰ ì •ë³´ ì½œë°± (percent, mb_done, mb_total, rows_sec)
            table_status_callback: í…Œì´ë¸”ë³„ ìƒíƒœ ì½œë°± (table_name, status, message)
            raw_output_callback: mysqlsh ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°±
            retry_tables: ì¬ì‹œë„í•  í…Œì´ë¸” ëª©ë¡ (ì„ íƒì )
            metadata_callback: ë©”íƒ€ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì½œë°± (chunk_counts, table_sizes ë“±)

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€, í…Œì´ë¸”ë³„ ê²°ê³¼ dict)
        """
        # í…Œì´ë¸”ë³„ Import ê²°ê³¼ ì¶”ì 
        import_results: dict = {}
        try:
            # Dump ë©”íƒ€ë°ì´í„° ë¶„ì„ (@.done.json)
            dump_metadata = self._analyze_dump_metadata(input_dir)
            if dump_metadata and progress_callback:
                total_size_gb = dump_metadata['total_bytes'] / (1024 * 1024 * 1024)
                large_tables = [
                    (name, size) for name, size in dump_metadata['table_sizes'].items()
                    if size > 100_000_000  # 100MB ì´ìƒ
                ]
                large_tables.sort(key=lambda x: -x[1])

                progress_callback(f"ğŸ“Š Dump ë©”íƒ€ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
                progress_callback(f"  â””â”€ ì „ì²´ ë°ì´í„° í¬ê¸°: {total_size_gb:.2f} GB")

                if large_tables:
                    progress_callback(f"  â””â”€ ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ({len(large_tables)}ê°œ):")
                    for name, size in large_tables[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                        size_mb = size / (1024 * 1024)
                        chunk_count = dump_metadata['chunk_counts'].get(name, 1)
                        progress_callback(f"     â€¢ {name}: {size_mb:.1f} MB ({chunk_count} chunks)")

            # ë©”íƒ€ë°ì´í„° ì½œë°± í˜¸ì¶œ (UIë¡œ ì „ë‹¬)
            if dump_metadata and metadata_callback:
                metadata_callback(dump_metadata)

            # Export ë©”íƒ€ë°ì´í„° í™•ì¸ (_export_metadata.json)
            metadata_path = os.path.join(input_dir, "_export_metadata.json")
            metadata = None
            source_schema = None
            tables_to_import = []

            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                source_schema = metadata.get('schema')
                tables_to_import = metadata.get('tables', [])
                if progress_callback:
                    progress_callback(f"ë©”íƒ€ë°ì´í„° í™•ì¸: {source_schema} ({metadata.get('type')}) - {len(tables_to_import)}ê°œ í…Œì´ë¸”")

            # ì¬ì‹œë„ ëª¨ë“œì¸ ê²½ìš° í…Œì´ë¸” ëª©ë¡ í•„í„°ë§
            if retry_tables:
                tables_to_import = [t for t in tables_to_import if t in retry_tables]
                if progress_callback:
                    progress_callback(f"ğŸ”„ ì¬ì‹œë„ ëª¨ë“œ: {len(tables_to_import)}ê°œ í…Œì´ë¸”ë§Œ Import")

            # í…Œì´ë¸” ìƒíƒœ ì´ˆê¸°í™” (pending ìƒíƒœë¡œ)
            for table in tables_to_import:
                import_results[table] = {'status': 'pending', 'message': ''}
                if table_status_callback:
                    table_status_callback(table, 'pending', '')

            # íƒ€ì„ì¡´ íŒ¨ì¹˜ (Asia/Seoul -> +09:00)
            if progress_callback:
                progress_callback("íƒ€ì„ì¡´ ë³´ì • ì¤‘... (Asia/Seoul -> +09:00)")

            patched_count = self._patch_timezone_in_dump(input_dir, progress_callback)
            if patched_count > 0 and progress_callback:
                progress_callback(f"âœ… {patched_count}ê°œ SQL íŒŒì¼ íƒ€ì„ì¡´ ë³´ì • ì™„ë£Œ")

            # ëŒ€ìƒ ìŠ¤í‚¤ë§ˆ ê²°ì •
            final_target_schema = target_schema or source_schema
            if not final_target_schema:
                return False, "ëŒ€ìƒ ìŠ¤í‚¤ë§ˆë¥¼ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", import_results

            # Import ëª¨ë“œë³„ ì²˜ë¦¬
            if import_mode == "recreate":
                # ì™„ì „ ì¬ìƒì„±: ìŠ¤í‚¤ë§ˆ DROP í›„ ì¬ìƒì„±
                if progress_callback:
                    progress_callback(f"âš ï¸ ìŠ¤í‚¤ë§ˆ '{final_target_schema}' ì™„ì „ ì¬ìƒì„± ì¤‘...")

                drop_schema_success, drop_schema_msg = self._drop_and_recreate_schema(
                    final_target_schema,
                    progress_callback
                )

                if not drop_schema_success:
                    return False, f"ìŠ¤í‚¤ë§ˆ ì¬ìƒì„± ì‹¤íŒ¨: {drop_schema_msg}", import_results

            elif import_mode == "replace":
                # ì „ì²´ êµì²´: ëª¨ë“  ê°ì²´ (í…Œì´ë¸”, ë·°, í”„ë¡œì‹œì €, ì´ë²¤íŠ¸) ì‚­ì œ í›„ ì¬ìƒì„±
                if progress_callback:
                    progress_callback(f"ğŸ”„ ì „ì²´ êµì²´ ëª¨ë“œ ì‹œì‘")
                    progress_callback(f"  â””â”€ {len(tables_to_import)}ê°œ í…Œì´ë¸”, View/Procedure/Event ì‚­ì œ ì˜ˆì •")

                # 1. í…Œì´ë¸” ì‚­ì œ
                if tables_to_import:
                    drop_success, drop_msg = self._drop_existing_tables(
                        final_target_schema,
                        tables_to_import,
                        progress_callback
                    )
                    if not drop_success:
                        return False, f"í…Œì´ë¸” ì‚­ì œ ì‹¤íŒ¨: {drop_msg}", import_results
                
                # 2. View, Procedure, Event ì‚­ì œ
                drop_objects_success, drop_objects_msg = self._drop_all_objects(
                    final_target_schema,
                    progress_callback
                )
                if not drop_objects_success:
                    return False, f"ê°ì²´ ì‚­ì œ ì‹¤íŒ¨: {drop_objects_msg}", import_results

            elif import_mode == "merge":
                # ë³‘í•©: ê¸°ì¡´ ë°ì´í„° ìœ ì§€, ìƒˆ ê²ƒë§Œ ì¶”ê°€
                if progress_callback:
                    progress_callback(f"ì¦ë¶„ ë³‘í•© ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„° ìœ ì§€")

            else:
                return False, f"ì•Œ ìˆ˜ ì—†ëŠ” Import ëª¨ë“œ: {import_mode}", import_results

            if progress_callback:
                progress_callback(f"DDL + Data Import ì‹œì‘ (ìŠ¤ë ˆë“œ: {threads}, ëª¨ë“œ: {import_mode})")

            # loadDump ì˜µì…˜ êµ¬ì„± (ëª¨ë“œë³„)
            options = [
                f"threads: {threads}",
                "loadDdl: true",  # DDL(í…Œì´ë¸” êµ¬ì¡°) ë¡œë“œ
                "loadData: true",  # Data ë¡œë“œ
                "showProgress: true"
            ]

            # ëª¨ë“œë³„ ì˜µì…˜
            if import_mode == "replace":
                # ì „ì²´ êµì²´: resetProgressë¡œ View/Procedure/Eventë„ ì¬ìƒì„±
                options.append("resetProgress: true")
                options.append("ignoreExistingObjects: false")
            elif import_mode == "merge":
                # ë³‘í•©: ê¸°ì¡´ ê°ì²´ ë¬´ì‹œ
                options.append("resetProgress: false")
                options.append("ignoreExistingObjects: true")
            elif import_mode == "recreate":
                # ì™„ì „ ì¬ìƒì„±: ìŠ¤í‚¤ë§ˆê°€ ë¹„ì–´ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ì„¤ì •
                options.append("resetProgress: true")
                options.append("ignoreExistingObjects: false")

            if target_schema:
                options.append(f'schema: "{target_schema}"')

            options_str = ", ".join(options)
            input_dir_escaped = input_dir.replace('\\', '/')

            # mysqlsh ëª…ë ¹ êµ¬ì„± (local_infile í™œì„±í™” í•„ìš”)
            # Timezone ì„¤ì •ì´ ìˆìœ¼ë©´ util.loadDump ì´ì „ì— ì‹¤í–‰
            timezone_cmd = f'session.runSql("{timezone_sql}");' if timezone_sql else ""

            js_code = f"""
                session.runSql("SET GLOBAL local_infile = ON");
                {timezone_cmd}
                util.loadDump("{input_dir_escaped}", {{
                    {options_str}
                }});
            """

            print(js_code)

            # Import ì‹¤í–‰ (ì‹¤ì‹œê°„ ì§„í–‰ë¥  íŒŒì‹±)
            success, msg, import_results = self._run_mysqlsh_import(
                js_code,
                progress_callback,
                tables_to_import,
                table_progress_callback,
                detail_callback,
                table_status_callback,
                raw_output_callback,
                import_results
            )

            if success and progress_callback:
                progress_callback(f"âœ… Import ì™„ë£Œ (DDL + Data, ëª¨ë“œ: {import_mode})")

            return success, msg, import_results

        except Exception as e:
            return False, f"Import ì˜¤ë¥˜: {str(e)}", import_results

    def _drop_and_recreate_schema(
        self,
        schema: str,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """
        ìŠ¤í‚¤ë§ˆ ì™„ì „ ì¬ìƒì„± (DROP + CREATE)

        Args:
            schema: ìŠ¤í‚¤ë§ˆëª…
            progress_callback: ì§„í–‰ ì½œë°±

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if progress_callback:
                progress_callback(f"ğŸ—‘ï¸ ìŠ¤í‚¤ë§ˆ '{schema}' DROP ì¤‘...")

            js_code = f"""
session.runSql("DROP DATABASE IF EXISTS `{schema}`");
session.runSql("CREATE DATABASE `{schema}` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci");
"""

            success, msg = self._run_mysqlsh(js_code, progress_callback)

            if success and progress_callback:
                progress_callback(f"  â””â”€ âœ… ìŠ¤í‚¤ë§ˆ '{schema}' ì¬ìƒì„± ì™„ë£Œ")

            return success, msg

        except Exception as e:
            return False, f"ìŠ¤í‚¤ë§ˆ ì¬ìƒì„± ì˜¤ë¥˜: {str(e)}"

    def _drop_all_objects(
        self,
        schema: str,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """
        ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  View, Procedure, Event ì‚­ì œ

        Args:
            schema: ìŠ¤í‚¤ë§ˆëª…
            progress_callback: ì§„í–‰ ì½œë°±

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if progress_callback:
                progress_callback(f"ğŸ—‘ï¸ View/Procedure/Function/Event ì‚­ì œ ì¤‘...")

            # Views, Procedures, Events ì¡°íšŒ ë° ì‚­ì œ
            js_code = f"""
// Views ì‚­ì œ
var views = session.runSql("SELECT TABLE_NAME FROM information_schema.VIEWS WHERE TABLE_SCHEMA = '{schema}'").fetchAll();
for (var i = 0; i < views.length; i++) {{
    var viewName = views[i][0];
    session.runSql("DROP VIEW IF EXISTS `{schema}`.`" + viewName + "`");
}}

// Procedures ì‚­ì œ
var procedures = session.runSql("SELECT ROUTINE_NAME FROM information_schema.ROUTINES WHERE ROUTINE_SCHEMA = '{schema}' AND ROUTINE_TYPE = 'PROCEDURE'").fetchAll();
for (var i = 0; i < procedures.length; i++) {{
    var procName = procedures[i][0];
    session.runSql("DROP PROCEDURE IF EXISTS `{schema}`.`" + procName + "`");
}}

// Functions ì‚­ì œ
var functions = session.runSql("SELECT ROUTINE_NAME FROM information_schema.ROUTINES WHERE ROUTINE_SCHEMA = '{schema}' AND ROUTINE_TYPE = 'FUNCTION'").fetchAll();
for (var i = 0; i < functions.length; i++) {{
    var funcName = functions[i][0];
    session.runSql("DROP FUNCTION IF EXISTS `{schema}`.`" + funcName + "`");
}}

// Events ì‚­ì œ
var events = session.runSql("SELECT EVENT_NAME FROM information_schema.EVENTS WHERE EVENT_SCHEMA = '{schema}'").fetchAll();
for (var i = 0; i < events.length; i++) {{
    var eventName = events[i][0];
    session.runSql("DROP EVENT IF EXISTS `{schema}`.`" + eventName + "`");
}}
"""

            success, msg = self._run_mysqlsh(js_code, progress_callback)

            if success and progress_callback:
                progress_callback(f"  â””â”€ âœ… View/Procedure/Event ì‚­ì œ ì™„ë£Œ")

            return success, msg

        except Exception as e:
            return False, f"ê°ì²´ ì‚­ì œ ì˜¤ë¥˜: {str(e)}"

    def _patch_timezone_in_dump(self, input_dir: str, progress_callback: Optional[Callable[[str], None]] = None) -> int:
        """
        Dump íŒŒì¼ ë‚´ì˜ Asia/Seoul íƒ€ì„ì¡´ì„ +09:00ìœ¼ë¡œ ë³´ì •
        (íƒ€ì¼“ ì„œë²„ì— íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ë°©ì§€)

        Args:
            input_dir: Dump ë””ë ‰í† ë¦¬ ê²½ë¡œ
            progress_callback: ì§„í–‰ ì½œë°±

        Returns:
            ë³´ì •ëœ íŒŒì¼ ê°œìˆ˜
        """
        patched_count = 0
        try:
            sql_files = glob_module.glob(os.path.join(input_dir, "*.sql"))

            if progress_callback:
                progress_callback(f"  â””â”€ {len(sql_files)}ê°œ SQL íŒŒì¼ ìŠ¤ìº” ì¤‘...")

            for file_path in sql_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if "'Asia/Seoul'" in content:
                        new_content = content.replace("'Asia/Seoul'", "'+09:00'")
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(new_content)
                        patched_count += 1
                except Exception:
                    continue

            return patched_count
        except Exception:
            return 0

    def _drop_existing_tables(
        self,
        schema: str,
        tables: List[str],
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """
        Import ì „ì— ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ (FK ì˜ì¡´ì„± ìˆœì„œ ê³ ë ¤)

        Args:
            schema: ìŠ¤í‚¤ë§ˆëª…
            tables: ì‚­ì œí•  í…Œì´ë¸” ëª©ë¡
            progress_callback: ì§„í–‰ ì½œë°±

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if progress_callback:
                progress_callback(f"ğŸ—‘ï¸ í…Œì´ë¸” ì‚­ì œ ì‹œì‘ ({len(tables)}ê°œ)...")

            # JSON ë°°ì—´ë¡œ í…Œì´ë¸” ëª©ë¡ ìƒì„±
            tables_json = json.dumps(tables)

            # JavaScriptë¡œ FK ì²´í¬ ë¹„í™œì„±í™” í›„ ê° í…Œì´ë¸” ì‚­ì œ
            js_code = f"""
session.runSql("SET FOREIGN_KEY_CHECKS = 0");
var tables = {tables_json};
for (var i = 0; i < tables.length; i++) {{
    session.runSql("DROP TABLE IF EXISTS `{schema}`.`" + tables[i] + "`");
}}
session.runSql("SET FOREIGN_KEY_CHECKS = 1");
"""

            success, msg = self._run_mysqlsh(js_code, progress_callback)

            if success and progress_callback:
                progress_callback(f"  â””â”€ âœ… {len(tables)}ê°œ í…Œì´ë¸” ì‚­ì œ ì™„ë£Œ")

            return success, msg

        except Exception as e:
            return False, f"í…Œì´ë¸” ì‚­ì œ ì˜¤ë¥˜: {str(e)}"

    def _run_mysqlsh(
        self,
        js_code: str,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, str]:
        """mysqlsh ëª…ë ¹ ì‹¤í–‰"""
        try:
            cmd = [
                "mysqlsh",
                "--uri", self.config.get_uri(),
                "--js",
                "-e", js_code
            ]

            if progress_callback:
                progress_callback(f"mysqlsh ì‹¤í–‰ ì¤‘...")

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=3600
            )

            if result.returncode == 0:
                return True, "ì„±ê³µ"
            else:
                error_msg = result.stderr or result.stdout or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                return False, error_msg

        except subprocess.TimeoutExpired:
            return False, "ì‘ì—… ì‹œê°„ ì´ˆê³¼ (1ì‹œê°„)"
        except Exception as e:
            return False, str(e)

    def _run_mysqlsh_import(
        self,
        js_code: str,
        progress_callback: Optional[Callable[[str], None]] = None,
        tables: Optional[List[str]] = None,
        table_progress_callback: Optional[Callable[[int, int, str], None]] = None,
        detail_callback: Optional[Callable[[dict], None]] = None,
        table_status_callback: Optional[Callable[[str, str, str], None]] = None,
        raw_output_callback: Optional[Callable[[str], None]] = None,
        import_results: Optional[dict] = None
    ) -> Tuple[bool, str, dict]:
        """
        Importìš© mysqlsh ëª…ë ¹ ì‹¤í–‰ (ì‹¤ì‹œê°„ ì¶œë ¥ íŒŒì‹±)

        Args:
            js_code: ì‹¤í–‰í•  JavaScript ì½”ë“œ
            progress_callback: ì¼ë°˜ ë©”ì‹œì§€ ì½œë°±
            tables: Importí•  í…Œì´ë¸” ëª©ë¡ (ì§„í–‰ë¥  í‘œì‹œìš©)
            table_progress_callback: í…Œì´ë¸”ë³„ ì§„í–‰ë¥  ì½œë°±
            detail_callback: ìƒì„¸ ì§„í–‰ ì •ë³´ ì½œë°± (percent, mb_done, mb_total, rows_sec, speed)
            table_status_callback: í…Œì´ë¸”ë³„ ìƒíƒœ ì½œë°± (table_name, status, message)
            raw_output_callback: mysqlsh ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°±
            import_results: í…Œì´ë¸”ë³„ ê²°ê³¼ dict (ìˆ˜ì •ë¨)

        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€, í…Œì´ë¸”ë³„ ê²°ê³¼ dict)
        """
        if import_results is None:
            import_results = {}

        process = None
        last_completed_count = 0
        error_messages = []
        current_loading_table = None

        try:
            cmd = [
                "mysqlsh",
                "--uri", self.config.get_uri(),
                "--js",
                "-e", js_code
            ]

            if progress_callback:
                progress_callback(f"mysqlsh ì‹¤í–‰ ì¤‘...")

            # Popenìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ì‹¤ì‹œê°„ ì¶œë ¥ ì½ê¸°
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                encoding='utf-8',
                errors='replace',
                bufsize=1,
                universal_newlines=True
            )

            total_tables = len(tables) if tables else 0

            while True:
                line = process.stdout.readline()

                if not line and process.poll() is not None:
                    break

                if line:
                    stripped_line = line.strip()
                    timestamp = datetime.now().strftime("%H:%M:%S")

                    # ì½˜ì†” ë””ë²„ê¹… ì¶œë ¥
                    print(f"[mysqlsh] {stripped_line}")

                    # ì‹¤ì‹œê°„ ì¶œë ¥ ì½œë°± (UIì— ì „ë‹¬)
                    if raw_output_callback:
                        raw_output_callback(f"[{timestamp}] {stripped_line}")

                    # --- íŒ¨í„´ 1: ìƒì„¸ ì§„í–‰ ì •ë³´ íŒŒì‹± ---
                    # ì˜ˆ: "1 thds loading | 92% (88.95 MB / 96.69 MB), 1.5 MB/s (285.00 rows/s), 5 / 6 tables done"
                    detail_match = re.search(
                        r'(\d+)%\s*\(([0-9.]+)\s*([KMGT]?B)\s*/\s*([0-9.]+)\s*([KMGT]?B)\)',
                        stripped_line
                    )
                    if detail_match and detail_callback:
                        percent = int(detail_match.group(1))
                        mb_done = float(detail_match.group(2))
                        unit_done = detail_match.group(3)
                        mb_total = float(detail_match.group(4))
                        unit_total = detail_match.group(5)

                        # ë‹¨ìœ„ ë³€í™˜ (MBë¡œ í†µì¼)
                        def to_mb(value, unit):
                            if unit == 'KB':
                                return value / 1024
                            elif unit == 'GB':
                                return value * 1024
                            elif unit == 'TB':
                                return value * 1024 * 1024
                            return value  # B ë˜ëŠ” MB

                        mb_done = to_mb(mb_done, unit_done)
                        mb_total = to_mb(mb_total, unit_total)

                        # rows/s íŒŒì‹±
                        rows_match = re.search(r'([0-9.]+)\s*[Kk]?\s*rows?/s', stripped_line)
                        rows_sec = 0
                        if rows_match:
                            rows_sec = float(rows_match.group(1))
                            if 'K' in stripped_line[rows_match.start():rows_match.end()].upper():
                                rows_sec *= 1000

                        # ì†ë„ íŒŒì‹± (MB/s, KB/s ë“±)
                        speed_match = re.search(r'([0-9.]+)\s*([KMGT]?B)/s', stripped_line)
                        speed_str = "0 B/s"
                        if speed_match:
                            speed_str = f"{speed_match.group(1)} {speed_match.group(2)}/s"

                        detail_callback({
                            'percent': percent,
                            'mb_done': round(mb_done, 2),
                            'mb_total': round(mb_total, 2),
                            'rows_sec': int(rows_sec),
                            'speed': speed_str
                        })

                    # --- íŒ¨í„´ 2: í…Œì´ë¸” ì™„ë£Œ ìˆ˜ íŒŒì‹± ---
                    # ì˜ˆ: "5 / 6 tables done"
                    table_done_match = re.search(r'(\d+)\s*/\s*(\d+)\s*tables?\s*done', stripped_line, re.IGNORECASE)
                    if table_done_match and tables:
                        current_count = int(table_done_match.group(1))
                        total_in_log = int(table_done_match.group(2))

                        # ìƒˆë¡œ ì™„ë£Œëœ í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
                        if current_count > last_completed_count:
                            # ìƒˆë¡œ ì™„ë£Œëœ í…Œì´ë¸”ë“¤ ìƒíƒœ ì—…ë°ì´íŠ¸
                            for i in range(last_completed_count, min(current_count, len(tables))):
                                table_name = tables[i]
                                import_results[table_name] = {'status': 'done', 'message': ''}
                                if table_status_callback:
                                    table_status_callback(table_name, 'done', '')

                            last_completed_count = current_count

                        # í˜„ì¬ ë¡œë”© ì¤‘ì¸ í…Œì´ë¸” í‘œì‹œ
                        if current_count < len(tables):
                            loading_table = tables[current_count]
                            if loading_table != current_loading_table:
                                current_loading_table = loading_table
                                import_results[loading_table] = {'status': 'loading', 'message': ''}
                                if table_status_callback:
                                    table_status_callback(loading_table, 'loading', '')

                        if table_progress_callback:
                            table_name = tables[current_count - 1] if current_count > 0 else "..."
                            table_progress_callback(current_count, total_in_log, table_name)

                    # --- íŒ¨í„´ 3: í…Œì´ë¸” ë¡œë”© ì‹œì‘ ê°ì§€ ---
                    # ì˜ˆ: "Loading DDL and Data from ... for table `schema`.`table_name`"
                    loading_match = re.search(r"Loading.*`(\w+)`\.`(\w+)`", stripped_line)
                    if loading_match and tables:
                        table_name = loading_match.group(2)
                        if table_name in import_results:
                            import_results[table_name] = {'status': 'loading', 'message': ''}
                            if table_status_callback:
                                table_status_callback(table_name, 'loading', '')

                    # --- íŒ¨í„´ 4: ì—ëŸ¬ ê°ì§€ ---
                    # ì˜ˆ: "ERROR: ...", "[ERROR] ...", "Error: ..."
                    error_match = re.search(r'(?:ERROR|Error|\[ERROR\])[:\s]+(.+)', stripped_line, re.IGNORECASE)
                    if error_match:
                        error_msg = error_match.group(1).strip()
                        error_messages.append(error_msg)

                        # í…Œì´ë¸” ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
                        table_error_match = re.search(r"`(\w+)`\.`(\w+)`", error_msg)
                        if table_error_match:
                            error_table = table_error_match.group(2)
                            if error_table in import_results:
                                import_results[error_table] = {'status': 'error', 'message': error_msg}
                                if table_status_callback:
                                    table_status_callback(error_table, 'error', error_msg)

                        if progress_callback:
                            progress_callback(f"âŒ ì—ëŸ¬: {error_msg}")

                    # --- íŒ¨í„´ 5: Deadlock ê°ì§€ ---
                    if 'deadlock' in stripped_line.lower():
                        error_messages.append(f"Deadlock detected: {stripped_line}")
                        if progress_callback:
                            progress_callback(f"âš ï¸ Deadlock ê°ì§€: {stripped_line}")

                    # --- íŒ¨í„´ 6: Warning ê°ì§€ ---
                    warning_match = re.search(r'(?:WARNING|Warning|\[WARNING\])[:\s]+(.+)', stripped_line, re.IGNORECASE)
                    if warning_match:
                        if progress_callback:
                            progress_callback(f"âš ï¸ ê²½ê³ : {warning_match.group(1).strip()}")

            # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
            rc = process.poll()
            if rc is None:
                process.wait(timeout=3600)
                rc = process.returncode

            if rc == 0:
                # ìµœì¢… ì§„í–‰ë¥  100% í‘œì‹œ
                if tables and table_progress_callback and total_tables > 0:
                    table_progress_callback(total_tables, total_tables, tables[-1])

                # ëª¨ë“  í…Œì´ë¸” ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                for table in tables:
                    if import_results.get(table, {}).get('status') != 'error':
                        import_results[table] = {'status': 'done', 'message': ''}
                        if table_status_callback:
                            table_status_callback(table, 'done', '')

                return True, "ì„±ê³µ", import_results
            else:
                # ì‹¤íŒ¨ ì‹œ pending ìƒíƒœì¸ í…Œì´ë¸”ë“¤ì„ errorë¡œ ë³€ê²½
                error_summary = "; ".join(error_messages[:3]) if error_messages else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                for table in tables:
                    if import_results.get(table, {}).get('status') in ('pending', 'loading'):
                        import_results[table] = {'status': 'error', 'message': error_summary}
                        if table_status_callback:
                            table_status_callback(table, 'error', error_summary)

                return False, f"mysqlsh ì‹¤í–‰ ì‹¤íŒ¨: {error_summary}", import_results

        except subprocess.TimeoutExpired:
            if process:
                process.kill()
            # íƒ€ì„ì•„ì›ƒ ì‹œ pending/loading í…Œì´ë¸”ë“¤ì„ errorë¡œ ë³€ê²½
            for table in (tables or []):
                if import_results.get(table, {}).get('status') in ('pending', 'loading'):
                    import_results[table] = {'status': 'error', 'message': 'ì‘ì—… ì‹œê°„ ì´ˆê³¼'}
                    if table_status_callback:
                        table_status_callback(table, 'error', 'ì‘ì—… ì‹œê°„ ì´ˆê³¼')
            return False, "ì‘ì—… ì‹œê°„ ì´ˆê³¼ (1ì‹œê°„)", import_results
        except Exception as e:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ pending/loading í…Œì´ë¸”ë“¤ì„ errorë¡œ ë³€ê²½
            for table in (tables or []):
                if import_results.get(table, {}).get('status') in ('pending', 'loading'):
                    import_results[table] = {'status': 'error', 'message': str(e)}
                    if table_status_callback:
                        table_status_callback(table, 'error', str(e))
            return False, str(e), import_results


class TableProgressTracker:
    """í…Œì´ë¸”ë³„ Import ì§„í–‰ìƒí™© ì¶”ì """

    def __init__(self, metadata: Optional[Dict]):
        """
        Args:
            metadata: _analyze_dump_metadata()ì˜ ë°˜í™˜ê°’
        """
        if metadata:
            self.chunk_counts = metadata.get('chunk_counts', {})
            self.table_sizes = metadata.get('table_sizes', {})
            self.total_bytes = metadata.get('total_bytes', 0)
        else:
            self.chunk_counts = {}
            self.table_sizes = {}
            self.total_bytes = 0

        self.completed_tables: Set[str] = set()

    def estimate_loading_tables(
        self,
        loaded_bytes: int,
        completed_tables: List[str]
    ) -> List[Tuple[str, int, int]]:
        """
        í˜„ì¬ ë¡œë”© ì¤‘ì¸ í…Œì´ë¸” ì¶”ì •

        Args:
            loaded_bytes: í˜„ì¬ê¹Œì§€ ë¡œë”©ëœ ë°”ì´íŠ¸ ìˆ˜
            completed_tables: ì™„ë£Œëœ í…Œì´ë¸” ëª©ë¡

        Returns:
            [(table_name, size_bytes, chunk_count), ...] (ìƒìœ„ 4ê°œ, í¬ê¸° í° ìˆœ)
        """
        # ì™„ë£Œëœ í…Œì´ë¸”ë“¤ì˜ bytes í•©ê³„
        self.completed_tables = set(completed_tables)
        completed_bytes = sum(
            self.table_sizes.get(t, 0) for t in self.completed_tables
        )

        # ë‚¨ì€ bytes
        remaining_bytes = loaded_bytes - completed_bytes

        # ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ì¤‘ ë¯¸ì™„ë£Œëœ í…Œì´ë¸” ì°¾ê¸° (10MB ì´ìƒ)
        loading_candidates = [
            (
                table,
                self.table_sizes.get(table, 0),
                self.chunk_counts.get(table, 1)
            )
            for table in self.table_sizes.keys()
            if table not in self.completed_tables and self.table_sizes.get(table, 0) > 10_000_000
        ]

        # í¬ê¸° í° ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 4ê°œ ë°˜í™˜
        loading_candidates.sort(key=lambda x: -x[1])
        return loading_candidates[:4]

    def get_table_info(self, table_name: str) -> Tuple[int, int]:
        """
        í…Œì´ë¸” ì •ë³´ ì¡°íšŒ

        Returns:
            (size_bytes, chunk_count)
        """
        return (
            self.table_sizes.get(table_name, 0),
            self.chunk_counts.get(table_name, 1)
        )

    def format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.2f} GB"


# í¸ì˜ í•¨ìˆ˜
def check_mysqlsh() -> Tuple[bool, str]:
    """mysqlsh ì„¤ì¹˜ í™•ì¸ (ê°„í¸ í•¨ìˆ˜)"""
    installed, msg, _ = MySQLShellChecker.check_installation()
    return installed, msg


def export_schema(
    host: str,
    port: int,
    user: str,
    password: str,
    schema: str,
    output_dir: str,
    threads: int = 4,
    progress_callback: Optional[Callable[[str], None]] = None
) -> Tuple[bool, str]:
    """
    ì „ì²´ ìŠ¤í‚¤ë§ˆ Export (ê°„í¸ í•¨ìˆ˜)
    """
    config = MySQLShellConfig(host, port, user, password)
    exporter = MySQLShellExporter(config)
    return exporter.export_full_schema(schema, output_dir, threads, progress_callback=progress_callback)


def export_tables(
    host: str,
    port: int,
    user: str,
    password: str,
    schema: str,
    tables: List[str],
    output_dir: str,
    threads: int = 4,
    include_fk_parents: bool = True,
    progress_callback: Optional[Callable[[str], None]] = None
) -> Tuple[bool, str, List[str]]:
    """
    ì„ íƒëœ í…Œì´ë¸” Export (ê°„í¸ í•¨ìˆ˜)
    """
    config = MySQLShellConfig(host, port, user, password)
    exporter = MySQLShellExporter(config)
    return exporter.export_tables(
        schema, tables, output_dir, threads,
        include_fk_parents=include_fk_parents,
        progress_callback=progress_callback
    )


def import_dump(
    host: str,
    port: int,
    user: str,
    password: str,
    input_dir: str,
    target_schema: Optional[str] = None,
    threads: int = 4,
    import_mode: str = "replace",
    progress_callback: Optional[Callable[[str], None]] = None
) -> Tuple[bool, str, dict]:
    """
    Dump Import (ê°„í¸ í•¨ìˆ˜)

    Args:
        import_mode: Import ëª¨ë“œ
            - "merge": ë³‘í•© (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
            - "replace": ì „ì²´ êµì²´ (ëª¨ë“  ê°ì²´ ì¬ìƒì„±, resetProgress=true)
            - "recreate": ì™„ì „ ì¬ìƒì„± (ìŠ¤í‚¤ë§ˆ DROP í›„ ì¬ìƒì„±)
    """
    config = MySQLShellConfig(host, port, user, password)
    importer = MySQLShellImporter(config)
    return importer.import_dump(
        input_dir,
        target_schema,
        threads,
        import_mode=import_mode,
        progress_callback=progress_callback
    )
